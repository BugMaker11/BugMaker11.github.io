<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>My New Post</title>
    <url>/2023/03/20/My-New-Post/</url>
    <content><![CDATA[<span id="more"></span>

<p><strong>Hello! it’s my first blog.</strong></p>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/03/20/hello-world/</url>
    <content><![CDATA[<span id="more"></span>

<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>动手学深度学习-softmax回归</title>
    <url>/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-softmax%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p>图像分类数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>读取数据集</p>
<p>ToTensor使数据从pil类型变换成32位浮点数格式,root为读取数据集位置,train表示读取的是否是训练集,transform表示读取的是向量格式,download表示从网络下载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trans = transforms.ToTensor()</span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;../data&quot;</span>,train = <span class="literal">True</span>,transform = trans,download=<span class="literal">True</span>) <span class="comment">#训练集</span></span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;../data&quot;</span>,train= <span class="literal">False</span>, transform= trans,download=<span class="literal">True</span>) <span class="comment">#测试集</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(mnist_train),<span class="built_in">len</span>(mnist_test)</span><br></pre></td></tr></table></figure>




<pre><code>(60000, 10000)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mnist_train[<span class="number">0</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([1, 28, 28])
</code></pre>
<p>用于在数字标签索引及其文本名称之间进行转换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签&quot;&quot;&quot;</span></span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>,<span class="string">&#x27;trouser&#x27;</span>,<span class="string">&#x27;pullover&#x27;</span>,<span class="string">&#x27;dress&#x27;</span>,<span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                  <span class="string">&#x27;sandal&#x27;</span>,<span class="string">&#x27;shirt&#x27;</span>,<span class="string">&#x27;sneaker&#x27;</span>,<span class="string">&#x27;bag&#x27;</span>,<span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br></pre></td></tr></table></figure>

<p>创建函数可视化样本</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs,num_rows,num_cols,titles=<span class="literal">None</span>,scale=<span class="number">1.5</span></span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制图像列表&quot;&quot;&quot;</span></span><br><span class="line">    figsize = (num_cols*scale,num_rows*scale)</span><br><span class="line">    _,axes = d2l.plt.subplots(num_rows,num_cols,figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i ,(ax,img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes,imgs)):</span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(img):</span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X,y = <span class="built_in">next</span>(<span class="built_in">iter</span>(data.DataLoader(mnist_train,batch_size=<span class="number">18</span>)))</span><br><span class="line">show_images(X.reshape(<span class="number">18</span>,<span class="number">28</span>,<span class="number">28</span>),<span class="number">2</span>,<span class="number">9</span>,titles=get_fashion_mnist_labels(y));</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-softmax%E5%9B%9E%E5%BD%92/output_11_0.png" alt="png"></p>
<p>读取小批量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_iter = data.DataLoader(mnist_train,batch_size,shuffle=<span class="literal">True</span>,num_workers=get_dataloader_workers())</span><br></pre></td></tr></table></figure>

<p>读取训练数据所需的时间</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">timer = d2l.Timer()</span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.2</span>f&#125;</span> sec&#x27;</span></span><br></pre></td></tr></table></figure>




<pre><code>&#39;25.70 sec&#39;
</code></pre>
<p>将数据读取整合到一起</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size,resize=<span class="literal">None</span></span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-NMIST数据集,然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>,transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">True</span>,transform=trans,download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">False</span>,transform=trans,download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train,batch_size,shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test,batch_size,shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_iter,test_iter = load_data_fashion_mnist(<span class="number">32</span>,resize=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape,X.dtype,y.shape,y.dtype)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64
</code></pre>
<h1 id="线性回归从零开始实现"><a href="#线性回归从零开始实现" class="headerlink" title="线性回归从零开始实现"></a>线性回归从零开始实现</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>生成数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w,b,num_examples</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y和x&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>,<span class="number">1</span>,(num_examples,<span class="built_in">len</span>(w))) <span class="comment">#X为高斯分布的随机数</span></span><br><span class="line">    y = torch.matmul(X,w)+b <span class="comment">#根据X和预设的w计算y</span></span><br><span class="line">    y += torch.normal(<span class="number">0</span>,<span class="number">0.01</span>,y.shape) <span class="comment">#对y添加一个随机扰动</span></span><br><span class="line">    <span class="keyword">return</span> X,y.reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w = torch.tensor([<span class="number">2</span>,-<span class="number">3.4</span>]) <span class="comment">#设置w参数</span></span><br><span class="line">true_b = <span class="number">4.2</span> <span class="comment">#设置b参数</span></span><br><span class="line">features,labels = synthetic_data(true_w,true_b,<span class="number">1000</span>) <span class="comment">#根据参数设置数据集</span></span><br></pre></td></tr></table></figure>

<p>批量读取数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size,features,labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples)) <span class="comment">#设置一个标号用来随机读取数据集中的数据</span></span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,num_examples,batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i:<span class="built_in">min</span>(i + batch_size,num_examples)]) <span class="comment">#选取一个标号生成向量</span></span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices],labels[batch_indices] <span class="comment">#每次单独取一个features,labels</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size,features,labels):</span><br><span class="line">    <span class="built_in">print</span>(X,<span class="string">&#x27;\n&#x27;</span>,y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.1081, -0.5731],
        [-0.4934, -0.1212],
        [-0.1073,  0.8958],
        [-0.2244,  0.1532],
        [-0.0630, -0.7970],
        [-0.8336, -1.2219],
        [-1.0196,  1.6207],
        [-0.1009,  0.5200],
        [-0.4763, -0.1988],
        [ 0.5296, -0.2843]]) 
 tensor([[ 5.9328],
        [ 3.6350],
        [ 0.9321],
        [ 3.2144],
        [ 6.7879],
        [ 6.6725],
        [-3.3650],
        [ 2.2413],
        [ 3.9385],
        [ 6.2498]])
</code></pre>
<p>初始化模型参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.normal(<span class="number">0</span>,<span class="number">0.01</span>,size=(<span class="number">2</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>定义模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X,w,b</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X,w) + b</span><br></pre></td></tr></table></figure>

<p>定义损失函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat,y</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape))**<span class="number">2</span> /<span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>定义优化算法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params,lr,batch_size</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr*param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size,features,labels):</span><br><span class="line">        l = loss(net(X,w,b),y) <span class="comment">#小批量的均方损失</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w,b],lr,batch_size) <span class="comment">#使用梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features,w,b),labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>,loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>) <span class="comment">#打印每个epoch的均方损失</span></span><br></pre></td></tr></table></figure>

<pre><code>epoch1,loss 0.000158
epoch2,loss 0.000051
epoch3,loss 0.000051
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差:<span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的估计误差:<span class="subst">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的估计误差:tensor([-1.5759e-04, -9.2030e-05], grad_fn=&lt;SubBackward0&gt;)
b的估计误差:tensor([-0.0002], grad_fn=&lt;RsubBackward1&gt;)
</code></pre>
<h1 id="softmax回归的简洁实现-使用pytorch"><a href="#softmax回归的简洁实现-使用pytorch" class="headerlink" title="softmax回归的简洁实现(使用pytorch)"></a>softmax回归的简洁实现(使用pytorch)</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>导入数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size) </span><br><span class="line"><span class="comment">#从d2l包中直接导入数据集</span></span><br></pre></td></tr></table></figure>

<p>初始化模型参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),nn.Linear(<span class="number">784</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight,std=<span class="number">0.01</span>) <span class="comment">#m用正态分布随机化一个数</span></span><br><span class="line">        </span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></table></figure>

<p>定义损失函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>调用优化算法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(),lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>

<p>训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-softmax%E5%9B%9E%E5%BD%92/output_50_0.svg" alt="svg"></p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo 设置</title>
    <url>/2023/03/20/hexo-%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<span id="more"></span>

<h3 id="hexo-常用命令"><a href="#hexo-常用命令" class="headerlink" title="hexo 常用命令"></a>hexo 常用命令</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new &quot;name&quot; 	 #新建文章</span><br><span class="line">hexo new page &quot;name&quot; #新建页面</span><br><span class="line">hexo g 				 #生成页面</span><br><span class="line">hexo d				 #部署</span><br><span class="line">hexo s				 #本地预览</span><br><span class="line">hexo clean 			 #清除缓存和已生成的静态文件</span><br><span class="line">hexo help			 #帮助</span><br></pre></td></tr></table></figure>



<h3 id="hexo-美化"><a href="#hexo-美化" class="headerlink" title="hexo 美化"></a>hexo 美化</h3><p>….未完待续</p>
<h3 id="hexo-设置观察"><a href="#hexo-设置观察" class="headerlink" title="hexo 设置观察"></a>hexo 设置观察</h3><p> $ax &#x3D; b + c$</p>
]]></content>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-多层感知机</title>
    <url>/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    <content><![CDATA[<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><p>多层感知机与softmax线性回归的区别:</p>
<ol>
<li>多层感知机有多层,而sotfmax线性回归只有一层</li>
<li>多层感知机有隐藏层,隐藏层需要激活函数来免除不同层之间的线性关系</li>
</ol>
<p>如果没有激活函数,多层感知机就与softmax线性回归没什么区别了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>使用ReLU函数作为激活函数来提供非线性变换</p>
<p>$ReLU(x) &#x3D; max(x,0)$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.linspace(-<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#cond = [True if (i&gt;0) else False for i in x] #也使用列表解析的方法</span></span><br><span class="line">y=<span class="number">0</span> + x*(x&gt;<span class="number">0</span>)</span><br><span class="line">plt.plot(x, y, color=<span class="string">&#x27;b&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>[&lt;matplotlib.lines.Line2D at 0x270f3bbf7c0&gt;]
</code></pre>
<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_5_1.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(-<span class="number">8.0</span>,<span class="number">8.0</span>,<span class="number">0.1</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.relu(x)</span><br><span class="line">d2l.plot(x.detach(),y.detach(),<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;relu(x)&#x27;</span>,figsize=(<span class="number">5</span>,<span class="number">2.5</span>))</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_6_0.svg" alt="svg"></p>
<p>使用sigmoid函数作为激活函数来提供非线性变换</p>
<p>$sigmoid(x)&#x3D;\frac{1}{1+e^{-x}}$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.sigmoid(x)</span><br><span class="line">d2l.plot(x.detach(),y.detach(),<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;sigmoid(x)&#x27;</span>,figsize=(<span class="number">5</span>,<span class="number">2.5</span>))</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_8_0.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#x.grad.data.zero_()</span></span><br><span class="line">y.backward(torch.ones_like(x),retain_graph=<span class="literal">True</span>)</span><br><span class="line">d2l.plot(x.detach(),x.grad,<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;grad of sigmoid&#x27;</span>,figsize=(<span class="number">5</span>,<span class="number">2.5</span>))</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_9_0.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>获取图像分类数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>

<p>初始化模型参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs,num_outputs,num_hiddens = <span class="number">784</span>,<span class="number">10</span>,<span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = nn.Parameter(torch.randn(num_inputs,num_hiddens,requires_grad=<span class="literal">True</span>)*<span class="number">0.01</span>)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens,requires_grad=<span class="literal">True</span>))</span><br><span class="line">W2 = nn.Parameter(torch.randn(num_hiddens,num_outputs,requires_grad=<span class="literal">True</span>)*<span class="number">0.01</span>)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs,requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1,b1,W2,b2]</span><br></pre></td></tr></table></figure>

<p>激活函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X,a)</span><br></pre></td></tr></table></figure>

<p>模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape((-<span class="number">1</span>,num_inputs))</span><br><span class="line">    H = relu(X@W1 + b1)</span><br><span class="line">    <span class="keyword">return</span> (H@W2 + b2)</span><br></pre></td></tr></table></figure>

<p>损失函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs,lr = <span class="number">10</span>,<span class="number">0.1</span></span><br><span class="line">updater = torch.optim.SGD(params,lr=lr)</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,updater)</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_22_0.svg" alt="svg"></p>
<p>多层感知机的简洁实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(), <span class="comment">#将数据变成一维</span></span><br><span class="line">                    nn.Linear(<span class="number">784</span>,<span class="number">256</span>), <span class="comment">#输入层</span></span><br><span class="line">                    nn.ReLU(), <span class="comment">#激活函数</span></span><br><span class="line">                    nn.Linear(<span class="number">256</span>,<span class="number">10</span>)) <span class="comment">#隐藏层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>): <span class="comment">#将nn模型中的权重初始化为随机数</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight,std=<span class="number">0.01</span>)</span><br><span class="line">        </span><br><span class="line">net.apply(init_weights); <span class="comment">#应用初始化</span></span><br></pre></td></tr></table></figure>

<p>设置训练的超参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size,lr,num_epochs = <span class="number">256</span>,<span class="number">0.1</span>,<span class="number">10</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(),lr=lr)</span><br></pre></td></tr></table></figure>

<p>训练模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_29_0.svg" alt="svg"></p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-微积分</title>
    <url>/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%BE%AE%E7%A7%AF%E5%88%86/</url>
    <content><![CDATA[<h1 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h1><p>在运行时每次都内核崩溃,加上下面代码后能够成功运行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib_inline <span class="keyword">import</span> backend_inline</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">3</span>*x**<span class="number">2</span> - <span class="number">4</span>*x</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">numerical_lim</span>(<span class="params">f, x, h</span>):</span><br><span class="line">    <span class="keyword">return</span> (f(x + h) - f(x)) / h</span><br><span class="line"></span><br><span class="line">h = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;h=<span class="subst">&#123;h:<span class="number">.5</span>f&#125;</span>, numerical limit=<span class="subst">&#123;numerical_lim(f, <span class="number">1</span>, h):<span class="number">.5</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    h *= <span class="number">0.1</span></span><br></pre></td></tr></table></figure>

<pre><code>h=0.10000, numerical limit=2.30000
h=0.01000, numerical limit=2.03000
h=0.00100, numerical limit=2.00300
h=0.00010, numerical limit=2.00030
h=0.00001, numerical limit=2.00003
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">use_svg_display</span>(): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用svg格式在Jupyter中显示绘图&quot;&quot;&quot;</span></span><br><span class="line">    backend_inline.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>) <span class="comment">#指定显示为svg格式</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_figsize</span>(<span class="params">figsize=(<span class="params"><span class="number">3.5</span>,<span class="number">2.5</span></span>)</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置matplotlib的图标大小&quot;&quot;&quot;</span></span><br><span class="line">    use_svg_display()</span><br><span class="line">    d2l.plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] =  figsize</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_axes</span>(<span class="params">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;</span></span><br><span class="line">    axes.set_xlabel(xlabel)</span><br><span class="line">    axes.set_ylabel(ylabel)</span><br><span class="line">    axes.set_xscale(xscale)</span><br><span class="line">    axes.set_yscale(yscale)</span><br><span class="line">    axes.set_xlim(xlim)</span><br><span class="line">    axes.set_ylim(ylim)</span><br><span class="line">    <span class="keyword">if</span> legend:</span><br><span class="line">        axes.legend(legend)</span><br><span class="line">    axes.grid()</span><br></pre></td></tr></table></figure>

<p>通过使用上面三个图形化配置函数,定义一个<strong>plot</strong> 函数用来绘制曲线.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">X,Y=<span class="literal">None</span>,xlabel=<span class="literal">None</span>,ylabel=<span class="literal">None</span>,legend=<span class="literal">None</span>,xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">         ylim=<span class="literal">None</span>,xscale=<span class="string">&#x27;linear&#x27;</span>,yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">         fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>,<span class="string">&#x27;m--&#x27;</span>,<span class="string">&#x27;g-&#x27;</span>,<span class="string">&#x27;r:&#x27;</span></span>),figsize=(<span class="params"><span class="number">3.5</span>,<span class="number">2.5</span></span>),axes=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制数据点&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        legend = []</span><br><span class="line">    </span><br><span class="line">    set_figsize(figsize)</span><br><span class="line">    axes = axes <span class="keyword">if</span> axes <span class="keyword">else</span> d2l.plt.gca()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#如果X有一个轴,输出True</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">has_one_axis</span>(<span class="params">X</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="built_in">hasattr</span>(X,<span class="string">&quot;ndim&quot;</span>) <span class="keyword">and</span> X.ndim == <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(X,<span class="built_in">list</span>)</span><br><span class="line">                <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(X[<span class="number">0</span>],<span class="string">&quot;__len__&quot;</span>))</span><br><span class="line">    <span class="keyword">if</span> has_one_axis(X):</span><br><span class="line">        X=[X]</span><br><span class="line">    <span class="keyword">if</span> Y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        X,Y = [[]]*<span class="built_in">len</span>(X),X</span><br><span class="line">    <span class="keyword">elif</span> has_one_axis(Y):</span><br><span class="line">        Y = [Y]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(X) != <span class="built_in">len</span>(Y):</span><br><span class="line">        X = X * <span class="built_in">len</span>(Y)</span><br><span class="line">    axes.cla()</span><br><span class="line">    <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y, fmts):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(x):</span><br><span class="line">            axes.plot(x, y, fmt)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            axes.plot(y, fmt)</span><br><span class="line">    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">例子:绘制f(x)=<span class="number">2</span>*x-<span class="number">3</span>在 x=<span class="number">1</span>处的切线 y=2x-<span class="number">3</span> </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">3</span>,<span class="number">0.1</span>)</span><br><span class="line">plot(x,[f(x), <span class="number">2</span>*x-<span class="number">3</span>],<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;f(x)&#x27;</span>, legend=[<span class="string">&#x27;f(x)&#x27;</span>,<span class="string">&#x27;Tangent line(x=1)&#x27;</span>])</span><br></pre></td></tr></table></figure>


<p><img src="/output_11_0.svg" alt="svg"></p>
<p>偏导数</p>
<p>用于描述多元函数对于变量的微分</p>
<p>梯度</p>
<p>连结一个多元函数对所有变量的偏导数,得到该函数的梯度</p>
<h1 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h1><p>深度学习框架不再手工进行求导,而是通过自动微分加快求导.系统会构建一个计算图来跟踪求导运算.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">4.0</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>


<pre><code>tensor([0., 1., 2., 3.])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.requires_grad_(<span class="literal">True</span>) <span class="comment">#表示为梯度开辟空间来存储</span></span><br><span class="line">x.grad  <span class="comment">#x.grad表示x的梯度</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = <span class="number">2</span>* torch.dot(x,x) <span class="comment">#y=2xT*x</span></span><br><span class="line">y</span><br></pre></td></tr></table></figure>


<pre><code>tensor(28., grad_fn=&lt;MulBackward0&gt;)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y.backward() <span class="comment">#调用反向传播函数自动计算y对于x每个分量的梯度</span></span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>


<pre><code>tensor([ 0.,  4.,  8., 12.])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad == <span class="number">4</span> * x  <span class="comment">#验证梯度</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([True, True, True, True])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()  <span class="comment">#清除上一次求的梯度,因为pytorch求的导数会累加起来</span></span><br><span class="line">y = x.<span class="built_in">sum</span>()  <span class="comment">#x.sum() = x1+x2+x3+...,所以x.sum()对x的梯度为(1,1,1,...)</span></span><br><span class="line">y.backward() <span class="comment">#求梯度</span></span><br><span class="line">x.grad   <span class="comment">#展示梯度</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([1., 1., 1., 1.])
</code></pre>
<p>非标量变量的反向传播(y不是标量,而是x的函数时)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x*x</span><br><span class="line">y.<span class="built_in">sum</span>().backward() <span class="comment">#将y转化为标量后进行求导y.sum()=x1^2+x2^2+.....</span></span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>


<pre><code>tensor([0., 2., 4., 6.])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y  ,y.backward()<span class="comment">#y是一个向量,x也是一个向量,y对x求导结果是一个矩阵,</span></span><br><span class="line"><span class="comment"># 而我们需要的是x1^2对x1的偏导,x2^2对x2的偏导...,所以要将y转化为标量后求导</span></span><br></pre></td></tr></table></figure>

<p>分离计算</p>
<p>非标量变量需要当作标量变量使用时进行分离,分离后作为常数进行处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()  <span class="comment">#detach()将y分离为常数</span></span><br><span class="line">z = u*x </span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u  <span class="comment">#z对x的导数=u</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([True, True, True, True])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == <span class="number">2</span>*x</span><br></pre></td></tr></table></figure>


<pre><code>tensor([True, True, True, True])
</code></pre>
<p>Python控制流的梯度计算</p>
<p>即使一个具有多种函数的控制流对x求导,我们也能计算梯度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a*<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.randn(size=(),requires_grad=<span class="literal">True</span>)</span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a.grad == d/a <span class="comment">#f(a)是线性的,所以可以用d/a验证a.grad导数是否算对了</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor(True)
</code></pre>
<p>练习</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d.backward()  <span class="comment">#再次运行反向传播函数会报错,因为导数会积累,</span></span><br><span class="line"><span class="comment">#确定要再次运行反向传播函数可以加上retain_graph=True 参数</span></span><br></pre></td></tr></table></figure>


<pre><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

~\AppData\Local\Temp\ipykernel_19960\2703853229.py in &lt;module&gt;
----&gt; 1 d.backward()  #再次运行反向传播函数会报错


F:\anaconda\lib\site-packages\torch\_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)
    485                 inputs=inputs,
    486             )
--&gt; 487         torch.autograd.backward(
    488             self, gradient, retain_graph, create_graph, inputs=inputs
    489         )


F:\anaconda\lib\site-packages\torch\autograd\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    198     # some Python versions print out the first line of a multi-line function
    199     # calls in the traceback and some print out the last line
--&gt; 200     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    201         tensors, grad_tensors_, retain_graph, create_graph, inputs,
    202         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass


RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
</code></pre>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-数据操作,预处理笔记</title>
    <url>/2023/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>torch.arange(n)<br>生成从0到n-1的所有整数作为一个张量,n&#x3D;0时张量为空</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">12</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>


<pre><code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.arange(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([], dtype=torch.int64)
</code></pre>
<p><strong>x.shape</strong><br>输出张量的形状(每个轴的长度)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure>


<pre><code>torch.Size([12])
</code></pre>
<p>求张量中元素的总数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.numel()</span><br></pre></td></tr></table></figure>


<pre><code>12
</code></pre>
<p>改变张量的形状,可以输入张量每个维度的长度,也可以少输入一个维度的长度,因为可以自动推断出那个维度的长度.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = x.reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">X</span><br><span class="line">X.shape</span><br></pre></td></tr></table></figure>


<pre><code>torch.Size([3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = x.reshape(<span class="number">3</span>,<span class="number">2</span>,-<span class="number">1</span>)</span><br><span class="line">X</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[[ 0,  1],
         [ 2,  3]],

        [[ 4,  5],
         [ 6,  7]],

        [[ 8,  9],
         [10, 11]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.reshape(-<span class="number">1</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.reshape(<span class="number">3</span>,-<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
</code></pre>
<p>torch.zeros((n1,…)):创建全0的张量<br>torch.ones((n1,n2,n3,…))创建全1的张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.zeros((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]]])
</code></pre>
<p>创建正态分布随机数构成的张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-1.0532, -1.2373,  1.5930, -1.3120],
        [ 1.7310, -0.5996,  0.6864, -1.3625],
        [-0.4236,  0.8607,  0.0581, -1.2500]])
</code></pre>
<p>为创建的张量赋值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.tensor([[<span class="number">2</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[2, 1, 4, 3],
        [1, 2, 3, 4],
        [4, 3, 2, 1]])
</code></pre>
<p>张量的计算 + - * &#x2F; **<br>x ** y 表示x的y次幂</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">8</span>])</span><br><span class="line">y = torch.tensor([<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">x+y , x-y , x*y , x/y , x**y</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([ 3.,  4.,  6., 10.]),
 tensor([-1.,  0.,  2.,  6.]),
 tensor([ 2.,  4.,  8., 16.]),
 tensor([0.5000, 1.0000, 2.0000, 4.0000]),
 tensor([ 1.,  4., 16., 64.]))
</code></pre>
<p>对张量中的每个元素求幂运算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.exp(x)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])
</code></pre>
<p>将两个张量合并在一起 torch.cat( (x,y),dim&#x3D;?)<br>dim&#x3D;0时表示按行合并<br>dim&#x3D;1时表示按列合并</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>,dtype=torch.float32).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]])</span><br><span class="line">torch.cat((X,Y),dim=<span class="number">0</span>),torch.cat((X,Y),dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果:</p>
<pre><code>(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [ 2.,  1.,  4.,  3.],
         [ 1.,  2.,  3.,  4.],
         [ 4.,  3.,  2.,  1.]]),
 tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
</code></pre>
<p>逻辑运算符构建二元张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X == Y</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[False,  True, False,  True],
        [False, False, False, False],
        [False, False, False, False]])
</code></pre>
<p>对张量中的所有元素进行求和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>


<pre><code>tensor(66.)
</code></pre>
<p>广播机制:</p>
<p>当两个张量的形状不同时,通过广播机制可以使两个张量通过复制元素变成相同的形状,比如下面的例子中 a + b 后 a , b的形状就分别从 (3,1) 和 (1,2) 变成 (3,2)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[0],
         [1],
         [2]]),
 tensor([[0, 1]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a + b</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[0, 1],
        [1, 2],
        [2, 3]])
</code></pre>
<p>索引和切片</p>
<p>张量可以直接通过 [数字] 进行索引,同时选择多个元素就是切片,通过 数字:数字 实现.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X[-<span class="number">1</span>],X[<span class="number">1</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([ 8.,  9., 10., 11.]),
 tensor([[ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.]]))
</code></pre>
<p>除读取外还可以通过指定索引来将元素写入矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X[<span class="number">1</span>,<span class="number">2</span>] = <span class="number">9</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  9.,  7.],
        [ 8.,  9., 10., 11.]])
</code></pre>
<p>可以同时为多个元素赋值,只有一个:时表示所有元素</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X[<span class="number">0</span>:<span class="number">2</span>, :] = <span class="number">12</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[12., 12., 12., 12.],
        [12., 12., 12., 12.],
        [ 8.,  9., 10., 11.]])
</code></pre>
<p>执行原地操作,用[:]将张量中的所有元素改写而不是用结果创建一个新的变量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = torch.zeros_like(Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;id(Z)&quot;</span>,<span class="built_in">id</span>(Z))</span><br><span class="line">Z[:] = X + Y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;id(Z)&quot;</span>,<span class="built_in">id</span>(Z))</span><br></pre></td></tr></table></figure>

<pre><code>id(Z) 1382002751504
id(Z) 1382002751504
</code></pre>
<p>类型转换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="built_in">type</span>(A),<span class="built_in">type</span>(B)</span><br></pre></td></tr></table></figure>




<pre><code>(numpy.ndarray, torch.Tensor)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a,a.item(),<span class="built_in">float</span>(a),<span class="built_in">int</span>(a)</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([3.5000]), 3.5, 3.5, 3)
</code></pre>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>首先(<strong>创建一个人工数据集，并存储在CSV（逗号分隔值）文件</strong>) <code>../data/house_tiny.csv</code>中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>,<span class="string">&#x27;data&#x27;</span>),exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>,<span class="string">&#x27;data&#x27;</span>,<span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,10600\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>从创建的CSV文件中加载原始数据集</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install pandas</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line">data</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>NumRooms</th>
      <th>Alley</th>
      <th>Price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>Pave</td>
      <td>127500</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.0</td>
      <td>NaN</td>
      <td>10600</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.0</td>
      <td>NaN</td>
      <td>178100</td>
    </tr>
    <tr>
      <th>3</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>140000</td>
    </tr>
  </tbody>
</table>

</div>

<p>为了处理缺失的数据，我们用插值法,将缺失的房间数插值为房间数的平均值.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs,outputs = data.iloc[:,<span class="number">0</span>:<span class="number">2</span>],data.iloc[:,<span class="number">2</span>]</span><br><span class="line">inputs = inputs.fillna(inputs.mean())</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>

<pre><code>   NumRooms Alley
0       3.0  Pave
1       2.0   NaN
2       4.0   NaN
3       3.0   NaN
</code></pre>
<p>将缺失的Alley单独视为一类,数值为1,不缺失的房子则为0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = pd.get_dummies(inputs,dummy_na=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>

<pre><code>   NumRooms  Alley_Pave  Alley_nan
0       3.0           1          0
1       2.0           0          1
2       4.0           0          1
3       3.0           0          1
</code></pre>
<p>将inputs 和 outputs 中的所有条目转换为张量格式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">X,y = torch.tensor(inputs.values),torch.tensor(outputs.values)</span><br><span class="line">X,y</span><br></pre></td></tr></table></figure>

<p>输出结果:</p>
<pre><code>(tensor([[3., 1., 0.],
         [2., 0., 1.],
         [4., 0., 1.],
         [3., 0., 1.]], dtype=torch.float64),
 tensor([127500,  10600, 178100, 140000]))
</code></pre>
<p>练习:创建原始数据集,删除缺失值最多的列,将预处理后的数据集转换为张量格式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>,<span class="string">&#x27;data&#x27;</span>),exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>,<span class="string">&#x27;data&#x27;</span>,<span class="string">&#x27;information.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;Name,Age,Height,Gender,Province,Grades\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,12,175,&quot;men&quot;,&quot;henan&quot;,100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;&quot;xiaoming&quot;,NA,180,&quot;girl&quot;,&quot;hebei&quot;,80\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;&quot;xiaoli&quot;,17,174,NA,NA,90\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;&quot;xiaoqiang&quot;,NA,168,&quot;girl&quot;,NA,70\n&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line">data</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>Age</th>
      <th>Height</th>
      <th>Gender</th>
      <th>Province</th>
      <th>Grades</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>12.0</td>
      <td>175</td>
      <td>men</td>
      <td>henan</td>
      <td>100</td>
    </tr>
    <tr>
      <th>1</th>
      <td>xiaoming</td>
      <td>NaN</td>
      <td>180</td>
      <td>girl</td>
      <td>hebei</td>
      <td>80</td>
    </tr>
    <tr>
      <th>2</th>
      <td>xiaoli</td>
      <td>17.0</td>
      <td>174</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>90</td>
    </tr>
    <tr>
      <th>3</th>
      <td>xiaoqiang</td>
      <td>NaN</td>
      <td>168</td>
      <td>girl</td>
      <td>NaN</td>
      <td>70</td>
    </tr>
  </tbody>
</table>

</div>

]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-模型选择,过拟合,欠拟合</title>
    <url>/2023/03/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88/</url>
    <content><![CDATA[<h1 id="模型选择-欠拟合-过拟合"><a href="#模型选择-欠拟合-过拟合" class="headerlink" title="模型选择,欠拟合,过拟合"></a>模型选择,欠拟合,过拟合</h1><p>欠拟合: 简单的模型用复杂的数据进行训练时容易欠拟合</p>
<p>过拟合: 复杂的模型用简单的数据进行训练时容易过拟合,模型对训练数据的拟合比潜在的分布更接近</p>
<p>所以: 模型容量需要匹配数据的复杂度</p>
<p><img src="/2023/03/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88/1.png" alt="1.png"></p>
<h3 id="模型容量"><a href="#模型容量" class="headerlink" title="模型容量"></a>模型容量</h3><p>模型容量: 模型拟合各种函数的能力,低容量难以拟合数据,高容量容易记住所有训练数据</p>
<p><img src="/2023/03/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88/2.png" alt="2.png"></p>
<h3 id="模型容量选择"><a href="#模型容量选择" class="headerlink" title="模型容量选择"></a>模型容量选择</h3><p>模型容量低时对训练数据拟合的不好,训练误差大</p>
<p>模型容量高时对训练数据拟合更好,但模型容量太高容易导致过拟合,泛化误差反而增加</p>
<p>泛化误差最小时训练的模型对新数据的拟合最好,模型最优</p>
<p><img src="/2023/03/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88/3.png" alt="3.png"></p>
<h3 id="估计模型容量"><a href="#估计模型容量" class="headerlink" title="估计模型容量"></a>估计模型容量</h3><ol>
<li>参数的个数</li>
<li>参数值的选择范围</li>
</ol>
<p><strong>VC维</strong>  :</p>
<p>模型能够记住的最大的数据集的大小,可以用来衡量模型好坏,但深度学习中很少使用,因为不准确且很难计算</p>
<p>例子: 二维感知机能够区分3个点的任意放置,4个点时就有不能记住的类型了(xor),所以二维感知机的vc维&#x3D;3</p>
<p><img src="/2023/03/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88/4.png" alt="4.png"></p>
<p><strong>数据复杂度:</strong></p>
<ol>
<li>样本个数</li>
<li>每个样本的元素个数</li>
<li>时间,空间结构</li>
<li>多样性</li>
</ol>
<p>训练误差(training error): 模型在训练数据集上计算得到的误差</p>
<p>泛化误差(generalization error): 模型应用在新数据集上计算得到的误差</p>
<p>训练数据集: 训练模型参数</p>
<p>验证数据集: 用来训练时评估模型好坏的数据集,用来选择模型的超参数</p>
<p>测试数据集: 只用一次的数据集,用来应用模型</p>
<p>K折交叉验证</p>
<p>将原始数据集分成K个不交叉的子集,然后执行K次模型训练和验证,每次在K-1个子集上训练,在最后一个子集上验证,最后对K次实验的结果取平均来估计训练和验证误差</p>
<p>K折交叉验证常用于样本数据很小时,避免验证的数据集占比过大,K的数量越大模型训练效果越好,但K越大训练的次数越多,要考虑到成本因素进行控制</p>
<h1 id="过拟合和欠拟合的样例"><a href="#过拟合和欠拟合的样例" class="headerlink" title="过拟合和欠拟合的样例"></a>过拟合和欠拟合的样例</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>生成数据集</p>
<p><strong>$$y &#x3D; 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{ where }\epsilon \sim \mathcal{N}(0, 0.1^2).$$</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_degree = <span class="number">20</span> </span><br><span class="line">n_train,n_test = <span class="number">100</span>,<span class="number">100</span> <span class="comment">#训练集,验证集的大小</span></span><br><span class="line">true_w = np.zeros(max_degree) <span class="comment">#创造一个20维的向量w,4维之后的都是扰动</span></span><br><span class="line">true_w[<span class="number">0</span>:<span class="number">4</span>] = np.array([<span class="number">5</span>,<span class="number">1.2</span>,-<span class="number">3.4</span>,<span class="number">5.6</span>]) <span class="comment">#前4维是正确的参数</span></span><br><span class="line">features = np.random.normal(size=(n_train + n_test,<span class="number">1</span>)) <span class="comment">#生成一个200行1列的随机数据作为x</span></span><br><span class="line">np.random.shuffle(features) <span class="comment">#将features打乱</span></span><br><span class="line">poly_features = np.power(features,np.arange(max_degree).reshape(<span class="number">1</span>,-<span class="number">1</span>)) <span class="comment">#将featruers中的每个x做[0次方,1次方,2次方...],此时x作为多项式回归模型的输入，使模型能够捕捉特征与目标变量之间的非线性关系。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_degree):</span><br><span class="line">    poly_features[:,i] /= math.gamma(i+<span class="number">1</span>) <span class="comment">#对生成的features进行归一化</span></span><br><span class="line">labels = np.dot(poly_features,true_w) <span class="comment">#x和w点积</span></span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>,size = labels.shape) <span class="comment">#x和w点积+扰动=y</span></span><br></pre></td></tr></table></figure>

<p>true_w、features、poly_features 和 labels 都是 numpy 数组，分别表示真实权重、样本特征、多项式特征矩阵和目标变量。通过将它们转换为 PyTorch 的 tensor 类型，我们可以在 PyTorch 中进行深度学习模型的训练和优化，同时还可以利用 GPU 等硬件加速功能来提高计算性能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w,features,poly_features,labels = [torch.tensor(x,dtype=torch.float32) </span><br><span class="line">                            <span class="keyword">for</span> x <span class="keyword">in</span> [true_w,features,poly_features,labels]]</span><br></pre></td></tr></table></figure>

<p>features[:2] 表示选取 features 数组的前两行，即前两个样本的特征向量。</p>
<p>poly_features[:2,:] 表示选取 poly_features 数组的前两行和所有列，即前两个样本的多项式特征矩阵。</p>
<p>labels[:2] 表示选取 labels 数组的前两个元素，即前两个样本的目标变量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">features[:<span class="number">2</span>],poly_features[:<span class="number">2</span>,:],labels[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[ 0.4346],
         [-0.0716]]),
 tensor([[ 1.0000e+00,  4.3461e-01,  9.4444e-02,  1.3682e-02,  1.4866e-03,
           1.2922e-04,  9.3600e-06,  5.8114e-07,  3.1571e-08,  1.5246e-09,
           6.6260e-11,  2.6180e-12,  9.4816e-14,  3.1699e-15,  9.8404e-17,
           2.8512e-18,  7.7447e-20,  1.9800e-21,  4.7807e-23,  1.0935e-24],
         [ 1.0000e+00, -7.1576e-02,  2.5616e-03, -6.1116e-05,  1.0936e-06,
          -1.5655e-08,  1.8676e-10, -1.9096e-12,  1.7086e-14, -1.3588e-16,
           9.7258e-19, -6.3285e-21,  3.7748e-23, -2.0783e-25,  1.0626e-27,
          -5.0703e-30,  2.2682e-32, -9.5499e-35,  3.7975e-37, -1.4306e-39]]),
 tensor([5.2514, 4.9395]))
</code></pre>
<p>对模型进行训练和测试</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net,data_iter,loss</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>) <span class="comment">#累加器对象,有2维,一个表示累加总和,一个表示累加的数的个数</span></span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X) <span class="comment">#根据x计算y</span></span><br><span class="line">        y = y.reshape(out.shape) <span class="comment">#y改变成和out一样的形状</span></span><br><span class="line">        l = loss(out,y) <span class="comment">#算损失</span></span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(),l.numel()) <span class="comment">#将l的信息输入累加器</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>] <span class="comment">#返回平均损失</span></span><br></pre></td></tr></table></figure>

<p>训练函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features,test_features,train_labels,test_labels,num_epochs=<span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>) <span class="comment">#不损失降维操作的均方误差(即返回所有损失值,不进行求和等运算)</span></span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>] <span class="comment">#最后一个维度的大小,即每个样本的特征数</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape,<span class="number">1</span>,bias=<span class="literal">False</span>))</span><br><span class="line">    batch_size = <span class="built_in">min</span>(<span class="number">10</span>,train_labels.shape[<span class="number">0</span>]) <span class="comment">#barch_size 在不超过训练集大小的情况下,一次取10个</span></span><br><span class="line">    train_iter = d2l.load_array((train_features,train_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),batch_size) <span class="comment">#迭代器输入训练的x,y,每次输出batch_size个数</span></span><br><span class="line">    test_iter = d2l.load_array((test_features,test_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),batch_size,is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(),lr=<span class="number">0.01</span>) <span class="comment">#定义了一个使用随机梯度下降（SGD）算法进行模型参数优化的优化器</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>,ylabel=<span class="string">&#x27;loss&#x27;</span>,yscale=<span class="string">&#x27;log&#x27;</span>,xlim=[<span class="number">1</span>,num_epochs],ylim=[<span class="number">1e-3</span>,<span class="number">1e2</span>],legend=[<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;test&#x27;</span>]) <span class="comment">#绘制训练损失和测试损失随着训练迭代次数的变化趋势</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net,train_iter,loss,trainer)</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch +<span class="number">1</span>,(evaluate_loss(net,train_iter,loss),evaluate_loss(net,test_iter,loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>,net[<span class="number">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure>

<p><strong>三阶多项式函数拟合</strong></p>
<p>四个输入分别是: 前n_train(100)行4列作为训练集,n_train(100)行后面所有行4列作为测试集,labels前n_train行作为train_labels(y^),labels的n_train后面所有行作为test_labels(y^)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train,:<span class="number">4</span>],poly_features[n_train:,:<span class="number">4</span>],labels[:n_train],labels[n_train:])</span><br></pre></td></tr></table></figure>

<pre><code>weight: [[ 4.99034    1.1656325 -3.4056838  5.650331 ]]
</code></pre>
<p><img src="/2023/03/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88/output_28_1.svg" alt="svg"></p>
<p>可以看出,w给前4维时拟合的很好</p>
<p><strong>线性函数拟合(欠拟合)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>],labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>

<pre><code>weight: [[3.4050567 3.404248 ]]
</code></pre>
<p><img src="/2023/03/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88/output_31_1.svg" alt="svg"></p>
<p>w只给前2个维度时给的数据不足,欠拟合</p>
<p><strong>高阶多项式函数拟合(过拟合)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br></pre></td></tr></table></figure>

<pre><code>weight: [[ 4.964952    1.2093626  -3.2487652   5.3281727  -0.5115174   1.2561612
  -0.06610011  0.18418686  0.11113311  0.01419835  0.15148187  0.10667927
   0.18928145 -0.13205634  0.18986128  0.16003044 -0.0809902  -0.20620278
  -0.03729296 -0.01795298]]
</code></pre>
<p><img src="/2023/03/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88/output_34_1.svg" alt="svg"></p>
<p>w给了20维度,因为4维之后的参数都是扰动,所以模型过大会导致过拟合</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-线性代数</title>
    <url>/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</url>
    <content><![CDATA[<h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><p>标量运算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line">y = torch.tensor(<span class="number">2.0</span>)</span><br><span class="line">x+y ,  x*y  , x/y  , x**y</span><br></pre></td></tr></table></figure>


<pre><code>(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))
</code></pre>
<p>向量长度,形状</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>


<pre><code>tensor([0, 1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x[<span class="number">3</span>]</span><br></pre></td></tr></table></figure>


<pre><code>tensor(3)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(x) <span class="comment">#x的长度</span></span><br></pre></td></tr></table></figure>


<pre><code>4
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.shape <span class="comment">#x的形状</span></span><br></pre></td></tr></table></figure>


<pre><code>torch.Size([4])
</code></pre>
<p>矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">A</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.T <span class="comment">#矩阵转置</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0,  4,  8, 12, 16],
        [ 1,  5,  9, 13, 17],
        [ 2,  6, 10, 14, 18],
        [ 3,  7, 11, 15, 19]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">B = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">0</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">B</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[1, 2, 3],
        [2, 0, 4],
        [3, 4, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">B == B.T <span class="comment">#矩阵比较</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<pre><code>tensor([[True, True, True],
        [True, True, True],
        [True, True, True]])
</code></pre>
<p>张量</p>
<p>具有相同形状的张量,他们按元素二元运算的结果是同一形状的张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>,dtype=torch.float32).reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">B = A.clone()</span><br><span class="line">A,A+B</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [12., 13., 14., 15.],
         [16., 17., 18., 19.]]),
 tensor([[ 0.,  2.,  4.,  6.],
         [ 8., 10., 12., 14.],
         [16., 18., 20., 22.],
         [24., 26., 28., 30.],
         [32., 34., 36., 38.]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A*B</span><br><span class="line"><span class="comment">#Hadamard积,按元素进行分别求积</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([[  0.,   1.,   4.,   9.],
        [ 16.,  25.,  36.,  49.],
        [ 64.,  81., 100., 121.],
        [144., 169., 196., 225.],
        [256., 289., 324., 361.]])
</code></pre>
<p>张量乘或加一个标量不会改变张量的形状,因为每个元素都与标量乘或加</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">2</span></span><br><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,-<span class="number">1</span>)</span><br><span class="line">a + X , (a * X).shape </span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[[ 2,  3,  4,  5],
          [ 6,  7,  8,  9],
          [10, 11, 12, 13]],
 
         [[14, 15, 16, 17],
          [18, 19, 20, 21],
          [22, 23, 24, 25]]]),
 torch.Size([2, 3, 4]))
</code></pre>
<h1 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h1><p>计算元素和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>,dtype = torch.float32)</span><br><span class="line">x, x.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([0., 1., 2., 3.]), tensor(6.))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.shape,A.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>


<pre><code>(torch.Size([5, 4]), tensor(190.))
</code></pre>
<p>axis&#x3D;n,该张量的第n维消失,表现在形状中就是第形状的第n个数没了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) <span class="comment">#A的形状是[5,4],第0维消失后形状变为[4]</span></span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([40., 45., 50., 55.]), torch.Size([4]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>,<span class="number">1</span>]) <span class="comment">#[0,1]维都消失相当于计算元素和</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor(190.)
</code></pre>
<p>平均值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean()</span><br></pre></td></tr></table></figure>


<pre><code>tensor(9.5000)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>()/A.numel()</span><br></pre></td></tr></table></figure>


<pre><code>tensor(9.5000)
</code></pre>
<p>计算平均值是可以沿指定轴降低张量维度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean(axis=<span class="number">0</span>) <span class="comment">#axis=0,表示在第0维降低张量,求平均值,结果的形状是[4]</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([ 8.,  9., 10., 11.])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / A.shape[<span class="number">0</span>]  <span class="comment">#=a.mean(axis=0)</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([ 8.,  9., 10., 11.])
</code></pre>
<p>非降维求和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>) <span class="comment">#用keepdims参数来保持轴数不变</span></span><br><span class="line">sum_A,sum_A.shape</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[ 6.],
         [22.],
         [38.],
         [54.],
         [70.]]),
 torch.Size([5, 1]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A/sum_A  <span class="comment">#通过广播sum_A被拓展到和A一样大,所以能够相除</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([[0.0000, 0.1667, 0.3333, 0.5000],
        [0.1818, 0.2273, 0.2727, 0.3182],
        [0.2105, 0.2368, 0.2632, 0.2895],
        [0.2222, 0.2407, 0.2593, 0.2778],
        [0.2286, 0.2429, 0.2571, 0.2714]])
</code></pre>
<p>求A元素的累积总和,axis&#x3D;0表示按行进行累积,A.cumsum(axis&#x3D;0)中的每一个数表示该列到这里数的累计和,例如4行0列的元素40&#x3D;0+4+8+12+16</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.cumsum(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  6.,  8., 10.],
        [12., 15., 18., 21.],
        [24., 28., 32., 36.],
        [40., 45., 50., 55.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])
</code></pre>
<p>点积</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.ones(<span class="number">4</span>, dtype = torch.float32)</span><br><span class="line">x , y , torch.dot(x,y)</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))
</code></pre>
<p>也可以按元素乘法后进行求和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(x*y)</span><br></pre></td></tr></table></figure>


<pre><code>tensor(6.)
</code></pre>
<p>矩阵-向量积</p>
<p>矩阵乘列向量的结果为一个列向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A , x , A.shape, x.shape , torch.mv(A,x) , torch.mv(A,x).shape</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [12., 13., 14., 15.],
         [16., 17., 18., 19.]]),
 tensor([0., 1., 2., 3.]),
 torch.Size([5, 4]),
 torch.Size([4]),
 tensor([ 14.,  38.,  62.,  86., 110.]),
 torch.Size([5]))
</code></pre>
<p>矩阵-矩阵乘法</p>
<p>torch.mm()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">B = torch.ones(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">torch.mm(A,B)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 6.,  6.,  6.],
        [22., 22., 22.],
        [38., 38., 38.],
        [54., 54., 54.],
        [70., 70., 70.]])
</code></pre>
<p>L2范数</p>
<p>L2范数相当于距离,是各元素平方和的平方根</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure>


<pre><code>tensor(5.)
</code></pre>
<p>L1范数</p>
<p>L1范数是各元素绝对值之和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>


<pre><code>tensor(7.)
</code></pre>
<p>矩阵的Frobenius范数</p>
<p>是矩阵元素平方和的平方根,与L2范数类似</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.norm(torch.ones(<span class="number">4</span>,<span class="number">9</span>) )</span><br></pre></td></tr></table></figure>


<pre><code>tensor(6.)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.ones((<span class="number">4</span>,<span class="number">9</span>))</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])
</code></pre>
<p>范数在机器学习中通常作为项目之间的距离:<br>最大化不同项目之间的距离.<br>最小化相似项目之间的距离.</p>
<p>练习</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.T.T == A</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">B = torch.arange(<span class="number">20</span>)</span><br><span class="line">B = B.reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">A,B</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [12., 13., 14., 15.],
         [16., 17., 18., 19.]]),
 tensor([[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11],
         [12, 13, 14, 15],
         [16, 17, 18, 19]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(A+B).T == (A.T + B.T)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[True, True, True, True, True],
        [True, True, True, True, True],
        [True, True, True, True, True],
        [True, True, True, True, True]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">C = torch.arange(<span class="number">16</span>)</span><br><span class="line">C = C.reshape(<span class="number">4</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">C == C.T</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ True, False, False, False],
        [False,  True, False, False],
        [False, False,  True, False],
        [False, False, False,  True]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">C + C.T == (C+C.T).T</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>)</span><br><span class="line">X = X.reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">X</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(X)</span><br></pre></td></tr></table></figure>


<pre><code>2
</code></pre>
<p>len(X)的结果是第一维的长度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A/A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<pre><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

~\AppData\Local\Temp\ipykernel_10048\3337381381.py in &lt;module&gt;
----&gt; 1 A/A.sum(axis=1)


RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1
</code></pre>
<p>A&#x2F;A.sum(axis&#x3D;1)会报错,因为A.sum(axis&#x3D;1)在降维后并没有保持轴数不变,所以A和A.sum(axis&#x3D;1)轴数不同,A有两个轴,A.sum(axis&#x3D;1)只有一个轴,此时并不会触发广播机制.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[12, 14, 16, 18],
        [20, 22, 24, 26],
        [28, 30, 32, 34]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[12, 15, 18, 21],
        [48, 51, 54, 57]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>(axis=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 6, 22, 38],
        [54, 70, 86]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">D = torch.ones(<span class="number">27</span>)</span><br><span class="line">E = torch.ones(<span class="number">81</span>)</span><br><span class="line">D = D.reshape(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">E = E.reshape(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">D,E</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],
 
         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],
 
         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]),
 tensor([[[[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]]],
</code></pre>
<p>​    </p>
<pre><code>         [[[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]]],




         [[[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]]]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.linalg.norm(D)</span><br></pre></td></tr></table></figure>


<pre><code>5.196152
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.linalg.norm(E)</span><br></pre></td></tr></table></figure>


<pre><code>9.0
</code></pre>
<p>计算得到矩阵的范数</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习环境配置2</title>
    <url>/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/</url>
    <content><![CDATA[<span id="more"></span>

<h4 id="参考教程与视频"><a href="#参考教程与视频" class="headerlink" title="参考教程与视频"></a>参考教程与视频</h4><p>本文主要参考李沐在B站发布的视频:<a href="https://www.bilibili.com/video/BV18K411w7Vs/?spm_id_from=333.999.0.0&vd_source=4d05ddde14c10a4b5df542785dd6efc5">Windows 下安装 CUDA 和 Pytorch 跑深度学习 - 动手学深度学习v2_哔哩哔哩_bilibili</a></p>
<h4 id="看前提醒"><a href="#看前提醒" class="headerlink" title="看前提醒"></a>看前提醒</h4><ol>
<li>本文基于Windows环境安装.</li>
<li>本文安装GPU版本的pytorch，不含独立显卡的电脑不能安装GUP版本pytorch.</li>
<li></li>
</ol>
<h4 id="检查电脑显卡"><a href="#检查电脑显卡" class="headerlink" title="检查电脑显卡"></a>检查电脑显卡</h4><p><code>windows + r</code> 后输入 <code>dxdiag</code> 打开 <code>DirectX诊断工具</code> ,选择显示栏,查看是否有独立显卡.</p>
<h4 id="下载CUDA"><a href="#下载CUDA" class="headerlink" title="下载CUDA"></a>下载CUDA</h4><p>在CUDA官网中找到想要下载的CUDA版本和下载方式,我选择的是11 Version,下载方式为：exe(local)。选择好后点击 download 开始下载.</p>
<p>下载地址：<a href="https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local">CUDA Toolkit 12.1 Downloads | NVIDIA Developer</a></p>
<p><img src="/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/image-20230320205838829.png" alt="image-20230320205838829"></p>
<p>下载完成后解压并默认安装。</p>
<p>安装后点击 <code>win + r</code> ,输入 <code>cmd</code> 进入命令行界面,输入<code>nvidia-smi</code> 检查 CUDA 是否安装成功.</p>
<p><img src="/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/image-20230320210301689.png" alt="image-20230320210301689"></p>
<h4 id="安装anaconda"><a href="#安装anaconda" class="headerlink" title="安装anaconda"></a>安装anaconda</h4><p>李沐老师在视频中安装的是 miniconda ,因为 anaconda 有图形化界面可以操作,我选择下载 anaconda.</p>
<p>搜索 anaconda 进入 anaconda 官网,选择 download 进行下载.下载完成后选择默认进行安装.</p>
<p>安装完成后打开 anaconda prompt 输入 python 查看能否进入 python 环境.</p>
<p>参考网址:<a href="https://www.anaconda.com/">https://www.anaconda.com</a></p>
<h4 id="安装PyTorch"><a href="#安装PyTorch" class="headerlink" title="安装PyTorch"></a>安装PyTorch</h4><p>搜索 pytorch 进入 PyTorch 官方网站, 点击 install 后选择:</p>
<table>
<thead>
<tr>
<th>选项</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>PyTorch Build</td>
<td>Stable(2.0.0)</td>
</tr>
<tr>
<td>Your OS</td>
<td>Windows</td>
</tr>
<tr>
<td>Package</td>
<td>Pip</td>
</tr>
<tr>
<td>Language</td>
<td>Python</td>
</tr>
<tr>
<td>Compute Platform</td>
<td>CUDA 11.8</td>
</tr>
</tbody></table>
<p>选择完后将下方 <code>Run this Command:</code> 栏的命令复制到 anaconda prompt 中回车安装.</p>
<p>注意:</p>
<ul>
<li>李沐老师推荐安装与 CUDA版本一致的 PyTorch 版本,就算不完全一致也应该选择大版本一致的 PyTorch 版本.</li>
<li>安装时应关闭代理,我在打开代理时安装多次安装不成功.</li>
</ul>
<p>安装完成后输入 python 进入 python 环境,输入以下代码检测是否成功安装:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.ones((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">a = a.cuda(<span class="number">0</span>)</span><br><span class="line">b = torch.ones((<span class="number">3</span>,<span class="number">1</span>)).cuda(<span class="number">0</span>)</span><br><span class="line">a + b</span><br></pre></td></tr></table></figure>

<p>看到如下输出则成功安装:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>]],device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="下载课程"><a href="#下载课程" class="headerlink" title="下载课程"></a>下载课程</h4><p>进入动手学深度学习官网:<a href="https://zh.d2l.ai/">《动手学深度学习》 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p>
<p>点击 <code>Jupyter 记事本</code> 下载课程内容.下载完成后解压到本地.</p>
<p><img src="/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/image-20230320212516743.png" alt="image-20230320212516743"></p>
<p>打开 Anaconda Prompt ,输入如下命令,安装所需要的包.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install jupyter d2l</span><br></pre></td></tr></table></figure>

<p>最后,打开课程所在目录, 按  <code>shift + 鼠标右键</code> 选择在此处打开 Powershell 窗口,在命令行中输入 <code>jupyter notebook</code> 就可以成功打开课程的记事本!</p>
<p><img src="/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/image-20230320213115708.png" alt="image-20230320213115708"></p>
<p><img src="/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/image-20230320213324026.png" alt="image-20230320213324026"></p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-线性回归</title>
    <url>/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>y^ &#x3D; Xw + b</p>
<p>损失函数</p>
<p>$$l^{(i)}(\mathbf{w}, b) &#x3D; \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$</p>
<p>n个样本的平均损失函数</p>
<p>$$L(\mathbf{w}, b) &#x3D;\frac{1}{n}\sum_{i&#x3D;1}^n l^{(i)}(\mathbf{w}, b) &#x3D;\frac{1}{n} \sum_{i&#x3D;1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$</p>
<p>目的是找到一组参数,使训练样本上的总损失最小:</p>
<p>$$\mathbf{w}^*, b^* &#x3D; \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).$$</p>
<p>矢量化加速</p>
<p>对于矢量的计算不再用for单独遍历,而是直接调用线性代数库进行矢量计算能够更快</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n = <span class="number">10000</span></span><br><span class="line">a = torch.ones([n])</span><br><span class="line">b = torch.ones([n])</span><br></pre></td></tr></table></figure>

<p>定义计数器来测量运行时间</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Timer</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;记录多次运行时间&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.times = []</span><br><span class="line">        self.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;启动计时器&quot;&quot;&quot;</span></span><br><span class="line">        self.tik = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">stop</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;停止计时器并将时间记录在列表中&quot;&quot;&quot;</span></span><br><span class="line">        self.times.append(time.time() - self.tik)</span><br><span class="line">        <span class="keyword">return</span> self.times[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">avg</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回平均时间&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(self.times) / <span class="built_in">len</span>(self.times)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sum</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回时间总和&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(self.times)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cumsum</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回累计时间&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> np.array(self.times).cumsum().tolist()</span><br></pre></td></tr></table></figure>

<p>使用for循环执行加法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = torch.zeros(n)</span><br><span class="line">timer = Timer()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    c[i] = a[i] + b[i]</span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.5</span>f&#125;</span> sec&#x27;</span></span><br></pre></td></tr></table></figure>


<pre><code>&#39;0.13959 sec&#39;
</code></pre>
<p>使用重载的+来求和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">timer.start()</span><br><span class="line">d = a + b</span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.5</span>f&#125;</span> sec&#x27;</span></span><br></pre></td></tr></table></figure>


<pre><code>&#39;0.00127 sec&#39;
</code></pre>
<p>正态分布</p>
<p>计算正态分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">x,mu,sigma</span>):</span><br><span class="line">    p = <span class="number">1</span> / math.sqrt(<span class="number">2</span>*math.pi*sigma**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> p *np.exp(-<span class="number">0.5</span>/sigma**<span class="number">2</span>*(x-mu)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>可视化正态分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(-<span class="number">7</span>,<span class="number">7</span>,<span class="number">0.01</span>)</span><br><span class="line">x</span><br><span class="line">params = [(<span class="number">0</span>,<span class="number">1</span>),(<span class="number">0</span>,<span class="number">2</span>),(<span class="number">3</span>,<span class="number">1</span>)]</span><br><span class="line">d2l.plot(x,[normal(x,mu,sigma) <span class="keyword">for</span> mu, sigma <span class="keyword">in</span> params], xlabel=<span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">         ylabel=<span class="string">&#x27;p(x)&#x27;</span>, figsize=(<span class="number">4.5</span>, <span class="number">2.5</span>),</span><br><span class="line">        legend=[<span class="string">f&#x27;mean <span class="subst">&#123;mu&#125;</span>, std <span class="subst">&#123;sigma&#125;</span>&#x27;</span> <span class="keyword">for</span> mu, sigma <span class="keyword">in</span> params])</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_16_0.svg" alt="svg"></p>
<p>线性回归从零开始实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>生成数据集</p>
<p>使用带噪声的线性模型构造人造数据集.生成一个包含1000个样本的数据集,每个样本包含从正态分布采样的两个特征.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w,b,num_examples</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>,<span class="number">1</span>,(num_examples,<span class="built_in">len</span>(w)))</span><br><span class="line">    y = torch.matmul(X,w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>,<span class="number">0.01</span>,y.shape)</span><br><span class="line">    <span class="keyword">return</span> X,y.reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>x通过随机生成,w,b人为设定,将设定的w,b输入到生成数据集的函数synthetic_data()中,随机生成x和对应的y.也就是假定了线性函数,并通过这个线性函数和噪声生成数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w = torch.tensor([<span class="number">2</span>,-<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features,labels = synthetic_data(true_w,true_b,<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;features:&#x27;</span>,features[<span class="number">0</span>],<span class="string">&#x27;\nlabel:&#x27;</span>,labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>features: tensor([-0.2833, -1.9873]) 
label: tensor([10.3827])
</code></pre>
<p>观察features[0]和labels的散点图,可以看到两者之间有线性关系</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.set_figsize()</span><br><span class="line">d2l.plt.scatter(features[:,<span class="number">0</span>].detach().numpy(),labels.detach().numpy(),<span class="number">1</span>);</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_25_0.svg" alt="svg"></p>
<p>读取数据集</p>
<p>data_iter 函数可以随机抽取生成数据集中的部分数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size,features,labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,num_examples,batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">        indices[i:<span class="built_in">min</span>(i + batch_size,num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices],labels[batch_indices]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size,features,labels):</span><br><span class="line">    <span class="built_in">print</span>(X,<span class="string">&#x27;\n&#x27;</span>,y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.6035,  0.9162],
        [ 1.7614,  1.0970],
        [-0.3809, -2.1023],
        [-1.7844, -0.4626],
        [ 1.1887, -1.6525],
        [-0.8178,  1.2127],
        [-0.2247,  0.8144],
        [ 0.4035,  1.8231],
        [ 0.3496, -0.2373],
        [ 0.4382, -0.9849]]) 
 tensor([[-0.1049],
        [ 3.9912],
        [10.5857],
        [ 2.1945],
        [12.1976],
        [-1.5510],
        [ 0.9946],
        [-1.2080],
        [ 5.7239],
        [ 8.4225]])
</code></pre>
<p>初始化模型参数w,b.初始化为正态分布的随机值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.normal(<span class="number">0</span>,<span class="number">0.01</span>,size=(<span class="number">2</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>定义模型</p>
<p>模型就是y&#x3D;xw+b</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X,w,b</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X,w) + b</span><br></pre></td></tr></table></figure>

<p>定义损失函数<strong>squared_loss()</strong></p>
<p>损失函数&#x3D; $\frac{1}{2} (\hat{y}-y)^2$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat,y</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape))**<span class="number">2</span>/<span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>定义优化算法</p>
<p>小批量梯度下降算法<strong>sgd()</strong>,参数每次-学习率*梯度&#x2F;样本量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params,lr,batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>指定训练参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">num_epochs = <span class="number">15</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size,features,labels):</span><br><span class="line">        l = loss(net(X,w,b),y)  <span class="comment">#计算损失函数</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()  <span class="comment">#求梯度</span></span><br><span class="line">        sgd([w,b],lr,batch_size) <span class="comment">#更新梯度</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment">#在梯度为0条件下,计算三次epoch的损失,对训练进行评价</span></span><br><span class="line">        train_1 = loss(net(features,w,b),labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>,loss <span class="subst">&#123;<span class="built_in">float</span>(train_1.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>epoch1,loss 2.184030
epoch2,loss 0.284604
epoch3,loss 0.037409
epoch4,loss 0.004994
epoch5,loss 0.000712
epoch6,loss 0.000142
epoch7,loss 0.000067
epoch8,loss 0.000056
epoch9,loss 0.000055
epoch10,loss 0.000055
epoch11,loss 0.000055
epoch12,loss 0.000055
epoch13,loss 0.000055
epoch14,loss 0.000055
epoch15,loss 0.000055
</code></pre>
<p>比较训练数据和真实数据的值,评价训练效果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差:<span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的误差:<span class="subst">&#123;true_b-b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的估计误差:tensor([-0.0004, -0.0001], grad_fn=&lt;SubBackward0&gt;)
b的误差:tensor([-0.0004], grad_fn=&lt;RsubBackward1&gt;)
</code></pre>
<p>线性回归的简洁实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>生成数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>

<p>读取数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays,batch_size,is_train=<span class="literal">True</span></span>): <span class="comment">#@save</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset,batch_size,shuffle=is_train)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features,labels),batch_size)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br></pre></td></tr></table></figure>




<pre><code>[tensor([[-2.0785, -1.1821],
         [-0.2558,  1.3086],
         [-0.6385,  0.1244],
         [-2.0940,  0.6876],
         [-0.2174,  1.4855],
         [ 0.3694, -0.7484],
         [ 0.6117, -0.2025],
         [-0.5963,  0.7489],
         [ 1.3280,  1.1161],
         [-0.9048,  0.0519]]),
 tensor([[ 4.0596],
         [-0.7617],
         [ 2.4987],
         [-2.3301],
         [-1.2802],
         [ 7.4839],
         [ 6.1274],
         [ 0.4620],
         [ 3.0419],
         [ 2.2106]])]
</code></pre>
<p>定义模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>初始化参数w,b为随机数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>,<span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0.])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.MSELoss() <span class="comment">#调用pytorch中直接有的均方误差</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(),lr = <span class="number">0.03</span>)<span class="comment">#直接调用pytorch中的sgd算法</span></span><br></pre></td></tr></table></figure>

<p>训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X),y) <span class="comment">#求损失函数</span></span><br><span class="line">        trainer.zero_grad() <span class="comment">#梯度清零</span></span><br><span class="line">        l.backward()  <span class="comment">#求梯度</span></span><br><span class="line">        trainer.step() <span class="comment">#更新模型</span></span><br><span class="line">    l = loss(net(features),labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>,loss:<span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>epoch 1,loss:0.000285
epoch 2,loss:0.000097
epoch 3,loss:0.000098
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = net[<span class="number">0</span>].weight.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w的估计误差：&#x27;</span>, true_w - w.reshape(true_w.shape))</span><br><span class="line">b = net[<span class="number">0</span>].bias.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b的估计误差：&#x27;</span>, true_b - b)</span><br></pre></td></tr></table></figure>

<pre><code>w的估计误差： tensor([0.0005, 0.0004])
b的估计误差： tensor([0.0006])
</code></pre>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-暂退法</title>
    <url>/2023/03/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9A%82%E9%80%80%E6%B3%95/</url>
    <content><![CDATA[<h1 id="丢弃法-暂退法-dropout"><a href="#丢弃法-暂退法-dropout" class="headerlink" title="丢弃法(暂退法)dropout"></a>丢弃法(暂退法)dropout</h1><p>丢弃法也是正则化统计模型的一种方法.相当于在计算后续层之前向网络的每一层注入噪音,使函数平滑(函数对输入的噪音具有适应性).在标准暂退法正则化中,通过下列方式消除偏差:</p>
<p>$$<br>\begin{aligned}<br>h’ &#x3D;<br>\begin{cases}<br>    0 &amp; \text{ 概率为 } p \<br>    \frac{h}{1-p} &amp; \text{ 其他情况}<br>\end{cases}<br>\end{aligned}<br>$$</p>
<p>在实践中暂退法相当于使多层感知机隐藏层中的部分隐藏单元删除,新的神经元网络是原始神经元网络的子集.</p>
<p>测试时为了保证结果的确定性不会使用暂退法,暂退法只在训练时使用.<br>或者在测试时使用暂退法来估计神经网络预测的稳定性.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X,dropout</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= dropout &lt;=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> dropout ==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">if</span> dropout ==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    mask = (torch.rand(X.shape) &gt;dropout).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> mask * X / (<span class="number">1.0</span> - dropout)</span><br></pre></td></tr></table></figure>

<p>测试dropout函数的执行结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>,dtype = torch.float32).reshape((<span class="number">2</span>,<span class="number">8</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X,<span class="number">0.</span>))</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X,<span class="number">0.5</span>))</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X,<span class="number">1.</span>))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
tensor([[ 0.,  0.,  4.,  6.,  0.,  0.,  0., 14.],
        [ 0., 18., 20., 22., 24.,  0.,  0.,  0.]])
tensor([[0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0.]])
</code></pre>
<p>定义模型参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs,num_outputs,num_hiddens1,num_hiddens2 = <span class="number">784</span>,<span class="number">10</span>,<span class="number">256</span>,<span class="number">256</span></span><br></pre></td></tr></table></figure>

<p>定义模型</p>
<p>常见的技巧是在靠近输入层的地方设置较低的暂退概率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dropout1,dropout2 = <span class="number">0.2</span>,<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_inputs,num_outputs,num_hiddens1,num_hiddens2,is_training = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Net,self).__init__()</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.training = is_training</span><br><span class="line">        self.lin1 = nn.Linear(num_inputs,num_hiddens1)</span><br><span class="line">        self.lin2 = nn.Linear(num_hiddens1,num_hiddens2)</span><br><span class="line">        self.lin3 = nn.Linear(num_hiddens2,num_outputs)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,X</span>):</span><br><span class="line">        H1 = self.relu(self.lin1(X.reshape((-<span class="number">1</span>,self.num_inputs))))</span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            H1 = dropout_layer(H1,dropout1)</span><br><span class="line">        H2 = self.relu(self.lin2(H1))</span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            H2 = dropout_layer(H2,dropout2)</span><br><span class="line">        out = self.lin3(H2)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">net = Net(num_inputs,num_outputs,num_hiddens1,num_hiddens2)</span><br></pre></td></tr></table></figure>

<p>训练和测试</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs,lr, batch_size = <span class="number">10</span>,<span class="number">0.5</span>,<span class="number">256</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction = <span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(),lr=lr)</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9A%82%E9%80%80%E6%B3%95/output_13_0.svg" alt="svg"></p>
<p>​    </p>
<p>简洁实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>,<span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Dropout(dropout1),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>,<span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Dropout(dropout2),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight,std=<span class="number">0.01</span>)</span><br><span class="line">        </span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></table></figure>

<p>对模型进行训练和测试</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(),lr=lr)</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9A%82%E9%80%80%E6%B3%95/output_17_0.svg" alt="svg"></p>
<p>​    </p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-权重衰减</title>
    <url>/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</url>
    <content><![CDATA[<h1 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h1><p>是一种正则化模型.用来调整函数的复杂性,使模型的复杂度达到合适的平衡位置.</p>
<p>我们用函数和f&#x3D;0这个函数的距离来衡量函数的复杂度,而函数的距离不能精确测量,</p>
<p>所以我们通过限制权重向量来衡量,最常用的方法是将权重向量的范数作为惩罚项加入到 <strong>最小化损失</strong> 中,将预测损失调整为:最小化预测损失和惩罚项之和.</p>
<p>原来的损失函数:<br>$$L(\mathbf{w}, b) &#x3D; \frac{1}{n}\sum_{i&#x3D;1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$</p>
<p>现在的损失函数:<br>$$L(\mathbf{w}, b) + \frac{\lambda}{2} |\mathbf{w}|^2,$$</p>
<p>小批量随机梯度下降更新公式:<br>$$<br>\begin{aligned}<br>\mathbf{w} &amp; \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).<br>\end{aligned}<br>$$</p>
<p>此时可以看到权重向量更新时,我们根据估计值与观测值的差异更新w,试图将w的大小缩小到0,所以叫权重衰减.$\lambda$ 小时对应较少约束的w,较大的 $\lambda$对w约束更大.</p>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80%E5%AF%B9%E6%9C%80%E4%BC%98%E8%A7%A3%E7%9A%84%E5%BD%B1%E5%93%8D.png" alt="image.png"></p>
<p>w*是无权重衰退的最优解,~w*是有权重衰退的最优解,权重衰退使模型的最优解时的w减小</p>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80%E5%AF%B9%E6%B5%8B%E8%AF%95%E9%9B%86%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E6%9B%B2%E7%BA%BF%E7%9A%84%E5%BD%B1%E5%93%8D.png" alt="image.png"></p>
<p>由上图可以看出,增加权重衰退后可以减轻模型的过拟合现象,使得函数不会突增突降,使得函数更平滑</p>
<h2 id="高维线性回归"><a href="#高维线性回归" class="headerlink" title="高维线性回归"></a>高维线性回归</h2><p>通过这个例子演示权重衰退</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>



<p>随机生成数据,数据符合下面公式:<br><strong>$$y &#x3D; 0.05 + \sum_{i &#x3D; 1}^d 0.01 x_i + \epsilon \text{ where }<br>\epsilon \sim \mathcal{N}(0, 0.01^2).$$</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_train, n_test, num_inputs,batch_size = <span class="number">20</span>,<span class="number">100</span>,<span class="number">200</span>,<span class="number">5</span></span><br><span class="line">true_w,true_b = torch.ones((num_inputs,<span class="number">1</span>))*<span class="number">0.01</span>,<span class="number">0.05</span> <span class="comment">#真实的w有200维,都初始化为0.01</span></span><br><span class="line">train_data = d2l.synthetic_data(true_w,true_b,n_train)</span><br><span class="line">train_iter = d2l.load_array(train_data,batch_size)</span><br><span class="line">test_data = d2l.synthetic_data(true_w,true_b,n_test)</span><br><span class="line">test_iter = d2l.load_array(test_data,batch_size,is_train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>初始化模型参数 的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>():</span><br><span class="line">    w = torch.normal(<span class="number">0</span>,<span class="number">1</span>,size=(num_inputs,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w,b]</span><br></pre></td></tr></table></figure>

<p>定义L2惩罚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">l2_penalty</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(w.<span class="built_in">pow</span>(<span class="number">2</span>)) /<span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>训练代码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">lambd</span>):</span><br><span class="line">    w,b = init_params()</span><br><span class="line">    net,loss = <span class="keyword">lambda</span> X: d2l.linreg(X,w,b), d2l.squared_loss</span><br><span class="line">    num_epochs,lr=<span class="number">100</span>,<span class="number">0.003</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>,ylabel=<span class="string">&#x27;loss&#x27;</span>,yscale=<span class="string">&#x27;log&#x27;</span>,xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X),y)+lambd*l2_penalty(w)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            d2l.sgd([w,b],lr,batch_size)</span><br><span class="line">        <span class="keyword">if</span>(epoch+<span class="number">1</span>) %<span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch +<span class="number">1</span>, (d2l.evaluate_loss(net,train_iter,loss),d2l.evaluate_loss(net,test_iter,loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数是:&#x27;</span>,torch.norm(w).item())</span><br></pre></td></tr></table></figure>

<p>忽略正则化直接训练:<br>lambd &#x3D; 0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(lambd=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数是: 12.700843811035156
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_19_1.svg" alt="svg"></p>
<p>​    </p>
<p>可以看到训练集的loss一直在下降,而测试机的loss一直不变,说明模型复杂数据过少时出现了过拟合现象</p>
<p>使用权重衰减</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(lambd=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数是: 0.3482859432697296
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_22_1.svg" alt="svg"></p>
<p>​    </p>
<p>可以看到随着训练集的loss下降,测试集的loss也在下降,缓解过拟合现象有一定效果,但测试集的loss仍然比较大</p>
<h2 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.data.normal_() <span class="comment">#对张量进行正态分布的初始化</span></span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    num_epochs,lr = <span class="number">100</span>,<span class="number">0.003</span></span><br><span class="line">    trainer = torch.optim.SGD([</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].weight,<span class="string">&#x27;weight_decay&#x27;</span>:wd&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].bias&#125;],lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>,ylabel=<span class="string">&#x27;loss&#x27;</span>,yscale=<span class="string">&#x27;log&#x27;</span>,xlim=[<span class="number">5</span>,num_epochs],legend=[<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X),y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span>(epoch +<span class="number">1</span> ) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>,(d2l.evaluate_loss(net,train_iter,loss),d2l.evaluate_loss(net,test_iter,loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数:&#x27;</span>,net[<span class="number">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure>

<p>由下列多组lambd值的loss图发现:lambd越大对于解决过拟合效果越好,但lambd过大时会出现抖动,甚至出现欠拟合现象</p>
<p>实际使用权重衰退时多使用$1e^{-3},1e^{-1},1e^{-2}$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_concise(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数: 12.800107955932617
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_27_1.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_concise(<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数: 13.542876243591309
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_34_1.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_concise(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数: 0.38027432560920715
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_28_1.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_concise(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数: 0.07607010006904602
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_29_1.svg" alt="svg"></p>
<p>​     </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_concise(<span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数: 0.026007002219557762
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_31_1.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_concise(<span class="number">50</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数: 0.016941126435995102
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_32_1.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_concise(<span class="number">150</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数: 0.008291924372315407
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_35_1.svg" alt="svg"></p>
<p>​    </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_concise(<span class="number">500</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数: 0.005958488676697016
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_37_1.svg" alt="svg"></p>
<p>​    </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_concise(<span class="number">600</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数: 0.015642918646335602
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_38_1.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_concise(<span class="number">637</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的L2范数: 2.4508492946624756
</code></pre>
<p><img src="/2023/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/output_39_1.svg" alt="svg"></p>
<p>​    </p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-数值稳定性和模型初始化</title>
    <url>/2023/03/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/</url>
    <content><![CDATA[<h1 id="数值稳定性和模型初始化"><a href="#数值稳定性和模型初始化" class="headerlink" title="数值稳定性和模型初始化"></a>数值稳定性和模型初始化</h1><h2 id="数值稳定性"><a href="#数值稳定性" class="headerlink" title="数值稳定性"></a>数值稳定性</h2><p>初始化方案的选择对于保持数值稳定性至关重要,我们还可以将初始化方案与非线性激活函数结合在一起,糟糕的选择可能会带来梯度消失和梯度爆炸</p>
<h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>对于L层神经网络,每一层的输出$h^{(l)}$可以表示为:<br>$$\mathbf{h}^{(l)} &#x3D; f_l (\mathbf{h}^{(l-1)})<br>\text{ 因此输出o可以表示为 } \mathbf{o} &#x3D; f_L \circ \ldots \circ f_1(\mathbf{x}).$$</p>
<p>输出o对任何一层参数$w^{(l)}$的梯度可以表示为:</p>
<p>$\partial_{\mathbf{W}^{(l)} }\mathbf{o}&#x3D;\underbrace{\partial_{\mathbf{h}^ {(L-1)} }\mathbf{h}^{(L)} }<em>{\mathbf{M}^{(L)}\stackrel{\mathrm{def} }{&#x3D;} }\cdot \ldots \cdot \underbrace{\partial</em>{\mathbf{h}^{(l)} } \mathbf{h}^{(l+1)} }<em>{ \mathbf{M}^{(l+1)} \stackrel{\mathrm{def} } {&#x3D;} } \underbrace{\partial</em>{\mathbf{W}^{(l)} } \mathbf{h}^{(l)} }_{\mathbf{v}^{(l)} \stackrel{\mathrm{def} } {&#x3D;} }.$</p>
<p><img src="/2023/03/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/%E6%A2%AF%E5%BA%A6.png" alt="svg"></p>
<p>因为该梯度是多个矩阵的乘积,当神经网络层数很多时,如$(0.8)^{n},(1.1)^{n}$,梯度就会接近0&#x2F;非常大,此时梯度的不稳定性容易威胁到优化算法的稳定性</p>
<p>梯度爆炸的问题:</p>
<ol>
<li>对16位浮点数容易超过表示的区间 $6e^{-5}-6e^{4}$ </li>
<li>对学习率敏感.学习率会被限制在极小的范围内,大的话会导致梯度过大,小的话会导致训练无进展</li>
</ol>
<p>梯度消失的问题:</p>
<ol>
<li>梯度值变为0,也是超过16位浮点数表示的区间</li>
<li>训练没有进展,梯度值变0参数几乎没有改变,没有训练效果</li>
<li>对于较深层的神经网络训练效果很差,因为梯度是反向传播计算,深层的梯度为0就没有训练效果</li>
</ol>
<h3 id="梯度消失例子"><a href="#梯度消失例子" class="headerlink" title="梯度消失例子"></a>梯度消失例子</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">x = torch.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, <span class="number">0.1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.sigmoid(x)</span><br><span class="line">y.backward(torch.ones_like(x))</span><br><span class="line"></span><br><span class="line">d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],</span><br><span class="line">         legend=[<span class="string">&#x27;sigmoid&#x27;</span>, <span class="string">&#x27;gradient&#x27;</span>], figsize=(<span class="number">4.5</span>, <span class="number">2.5</span>))</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/output_8_0.svg" alt="svg"></p>
<p>​    </p>
<p>如图可以看出,当sigmoid函数的输入很大或很小时,梯度都会接近0,梯度消失.</p>
<h3 id="梯度爆炸例子"><a href="#梯度爆炸例子" class="headerlink" title="梯度爆炸例子"></a>梯度爆炸例子</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">M = torch.normal(<span class="number">0</span>, <span class="number">1</span>, size=(<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;一个矩阵 \n&#x27;</span>,M)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    M = torch.mm(M,torch.normal(<span class="number">0</span>, <span class="number">1</span>, size=(<span class="number">4</span>, <span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;乘以100个矩阵后\n&#x27;</span>, M)</span><br></pre></td></tr></table></figure>

<pre><code>一个矩阵 
 tensor([[-1.5478,  0.8112,  0.8132, -0.7599],
        [-0.9211,  1.0384,  1.1417,  0.0916],
        [-0.3887,  1.1294,  1.5851,  0.5011],
        [-1.2066, -1.5591, -0.0459, -1.0900]])
乘以100个矩阵后
 tensor([[ 1.4470e+24,  3.7913e+24, -1.6832e+25, -4.9491e+23],
        [ 1.1020e+24,  2.8874e+24, -1.2819e+25, -3.7690e+23],
        [-8.9897e+24, -2.3554e+25,  1.0457e+26,  3.0749e+24],
        [ 4.9775e+25,  1.3042e+26, -5.7901e+26, -1.7025e+25]])
</code></pre>
<p>从上图可以看出,即使初始矩阵的元素值并不大,但经过100次相乘后,矩阵的元素值变得非常大</p>
<h2 id="模型初始化和激活函数"><a href="#模型初始化和激活函数" class="headerlink" title="模型初始化和激活函数"></a>模型初始化和激活函数</h2><p>目标:<br>让梯度值在合理的范围内</p>
<p>方法:</p>
<ol>
<li>将乘法变加法:ResNet,LSTM</li>
<li>归一化:梯度归一化,梯度裁剪</li>
<li>合理的<strong>权重初始</strong>和<strong>激活函数</strong></li>
</ol>
<h3 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h3><p>目标:让每层的方差是一个常数</p>
<p>让每层的(<strong>输出</strong>和<strong>梯度</strong>)的(<strong>均值</strong>和<strong>方差</strong>)保持一致</p>
<p>权重初始化</p>
<p>在合理区间内随机初始参数,因为训练开始的时候容易有数值不稳定,为了实现每层的方差是一个常数,需要满足下面两个条件:<br>$n_{t-1}\gamma_t &#x3D; 1 (输出的均值和方差不变的近似解)$ </p>
<p>$n_{t} \gamma_t  &#x3D; 1  (梯度的均值和方差不变的近似解)$</p>
<p>然而这两个条件很难同时满足,所以权衡之下,<strong>Xavier方法</strong>:</p>
<p>$\gamma_{t}(n_{t-1}+n_{t})&#x2F;2 &#x3D; 1 (其中\gamma 是权重的方差,n_{t-1}是输入的数量,n_{t}是输出的数量)$</p>
<p>$\sigma &#x3D; \sqrt{\frac{2}{n_{t-1} + n_{t}}} (其中\sigma 是权重w的标准差)$</p>
<p>所以满足这个标准差的初始参数就近似满足输出和梯度的均值和方差保持一致</p>
<p>此时初始参数w可以满足下列分布:</p>
<p>正态分布:$ \boldsymbol W^{[l]} \sim N(\mu&#x3D;0,\sigma^2&#x3D;\frac{1}{n^{[l-1]}})$</p>
<p>均匀分布: $\boldsymbol W\sim U[-\frac{\sqrt 6}{\sqrt {n_{in}+n_{out}}},\frac{\sqrt6}{\sqrt {n_{in}+n_{out}}}]$</p>
<h3 id="假设线性的激活函数"><a href="#假设线性的激活函数" class="headerlink" title="假设线性的激活函数"></a>假设线性的激活函数</h3><p>假设激活函数为 $\sigma(x) &#x3D; \alpha x + \beta$,为了保证让每层的(<strong>输出</strong>和<strong>梯度</strong>)的(<strong>均值</strong>和<strong>方差</strong>)保持一致,需要 $\alpha &#x3D; 1,\beta &#x3D; 0$</p>
<p>为了让激活函数的(<strong>输出</strong>和<strong>梯度</strong>)的(<strong>均值</strong>和<strong>方差</strong>)保持一致,需要修改部分的激活函数:</p>
<p>检查常用的激活函数;</p>
<p>使用泰勒展开:</p>
<p>$ (不满足)sigmoid(x) &#x3D; \frac{1}{2}+ \frac{x}{4} - \frac{x^{3}}{48} + \omicron (x^{5}) $ </p>
<p>$(满足)tanh(x) &#x3D; 0 + x - \frac{x^{3}}{3} + \omicron (x^{5})$</p>
<p>$(满足) relu(x) &#x3D; 0 + x $</p>
<p>调整$sigmoid$使之满足输出和梯度保持一致的条件:<br>$4 * sigmoid(x) - 2$</p>
<p><img src="/2023/03/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF.png" alt="img"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>合理的权重初始值和激活函数能够提升数值稳定性,但不能使数值一定稳定</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-二手车交易价格预测-线性模型</title>
    <url>/2023/03/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="二手车交易价格预测-线性模型"><a href="#二手车交易价格预测-线性模型" class="headerlink" title="二手车交易价格预测-线性模型"></a>二手车交易价格预测-线性模型</h1><p>本次套用李沐老师预测房价的模型代码,经过调整适配于二手车交易价格预测,虽然最后预测效果不好,不过可以用于练习学过的线性模型</p>
<h2 id="一-导包"><a href="#一-导包" class="headerlink" title="一.导包"></a>一.导包</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> requests</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<h2 id="二-下载数据集"><a href="#二-下载数据集" class="headerlink" title="二.下载数据集"></a>二.下载数据集</h2><p>本来应该从网络上下载,不过下载后的数据只有一列,用pandas处理比较麻烦,所以我直接下载到本地用excel预先处理,从网络下载的代码在后面附出</p>
<h3 id="1-从本地导入数据"><a href="#1-从本地导入数据" class="headerlink" title="1.从本地导入数据"></a>1.从本地导入数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">&#x27;F://d2l-zh/pytorch/data/used_car_train_20200313.csv&#x27;</span> )</span><br><span class="line">test_data = pd.read_csv(<span class="string">&#x27;F://d2l-zh/pytorch/data/used_car_testB_20200421.csv&#x27;</span> )</span><br></pre></td></tr></table></figure>

<h3 id="2-从网络下载数据"><a href="#2-从网络下载数据" class="headerlink" title="2.从网络下载数据"></a>2.从网络下载数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line">DATA_HUB = <span class="built_in">dict</span>()</span><br><span class="line">DATA_URL1 = <span class="string">&#x27;https://tianchi-race-prod-sh.oss-cn-shanghai.aliyuncs.com/file/race/documents/231784/used_car_train_20200313.zip?Expires=1680139786&amp;OSSAccessKeyId=LTAI5t7fj2oKqzKgLGz6kGQc&amp;Signature=6NVCaSeNQmjOjHQhufO%2F9CpKwkg%3D&amp;response-content-disposition=attachment%3B%20&#x27;</span></span><br><span class="line">DATA_URL2 = <span class="string">&#x27;https://tianchi-race-prod-sh.oss-cn-shanghai.aliyuncs.com/file/race/documents/231784/used_car_testB_20200421.zip?Expires=1680139718&amp;OSSAccessKeyId=LTAI5t7fj2oKqzKgLGz6kGQc&amp;Signature=d%2FbMvvlSkA%2Bz8tGj8xyNlrChWYc%3D&amp;response-content-disposition=attachment%3B%20&#x27;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">download</span>(<span class="params">name, cache_dir=os.path.join(<span class="params"><span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span></span>)</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载一个DATA_HUB中的文件，返回本地文件名&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> name <span class="keyword">in</span> DATA_HUB, <span class="string">f&quot;<span class="subst">&#123;name&#125;</span> 不存在于 <span class="subst">&#123;DATA_HUB&#125;</span>&quot;</span></span><br><span class="line">    url, sha1_hash = DATA_HUB[name]</span><br><span class="line">    os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#fname = os.path.join(cache_dir, url.split(&#x27;/&#x27;)[-1])</span></span><br><span class="line">    fname = os.path.join(url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>].split(<span class="string">&#x27;?&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(fname):</span><br><span class="line">        sha1 = hashlib.sha1()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                data = f.read(<span class="number">1048576</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sha1.update(data)</span><br><span class="line">        <span class="keyword">if</span> sha1.hexdigest() == sha1_hash:</span><br><span class="line">            <span class="keyword">return</span> fname  <span class="comment"># 命中缓存</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;正在从<span class="subst">&#123;url&#125;</span>下载<span class="subst">&#123;fname&#125;</span>...&#x27;</span>)</span><br><span class="line">    r = requests.get(url, stream=<span class="literal">True</span>, verify=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(r.content)</span><br><span class="line">    <span class="keyword">return</span> fname</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">download_extract</span>(<span class="params">name, folder=<span class="literal">None</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载并解压zip/tar文件&quot;&quot;&quot;</span></span><br><span class="line">    fname = download(name)</span><br><span class="line">    base_dir = os.path.dirname(fname)</span><br><span class="line">    data_dir, ext = os.path.splitext(fname)</span><br><span class="line">    <span class="keyword">if</span> ext == <span class="string">&#x27;.zip&#x27;</span>:</span><br><span class="line">        fp = zipfile.ZipFile(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> ext <span class="keyword">in</span> (<span class="string">&#x27;.tar&#x27;</span>, <span class="string">&#x27;.gz&#x27;</span>):</span><br><span class="line">        fp = tarfile.<span class="built_in">open</span>(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="literal">False</span>, <span class="string">&#x27;只有zip/tar文件可以被解压缩&#x27;</span></span><br><span class="line">    fp.extractall(base_dir)</span><br><span class="line">    <span class="keyword">return</span> os.path.join(base_dir, folder) <span class="keyword">if</span> folder <span class="keyword">else</span> data_dir</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_all</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载DATA_HUB中的所有文件&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> DATA_HUB:</span><br><span class="line">        download(name)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DATA_HUB[<span class="string">&#x27;tianchi_car_train&#x27;</span>] = (  <span class="comment">#@save</span></span><br><span class="line">    DATA_URL1 ,</span><br><span class="line">    <span class="string">&#x27;585e9cc93e70b39160e7921475f9bcd7d31219ce&#x27;</span>)</span><br><span class="line"></span><br><span class="line">DATA_HUB[<span class="string">&#x27;tianchi_car_test&#x27;</span>] = (  <span class="comment">#@save</span></span><br><span class="line">    DATA_URL2 ,</span><br><span class="line">    <span class="string">&#x27;fa19780a7b011d9b009e8bff8e99922a8ee2eb90&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = pd.read_csv(download_extract(<span class="string">&#x27;tianchi_car_train&#x27;</span>) )</span><br><span class="line">test_data = pd.read_csv(download_extract(<span class="string">&#x27;tianchi_car_test&#x27;</span>) )</span><br></pre></td></tr></table></figure>

<h2 id="三-处理数据集"><a href="#三-处理数据集" class="headerlink" title="三.处理数据集"></a>三.处理数据集</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_data.shape)</span><br><span class="line"><span class="built_in">print</span>(test_data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(150000, 31)
(50000, 30)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除掉会使log()变成nan的行</span></span><br><span class="line"><span class="comment">#例子:df.drop(df[(df.score &lt; 50) &amp; (df.score &gt; 20)].index， inplace=True)</span></span><br><span class="line">train_data.drop(train_data[(train_data.price &lt; <span class="number">1</span>)].index, inplace=<span class="literal">True</span>) </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将去掉id和y的训练集,测试集数据连在一起,方便以后使用</span></span><br><span class="line">all_features = pd.concat( (train_data.drop([<span class="string">&#x27;SaleID&#x27;</span>, <span class="string">&#x27;price&#x27;</span>], axis=<span class="number">1</span>), test_data.drop([<span class="string">&#x27;SaleID&#x27;</span>], axis=<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(all_features.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(198397, 29)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features.info()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 198397 entries, 0 to 49999
Data columns (total 29 columns):
 #   Column             Non-Null Count   Dtype  
---  ------             --------------   -----  
 0   name               198397 non-null  int64  
 1   regDate            198397 non-null  int64  
 2   model              198397 non-null  int64  
 3   brand              198397 non-null  int64  
 4   bodyType           198397 non-null  int64  
 5   fuelType           198397 non-null  float64
 6   gearbox            198397 non-null  object 
 7   power              198397 non-null  object 
 8   kilometer          198397 non-null  object 
 9   notRepairedDamage  198397 non-null  object 
 10  regionCode         198397 non-null  int64  
 11  seller             198397 non-null  float64
 12  offerType          198397 non-null  float64
 13  creatDate          198397 non-null  float64
 14  v_0                198397 non-null  float64
 15  v_1                198397 non-null  float64
 16  v_2                198397 non-null  float64
 17  v_3                198397 non-null  float64
 18  v_4                198397 non-null  float64
 19  v_5                198397 non-null  float64
 20  v_6                198397 non-null  float64
 21  v_7                198397 non-null  float64
 22  v_8                198397 non-null  float64
 23  v_9                198397 non-null  float64
 24  v_10               198397 non-null  float64
 25  v_11               198397 non-null  float64
 26  v_12               196435 non-null  float64
 27  v_13               195188 non-null  float64
 28  v_14               181210 non-null  float64
dtypes: float64(19), int64(6), object(4)
memory usage: 45.4+ MB
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将dtype=object 的数值更改为数值型数据,否则这些数据会被当作字符串处理</span></span><br><span class="line"><span class="comment">#例子df_valid[&quot;val&quot;] = pd.to_numeric(df_valid[&quot;val&quot;],errors=&#x27;coerce&#x27;)</span></span><br><span class="line">all_features[<span class="string">&quot;gearbox&quot;</span>] = pd.to_numeric(all_features[<span class="string">&quot;gearbox&quot;</span>],errors=<span class="string">&#x27;coerce&#x27;</span>)</span><br><span class="line">all_features[<span class="string">&quot;power&quot;</span>] = pd.to_numeric(all_features[<span class="string">&quot;power&quot;</span>],errors=<span class="string">&#x27;coerce&#x27;</span>)</span><br><span class="line">all_features[<span class="string">&quot;kilometer&quot;</span>] = pd.to_numeric(all_features[<span class="string">&quot;kilometer&quot;</span>],errors=<span class="string">&#x27;coerce&#x27;</span>)</span><br><span class="line">all_features[<span class="string">&quot;notRepairedDamage&quot;</span>] = pd.to_numeric(all_features[<span class="string">&quot;notRepairedDamage&quot;</span>],errors=<span class="string">&#x27;coerce&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 找出所有的数值型数据,进行标准化</span></span><br><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">&#x27;object&#x27;</span>].index</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x - x.mean()) / (x.std()))</span><br><span class="line"><span class="comment"># 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#all_features = all_features.select_dtypes(exclude=[&#x27;float&#x27;, &#x27;int&#x27;]).fillna(0)</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(all_features[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<pre><code>       name   regDate     model     brand  bodyType  fuelType   gearbox  \
0 -1.103673  0.113972 -0.349400 -0.261700 -0.171899 -0.105626 -0.079676   
1 -1.078674 -0.074366 -0.147912 -0.896951  0.023307 -0.105626 -0.079676   
2 -0.872068  0.113991  1.363247  0.881754 -0.171899 -0.105626 -0.079676   
3  0.061542 -1.368231  1.242354  0.246502 -0.367106 -0.105626 -0.066799   
4  0.703951  1.600035  1.262503 -0.388750 -0.171899 -0.105626 -0.079676   

      power  kilometer  notRepairedDamage  ...       v_5       v_6       v_7  \
0 -0.227020  -0.062821          -0.232539  ... -0.065283  0.299142 -0.049734   
1 -0.395822  -0.053310           0.000000  ...  0.258353  0.441721 -0.043089   
2  0.062756  -0.062821          -0.232539  ...  0.109697  0.396048 -0.011474   
3  0.147157  -0.053310          -0.232539  ...  0.364179  0.361465 -0.057885   
4 -0.204513  -0.091354          -0.232539  ... -0.150251  0.083322 -0.090217   

        v_8       v_9      v_10      v_11      v_12      v_13      v_14  
0 -0.131639 -0.141617 -0.720934  1.620970 -1.032394  0.610606  0.862376  
1 -0.129301 -0.188182 -1.261222  1.268456 -0.418342 -1.376912  0.226001  
2 -0.107303 -0.184249 -1.246841  1.122631  0.728116 -0.674415 -0.226134  
3 -0.125098 -0.200648 -1.156604  0.864820 -0.184876 -1.941822 -0.462656  
4 -0.097010 -0.127036 -0.457153  0.677966  0.448008  2.220238  1.821559  

[5 rows x 29 columns]
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#划分训练集和测试集</span></span><br><span class="line">n_train = train_data.shape[<span class="number">0</span>]</span><br><span class="line">train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)</span><br><span class="line">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)</span><br><span class="line">train_labels = torch.tensor(</span><br><span class="line">    train_data.price.values.reshape(-<span class="number">1</span>, <span class="number">1</span>), dtype=torch.float32)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设置模型超参数</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">in_features = train_features.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    net = nn.Sequential(nn.Linear(in_features,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>

<p>用均方根误差来衡量差异</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log_rmse</span>(<span class="params">net, features, labels</span>):</span><br><span class="line">    <span class="comment"># 为了在取对数时进一步稳定该值，将小于1的值设置为1</span></span><br><span class="line">    clipped_preds = torch.clamp(net(features), <span class="number">1</span>, <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">    rmse = torch.sqrt(loss(torch.log(clipped_preds),</span><br><span class="line">                           torch.log(labels)))</span><br><span class="line">    <span class="keyword">return</span> rmse.item()</span><br></pre></td></tr></table></figure>

<p>用adam优化器实现小批量随机梯度下降</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_features, train_labels, test_features, test_labels,</span></span><br><span class="line"><span class="params">          num_epochs, learning_rate, weight_decay, batch_size</span>):</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels), batch_size)</span><br><span class="line">    <span class="comment"># 这里使用的是Adam优化算法</span></span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(),</span><br><span class="line">                                 lr = learning_rate,</span><br><span class="line">                                 weight_decay = weight_decay)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        train_ls.append(log_rmse(net, train_features, train_labels))</span><br><span class="line">        <span class="keyword">if</span> test_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            test_ls.append(log_rmse(net, test_features, test_labels))</span><br><span class="line">    <span class="keyword">return</span> train_ls, test_ls</span><br></pre></td></tr></table></figure>

<p>将训练集划分为k折需要的数据:一份验证集,k-1份训练集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_k_fold_data</span>(<span class="params">k, i, X, y</span>):</span><br><span class="line">    <span class="keyword">assert</span> k &gt; <span class="number">1</span></span><br><span class="line">    fold_size = X.shape[<span class="number">0</span>] // k</span><br><span class="line">    X_train, y_train = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        idx = <span class="built_in">slice</span>(j * fold_size, (j + <span class="number">1</span>) * fold_size)</span><br><span class="line">        X_part, y_part = X[idx, :], y[idx]</span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            X_valid, y_valid = X_part, y_part</span><br><span class="line">        <span class="keyword">elif</span> X_train <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            X_train, y_train = X_part, y_part</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X_train = torch.cat([X_train, X_part], <span class="number">0</span>)</span><br><span class="line">            y_train = torch.cat([y_train, y_part], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, y_train, X_valid, y_valid</span><br></pre></td></tr></table></figure>

<p>k折交叉验证训练代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">k_fold</span>(<span class="params">k, X_train, y_train, num_epochs, learning_rate, weight_decay,</span></span><br><span class="line"><span class="params">           batch_size</span>):</span><br><span class="line">    train_l_sum, valid_l_sum = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        data = get_k_fold_data(k, i, X_train, y_train)</span><br><span class="line">        net = get_net()</span><br><span class="line">        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,</span><br><span class="line">                                   weight_decay, batch_size)</span><br><span class="line">        train_l_sum += train_ls[-<span class="number">1</span>]</span><br><span class="line">        valid_l_sum += valid_ls[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            d2l.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>)), [train_ls, valid_ls],</span><br><span class="line">                     xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;rmse&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                     legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>], yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;折<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>，训练log rmse:<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;验证log rmse:<span class="subst">&#123;<span class="built_in">float</span>(valid_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> train_l_sum / k, valid_l_sum / k</span><br></pre></td></tr></table></figure>

<p>设置训练超参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span> ,<span class="number">100</span> , <span class="number">2</span>, <span class="number">0.1</span>, <span class="number">64</span></span><br></pre></td></tr></table></figure>

<p>进行训练,训练时调参,找到最优的参数用来后面拿到完整的训练集上进行训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span>,<span class="number">100</span>, <span class="number">5</span>, <span class="number">0.1</span>, <span class="number">64</span></span><br><span class="line">train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,</span><br><span class="line">                          weight_decay, batch_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;k&#125;</span>-折验证: 平均训练log rmse: <span class="subst">&#123;<span class="built_in">float</span>(train_l):f&#125;</span>, &#x27;</span></span><br><span class="line">      <span class="string">f&#x27;平均验证log rmse: <span class="subst">&#123;<span class="built_in">float</span>(valid_l):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>折1，训练log rmse:2.859440, 验证log rmse:2.838621
折2，训练log rmse:2.829567, 验证log rmse:2.825060
折3，训练log rmse:2.786797, 验证log rmse:2.801329
折4，训练log rmse:2.848498, 验证log rmse:2.866435
折5，训练log rmse:2.880568, 验证log rmse:2.871375
5-折验证: 平均训练log rmse: 2.840974, 平均验证log rmse: 2.840564
</code></pre>
<p><img src="/2023/03/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/output_36_1.svg" alt="svg"></p>
<p>​    </p>
<p>不使用k折交叉验证时的训练模型</p>
<p>当测试集上的预测与k折交叉验证的预测相似时就可以将生成的csv文件上传到竞赛</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_and_pred</span>(<span class="params">train_features, test_features, train_labels, test_data,</span></span><br><span class="line"><span class="params">                   num_epochs, lr, weight_decay, batch_size</span>):</span><br><span class="line">    net = get_net()</span><br><span class="line">    train_ls, _ = train(net, train_features, train_labels, <span class="literal">None</span>, <span class="literal">None</span>,</span><br><span class="line">                        num_epochs, lr, weight_decay, batch_size)</span><br><span class="line">    d2l.plot(np.arange(<span class="number">1</span>, num_epochs + <span class="number">1</span>), [train_ls], xlabel=<span class="string">&#x27;epoch&#x27;</span>,</span><br><span class="line">             ylabel=<span class="string">&#x27;log rmse&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;训练log rmse：<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将网络应用于测试集。</span></span><br><span class="line">    preds = net(test_features).detach().numpy()</span><br><span class="line">    <span class="comment"># 将其重新格式化以导出到Kaggle</span></span><br><span class="line">    test_data[<span class="string">&#x27;price&#x27;</span>] = pd.Series(preds.reshape(<span class="number">1</span>, -<span class="number">1</span>)[<span class="number">0</span>])</span><br><span class="line">    submission = pd.concat([test_data[<span class="string">&#x27;SaleID&#x27;</span>], test_data[<span class="string">&#x27;price&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">    submission.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_and_pred(train_features, test_features, train_labels, test_data,</span><br><span class="line">               num_epochs, lr, weight_decay, batch_size)</span><br></pre></td></tr></table></figure>

<pre><code>训练log rmse：2.835125
</code></pre>
<p><img src="/2023/03/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/output_39_1.svg" alt="svg"></p>
<p>​    </p>
<p>线性模型分析:线性模型对于二手车交易价格预测这个项目明显不合适,数据量有两万条,过大,而模型过于简单,loss经过调参最低也有2.8左右,明显欠拟合.</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-二手车交易价格预测-多层感知机</title>
    <url>/2023/03/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    <content><![CDATA[<h1 id="二手车交易价格预测-多层感知机"><a href="#二手车交易价格预测-多层感知机" class="headerlink" title="二手车交易价格预测-多层感知机"></a>二手车交易价格预测-多层感知机</h1><p>前面数据的导入与线性模型相同,数据处理经过启发应该筛选掉相关性不强的列,在这里进行调整</p>
<h2 id="一-导包"><a href="#一-导包" class="headerlink" title="一.导包"></a>一.导包</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<h2 id="二-下载数据"><a href="#二-下载数据" class="headerlink" title="二.下载数据"></a>二.下载数据</h2><p>这里只演示从本地导入数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">&#x27;F://d2l-zh/pytorch/data/used_car_train_20200313.csv&#x27;</span> )</span><br><span class="line">test_data = pd.read_csv(<span class="string">&#x27;F://d2l-zh/pytorch/data/used_car_testB_20200421.csv&#x27;</span> )</span><br></pre></td></tr></table></figure>

<h2 id="三-处理数据"><a href="#三-处理数据" class="headerlink" title="三.处理数据"></a>三.处理数据</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#一些废弃的代码</span><br><span class="line">#将dtype=object 的数值更改为数值型数据,否则这些数据会被当作字符串处理</span><br><span class="line"></span><br><span class="line">#例子df_valid[&quot;val&quot;] = pd.to_numeric(df_valid[&quot;val&quot;],errors=&#x27;coerce&#x27;)</span><br><span class="line"></span><br><span class="line">train_data[&quot;gearbox&quot;] = pd.to_numeric(train_data[&quot;gearbox&quot;],errors=&#x27;coerce&#x27;)</span><br><span class="line">train_data[&quot;power&quot;] = pd.to_numeric(train_data[&quot;power&quot;],errors=&#x27;coerce&#x27;)</span><br><span class="line">train_data[&quot;kilometer&quot;] = pd.to_numeric(train_data[&quot;kilometer&quot;],errors=&#x27;coerce&#x27;)</span><br><span class="line">train_data[&quot;notRepairedDamage&quot;] = pd.to_numeric(train_data[&quot;notRepairedDamage&quot;],errors=&#x27;coerce&#x27;)</span><br></pre></td></tr></table></figure>

<p>展示数据的shape</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_data.shape)</span><br><span class="line"><span class="built_in">print</span>(test_data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(150000, 31)
(50000, 30)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除掉会使log()变成nan的行</span></span><br><span class="line"><span class="comment">#例子:df.drop(df[(df.score &lt; 50) &amp; (df.score &gt; 20)].index， inplace=True)</span></span><br><span class="line">train_data.drop(train_data[(train_data.price &lt; <span class="number">1</span>)].index, inplace=<span class="literal">True</span>) </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将去掉id和y的训练集,测试集数据连在一起,方便以后使用</span></span><br><span class="line">all_features = pd.concat( (train_data.drop([<span class="string">&#x27;SaleID&#x27;</span>, <span class="string">&#x27;price&#x27;</span>], axis=<span class="number">1</span>), test_data.drop([<span class="string">&#x27;SaleID&#x27;</span>], axis=<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(all_features.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(198397, 29)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features.info()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 198397 entries, 0 to 49999
Data columns (total 29 columns):
 #   Column             Non-Null Count   Dtype  
---  ------             --------------   -----  
 0   name               198397 non-null  int64  
 1   regDate            198397 non-null  int64  
 2   model              198397 non-null  int64  
 3   brand              198397 non-null  int64  
 4   bodyType           198397 non-null  int64  
 5   fuelType           198397 non-null  float64
 6   gearbox            198397 non-null  object 
 7   power              198397 non-null  object 
 8   kilometer          198397 non-null  object 
 9   notRepairedDamage  198397 non-null  object 
 10  regionCode         198397 non-null  int64  
 11  seller             198397 non-null  float64
 12  offerType          198397 non-null  float64
 13  creatDate          198397 non-null  float64
 14  v_0                198397 non-null  float64
 15  v_1                198397 non-null  float64
 16  v_2                198397 non-null  float64
 17  v_3                198397 non-null  float64
 18  v_4                198397 non-null  float64
 19  v_5                198397 non-null  float64
 20  v_6                198397 non-null  float64
 21  v_7                198397 non-null  float64
 22  v_8                198397 non-null  float64
 23  v_9                198397 non-null  float64
 24  v_10               198397 non-null  float64
 25  v_11               198397 non-null  float64
 26  v_12               196435 non-null  float64
 27  v_13               195188 non-null  float64
 28  v_14               181210 non-null  float64
dtypes: float64(19), int64(6), object(4)
memory usage: 45.4+ MB
</code></pre>
<p>将所有 ‘ - ‘ 都替换为nan</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features.replace(<span class="string">&#x27;-&#x27;</span>, np.nan, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>查看替换效果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features[<span class="string">&#x27;notRepairedDamage&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure>


<pre><code>0       144307
1        16755
486         76
70          73
1530        63
         ...  
1766         1
7569         1
6533         1
5213         1
5158         1
Name: notRepairedDamage, Length: 4790, dtype: int64
</code></pre>
<p>去掉倾斜特别严重的列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features[<span class="string">&quot;seller&quot;</span>].value_counts()</span><br></pre></td></tr></table></figure>


<pre><code>0.000000       195187
300.000000         61
1000.000000        61
100.000000         57
500.000000         54
                ...  
866.000000          1
35.520548           1
33.721656           1
35.211289           1
33.552816           1
Name: seller, Length: 814, dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features[<span class="string">&quot;offerType&quot;</span>].value_counts()</span><br></pre></td></tr></table></figure>


<pre><code> 0.000000e+00    181210
 2.016031e+07       565
 2.016031e+07       537
 2.016032e+07       528
 2.016031e+07       526
                  ...  
 4.279370e+01         1
-3.688516e+00         1
 3.545805e+01         1
-3.705080e+00         1
-3.697117e+00         1
Name: offerType, Length: 2896, dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">del</span> all_features[<span class="string">&quot;seller&quot;</span>]</span><br><span class="line"><span class="keyword">del</span> all_features[<span class="string">&quot;offerType&quot;</span>]</span><br></pre></td></tr></table></figure>



<p>了解预测值的分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[<span class="string">&#x27;price&#x27;</span>]</span><br></pre></td></tr></table></figure>


<pre><code>0         1850.0
1         3600.0
2         6222.0
3         2400.0
4         5200.0
           ...  
149995    5900.0
149996    9500.0
149997    7500.0
149998    4999.0
149999    4700.0
Name: price, Length: 148397, dtype: float64
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 1) 总体分布概况（无界约翰逊分布等）</span></span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> st</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="comment">#train_features_2 = all_features[:n_train].values</span></span><br><span class="line"><span class="comment">#train_distribution = train_features_2.insert(loc=-1,column=&#x27;price&#x27;, value=train_data.price.values.reshape(-1, 1))</span></span><br><span class="line"><span class="comment">#np.insert(a,0,b,axis=1)</span></span><br><span class="line"><span class="comment">#np.insert(train_features_2,-1,train_data.price.values,axis=1)</span></span><br><span class="line"><span class="comment">#np.insert(train_features_2,0,train_data.SaleID.values,axis=1)</span></span><br><span class="line">y = train_data[<span class="string">&#x27;price&#x27;</span>]</span><br><span class="line">plt.figure(<span class="number">1</span>); plt.title(<span class="string">&#x27;Johnson SU&#x27;</span>)</span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.johnsonsu)</span><br><span class="line">plt.figure(<span class="number">2</span>); plt.title(<span class="string">&#x27;Normal&#x27;</span>)</span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.norm)</span><br><span class="line">plt.figure(<span class="number">3</span>); plt.title(<span class="string">&#x27;Log Normal&#x27;</span>)</span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.lognorm)</span><br><span class="line">![png](output_24_2.png)</span><br></pre></td></tr></table></figure>



<img src="output_24_3.png" alt="png" style="zoom:50%;" />

<p>​    </p>
<img src="output_24_4.png" alt="png" style="zoom:50%;" />

<p>​    </p>
<p>查看price的分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 3) 查看预测值的具体频数</span></span><br><span class="line">plt.hist(train_data[<span class="string">&#x27;price&#x27;</span>], orientation = <span class="string">&#x27;vertical&#x27;</span>,histtype = <span class="string">&#x27;bar&#x27;</span>, color =<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<img src="output_26_0.png" alt="png" style="zoom:50%;" />

<p>​    </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># log变换 z之后的分布较均匀，可以进行log变换进行预测，这也是预测问题常用的trick</span></span><br><span class="line">plt.hist(np.log(train_data[<span class="string">&#x27;price&#x27;</span>]), orientation = <span class="string">&#x27;vertical&#x27;</span>,histtype = <span class="string">&#x27;bar&#x27;</span>, color =<span class="string">&#x27;red&#x27;</span>) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<img src="output_27_0.png" alt="png" style="zoom:50%;" />

<p>​    </p>
<p>所以可以把price取log,等预测完后再补回来(最后并没有取log)</p>
<p>相关性分析</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">numeric_features = [<span class="string">&#x27;power&#x27;</span>, <span class="string">&#x27;kilometer&#x27;</span>, <span class="string">&#x27;v_0&#x27;</span>, <span class="string">&#x27;v_1&#x27;</span>, <span class="string">&#x27;v_2&#x27;</span>, <span class="string">&#x27;v_3&#x27;</span>, <span class="string">&#x27;v_4&#x27;</span>, <span class="string">&#x27;v_5&#x27;</span>, <span class="string">&#x27;v_6&#x27;</span>, <span class="string">&#x27;v_7&#x27;</span>, <span class="string">&#x27;v_8&#x27;</span>, <span class="string">&#x27;v_9&#x27;</span>, <span class="string">&#x27;v_10&#x27;</span>, <span class="string">&#x27;v_11&#x27;</span>, <span class="string">&#x27;v_12&#x27;</span>, <span class="string">&#x27;v_13&#x27;</span>,<span class="string">&#x27;v_14&#x27;</span> ]</span><br><span class="line"></span><br><span class="line">categorical_features = [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;model&#x27;</span>, <span class="string">&#x27;brand&#x27;</span>, <span class="string">&#x27;bodyType&#x27;</span>, <span class="string">&#x27;fuelType&#x27;</span>, <span class="string">&#x27;gearbox&#x27;</span>, <span class="string">&#x27;notRepairedDamage&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>,]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">numeric_features.append(<span class="string">&#x27;price&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 1) 相关性分析</span></span><br><span class="line">price_numeric = train_data[numeric_features]</span><br><span class="line">correlation = price_numeric.corr()</span><br><span class="line"><span class="built_in">print</span>(correlation[<span class="string">&#x27;price&#x27;</span>].sort_values(ascending = <span class="literal">False</span>),<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>price    1.000000
v_12     0.757191
v_2      0.339706
v_0      0.331059
v_5      0.210128
v_14     0.027706
v_1      0.017711
v_13    -0.020419
v_8     -0.063720
v_7     -0.065614
v_6     -0.066145
v_9     -0.124748
v_4     -0.158719
v_10    -0.235059
v_11    -0.337444
v_3     -0.712686
Name: price, dtype: float64 
</code></pre>
<p>​    </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f , ax = plt.subplots(figsize = (<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&#x27;Correlation of Numeric Features with Price&#x27;</span>,y=<span class="number">1</span>,size=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">sns.heatmap(correlation,square = <span class="literal">True</span>,  vmax=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure>

<img src="output_33_1.png" alt="png" style="zoom:50%;" />

<p>​    </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">del</span> price_numeric[<span class="string">&#x27;price&#x27;</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 2) 查看几个特征得 偏度和峰值</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> numeric_features:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#123;:15&#125;&#x27;</span>.<span class="built_in">format</span>(col), </span><br><span class="line">          <span class="string">&#x27;Skewness: &#123;:05.2f&#125;&#x27;</span>.<span class="built_in">format</span>(all_features[col].skew()) , </span><br><span class="line">          <span class="string">&#x27;   &#x27;</span> ,</span><br><span class="line">          <span class="string">&#x27;Kurtosis: &#123;:06.2f&#125;&#x27;</span>.<span class="built_in">format</span>(all_features[col].kurt())  </span><br><span class="line">         )</span><br></pre></td></tr></table></figure>

<pre><code>power           Skewness: 17.52     Kurtosis: 456.05
kilometer       Skewness: 18.52     Kurtosis: 382.51
v_0             Skewness: -2.89     Kurtosis: 006.69
v_1             Skewness: 00.88     Kurtosis: 000.90
v_2             Skewness: 00.93     Kurtosis: 003.43
v_3             Skewness: 00.05     Kurtosis: -00.45
v_4             Skewness: 00.32     Kurtosis: -00.05
v_5             Skewness: 04.30     Kurtosis: 060.73
v_6             Skewness: 07.39     Kurtosis: 062.89
v_7             Skewness: 10.68     Kurtosis: 114.42
v_8             Skewness: 09.01     Kurtosis: 083.07
v_9             Skewness: 05.79     Kurtosis: 047.44
v_10            Skewness: 00.45     Kurtosis: 001.47
v_11            Skewness: 00.32     Kurtosis: 000.73
v_12            Skewness: 00.09     Kurtosis: -00.47
v_13            Skewness: 00.22     Kurtosis: -00.36
v_14            Skewness: -1.21     Kurtosis: 002.36
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 3) 每个数字特征得分布可视化</span></span><br><span class="line">f = pd.melt(all_features, value_vars=numeric_features)</span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">&quot;variable&quot;</span>,  col_wrap=<span class="number">2</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>)</span><br><span class="line">g = g.<span class="built_in">map</span>(sns.distplot, <span class="string">&quot;value&quot;</span>)</span><br></pre></td></tr></table></figure>

<img src="output_36_1.png" alt="png" style="zoom:80%;" />

<p>​    </p>
<p>查看匿名特征的分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 4) 数字特征相互之间的关系可视化</span></span><br><span class="line">sns.<span class="built_in">set</span>()</span><br><span class="line">columns = [<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;v_12&#x27;</span>, <span class="string">&#x27;v_8&#x27;</span> , <span class="string">&#x27;v_0&#x27;</span>, <span class="string">&#x27;power&#x27;</span>, <span class="string">&#x27;v_5&#x27;</span>,  <span class="string">&#x27;v_2&#x27;</span>, <span class="string">&#x27;v_6&#x27;</span>, <span class="string">&#x27;v_1&#x27;</span>, <span class="string">&#x27;v_14&#x27;</span>]</span><br><span class="line">sns.pairplot(train_data[columns],height = <span class="number">2</span> ,kind =<span class="string">&#x27;scatter&#x27;</span>,diag_kind=<span class="string">&#x27;kde&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_38_1.png" alt="png"></p>
<p>用pandas_profiling生成一个较为全面的可视化和数据报告</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install pandas_profiling</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas_profiling</span><br><span class="line">pfr = pandas_profiling.ProfileReport(train_data)</span><br><span class="line">pfr.to_file(<span class="string">&quot;./example.html&quot;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将dtype=object 的数值更改为数值型数据,否则这些数据会被当作字符串处理</span></span><br><span class="line"><span class="comment">#例子df_valid[&quot;val&quot;] = pd.to_numeric(df_valid[&quot;val&quot;],errors=&#x27;coerce&#x27;)</span></span><br><span class="line">all_features[<span class="string">&quot;gearbox&quot;</span>] = pd.to_numeric(all_features[<span class="string">&quot;gearbox&quot;</span>],errors=<span class="string">&#x27;coerce&#x27;</span>)</span><br><span class="line">all_features[<span class="string">&quot;power&quot;</span>] = pd.to_numeric(all_features[<span class="string">&quot;power&quot;</span>],errors=<span class="string">&#x27;coerce&#x27;</span>)</span><br><span class="line">all_features[<span class="string">&quot;kilometer&quot;</span>] = pd.to_numeric(all_features[<span class="string">&quot;kilometer&quot;</span>],errors=<span class="string">&#x27;coerce&#x27;</span>)</span><br><span class="line">all_features[<span class="string">&quot;notRepairedDamage&quot;</span>] = pd.to_numeric(all_features[<span class="string">&quot;notRepairedDamage&quot;</span>],errors=<span class="string">&#x27;coerce&#x27;</span>)</span><br></pre></td></tr></table></figure>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#连续特征异常值简单处理</span></span><br><span class="line"><span class="comment">#df.loc[df[&#x27;power&#x27;]&gt;600,&#x27;power&#x27;] = 600</span></span><br><span class="line">all_features.loc[all_features[<span class="string">&#x27;power&#x27;</span>]&gt;<span class="number">600</span>,<span class="string">&#x27;power&#x27;</span>] = <span class="number">600</span></span><br><span class="line"><span class="comment">#车身类型bodyType&gt;7的设置为缺失?</span></span><br><span class="line">all_features.loc[all_features[<span class="string">&#x27;bodyType&#x27;</span>]&gt;<span class="number">7</span>,<span class="string">&#x27;bodyType&#x27;</span>] = np.nan</span><br><span class="line"><span class="comment">#燃油类型fuelType&gt;6的行设置为缺失?</span></span><br><span class="line">all_features.loc[all_features[<span class="string">&#x27;fuelType&#x27;</span>]&gt;<span class="number">6</span>,<span class="string">&#x27;fuelType&#x27;</span>] = np.nan</span><br><span class="line"><span class="comment">#gearbox&gt;1的设置为缺失</span></span><br><span class="line">all_features.loc[all_features[<span class="string">&#x27;gearbox&#x27;</span>]&gt;<span class="number">1</span>,<span class="string">&#x27;gearbox&#x27;</span>] = np.nan</span><br><span class="line"><span class="comment">#notRepairedDamage&gt;1的设置为缺失</span></span><br><span class="line">all_features.loc[all_features[<span class="string">&#x27;notRepairedDamage&#x27;</span>]&gt;<span class="number">1</span>,<span class="string">&#x27;notRepairedDamage&#x27;</span>] = np.nan</span><br><span class="line"><span class="comment">#seller&gt;1的设置为缺失   //已经被删掉了</span></span><br><span class="line"><span class="comment">#all_features.loc[all_features[&#x27;seller&#x27;]&gt;1,&#x27;seller&#x27;] = np.nan</span></span><br><span class="line"><span class="comment">#offerType&gt;1的设置为缺失  //已经被删掉了</span></span><br><span class="line"><span class="comment">#all_features.loc[all_features[&#x27;offerType&#x27;]&gt;1,&#x27;offerType&#x27;] = np.nan</span></span><br><span class="line"><span class="comment">#creatDate&lt;20150807的值设置为缺失</span></span><br><span class="line">all_features.loc[all_features[<span class="string">&#x27;creatDate&#x27;</span>]&lt;<span class="number">20150807</span>,<span class="string">&#x27;creatDate&#x27;</span>] = np.nan</span><br></pre></td></tr></table></figure>



<p>删掉和price相关性不大的列:v_5,v_14,v_1,v_13,v_8,v_7,v_6,v_9,v_4,v_10</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features.drop(<span class="string">&#x27;v_5&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">all_features.drop(<span class="string">&#x27;v_14&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">all_features.drop(<span class="string">&#x27;v_1&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">all_features.drop(<span class="string">&#x27;v_13&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">all_features.drop(<span class="string">&#x27;v_8&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">all_features.drop(<span class="string">&#x27;v_7&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">all_features.drop(<span class="string">&#x27;v_6&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">all_features.drop(<span class="string">&#x27;v_9&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">all_features.drop(<span class="string">&#x27;v_4&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">all_features.drop(<span class="string">&#x27;v_10&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>regDate</th>
      <th>model</th>
      <th>brand</th>
      <th>bodyType</th>
      <th>fuelType</th>
      <th>gearbox</th>
      <th>power</th>
      <th>kilometer</th>
      <th>notRepairedDamage</th>
      <th>...</th>
      <th>v_4</th>
      <th>v_5</th>
      <th>v_6</th>
      <th>v_7</th>
      <th>v_8</th>
      <th>v_9</th>
      <th>v_11</th>
      <th>v_12</th>
      <th>v_13</th>
      <th>v_14</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.103673</td>
      <td>0.113972</td>
      <td>-0.349400</td>
      <td>-0.261700</td>
      <td>-0.428617</td>
      <td>-0.668947</td>
      <td>-0.537451</td>
      <td>-0.689842</td>
      <td>-0.062821</td>
      <td>-0.340743</td>
      <td>...</td>
      <td>0.957081</td>
      <td>-0.065283</td>
      <td>0.299142</td>
      <td>-0.049734</td>
      <td>-0.131639</td>
      <td>-0.141617</td>
      <td>1.620970</td>
      <td>-1.032394</td>
      <td>0.610606</td>
      <td>0.862376</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.078674</td>
      <td>-0.074366</td>
      <td>-0.147912</td>
      <td>-0.896951</td>
      <td>0.139896</td>
      <td>-0.668947</td>
      <td>-0.537451</td>
      <td>-1.402302</td>
      <td>-0.053310</td>
      <td>0.000000</td>
      <td>...</td>
      <td>-1.271041</td>
      <td>0.258353</td>
      <td>0.441721</td>
      <td>-0.043089</td>
      <td>-0.129301</td>
      <td>-0.188182</td>
      <td>1.268456</td>
      <td>-0.418342</td>
      <td>-1.376912</td>
      <td>0.226001</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.872068</td>
      <td>0.113991</td>
      <td>1.363247</td>
      <td>0.881754</td>
      <td>-0.428617</td>
      <td>-0.668947</td>
      <td>-0.537451</td>
      <td>0.533214</td>
      <td>-0.062821</td>
      <td>-0.340743</td>
      <td>...</td>
      <td>-0.901775</td>
      <td>0.109697</td>
      <td>0.396048</td>
      <td>-0.011474</td>
      <td>-0.107303</td>
      <td>-0.184249</td>
      <td>1.122631</td>
      <td>0.728116</td>
      <td>-0.674415</td>
      <td>-0.226134</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.061542</td>
      <td>-1.368231</td>
      <td>1.242354</td>
      <td>0.246502</td>
      <td>-0.997130</td>
      <td>-0.668947</td>
      <td>1.861488</td>
      <td>0.889444</td>
      <td>-0.053310</td>
      <td>-0.340743</td>
      <td>...</td>
      <td>-1.970849</td>
      <td>0.364179</td>
      <td>0.361465</td>
      <td>-0.057885</td>
      <td>-0.125098</td>
      <td>-0.200648</td>
      <td>0.864820</td>
      <td>-0.184876</td>
      <td>-1.941822</td>
      <td>-0.462656</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.703951</td>
      <td>1.600035</td>
      <td>1.262503</td>
      <td>-0.388750</td>
      <td>-0.428617</td>
      <td>-0.668947</td>
      <td>-0.537451</td>
      <td>-0.594848</td>
      <td>-0.091354</td>
      <td>-0.340743</td>
      <td>...</td>
      <td>1.914255</td>
      <td>-0.150251</td>
      <td>0.083322</td>
      <td>-0.090217</td>
      <td>-0.097010</td>
      <td>-0.127036</td>
      <td>0.677966</td>
      <td>0.448008</td>
      <td>2.220238</td>
      <td>1.821559</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>49995</th>
      <td>0.709898</td>
      <td>0.125215</td>
      <td>-0.873268</td>
      <td>-0.515800</td>
      <td>-0.997130</td>
      <td>1.151507</td>
      <td>0.000000</td>
      <td>-1.224187</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.192838</td>
      <td>-2.682987</td>
      <td>0.597682</td>
      <td>-0.106862</td>
      <td>-0.121464</td>
      <td>1.054890</td>
      <td>1.079508</td>
      <td>-0.432896</td>
      <td>0.240641</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>49996</th>
      <td>1.387954</td>
      <td>1.792196</td>
      <td>0.355808</td>
      <td>-0.896951</td>
      <td>-0.997130</td>
      <td>-0.668947</td>
      <td>-0.537451</td>
      <td>0.723203</td>
      <td>-0.095159</td>
      <td>-0.340743</td>
      <td>...</td>
      <td>0.274264</td>
      <td>0.153067</td>
      <td>-0.458147</td>
      <td>-0.021447</td>
      <td>-0.078703</td>
      <td>-0.159558</td>
      <td>-1.414470</td>
      <td>1.922566</td>
      <td>0.093775</td>
      <td>0.521442</td>
    </tr>
    <tr>
      <th>49997</th>
      <td>1.055356</td>
      <td>0.129056</td>
      <td>-0.873268</td>
      <td>-0.515800</td>
      <td>-0.997130</td>
      <td>-0.668947</td>
      <td>1.861488</td>
      <td>0.343225</td>
      <td>-0.062821</td>
      <td>-0.340743</td>
      <td>...</td>
      <td>-0.632764</td>
      <td>0.237846</td>
      <td>-0.463196</td>
      <td>-0.036490</td>
      <td>-0.101246</td>
      <td>-0.174625</td>
      <td>-0.979948</td>
      <td>0.642117</td>
      <td>-0.864207</td>
      <td>0.249849</td>
    </tr>
    <tr>
      <th>49998</th>
      <td>1.233490</td>
      <td>-0.253344</td>
      <td>-0.147912</td>
      <td>-0.896951</td>
      <td>1.276922</td>
      <td>-0.668947</td>
      <td>1.861488</td>
      <td>0.687580</td>
      <td>-0.053310</td>
      <td>-0.340743</td>
      <td>...</td>
      <td>-1.638240</td>
      <td>0.451069</td>
      <td>-0.465405</td>
      <td>-0.116438</td>
      <td>-0.103993</td>
      <td>-0.195194</td>
      <td>-1.239622</td>
      <td>0.288114</td>
      <td>-1.373416</td>
      <td>0.293378</td>
    </tr>
    <tr>
      <th>49999</th>
      <td>0.165353</td>
      <td>1.051952</td>
      <td>-0.309102</td>
      <td>-0.007599</td>
      <td>-0.428617</td>
      <td>-0.668947</td>
      <td>-0.537451</td>
      <td>-1.402302</td>
      <td>-0.098963</td>
      <td>-0.340743</td>
      <td>...</td>
      <td>1.517869</td>
      <td>-0.112291</td>
      <td>0.313830</td>
      <td>-0.085760</td>
      <td>-0.107207</td>
      <td>-0.133914</td>
      <td>1.236599</td>
      <td>0.084987</td>
      <td>1.721624</td>
      <td>0.798387</td>
    </tr>
  </tbody>
</table>
<p>198397 rows × 26 columns</p>
</div>


<p>熟悉数据的相关统计量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>regDate</th>
      <th>model</th>
      <th>brand</th>
      <th>bodyType</th>
      <th>fuelType</th>
      <th>gearbox</th>
      <th>power</th>
      <th>kilometer</th>
      <th>notRepairedDamage</th>
      <th>...</th>
      <th>v_5</th>
      <th>v_6</th>
      <th>v_7</th>
      <th>v_8</th>
      <th>v_9</th>
      <th>v_10</th>
      <th>v_11</th>
      <th>v_12</th>
      <th>v_13</th>
      <th>v_14</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>198397.000000</td>
      <td>1.983970e+05</td>
      <td>198397.000000</td>
      <td>198397.000000</td>
      <td>198176.000000</td>
      <td>196294.000000</td>
      <td>185871.000000</td>
      <td>197710.000000</td>
      <td>192823.000000</td>
      <td>161062.000000</td>
      <td>...</td>
      <td>198397.000000</td>
      <td>198397.000000</td>
      <td>198397.000000</td>
      <td>198397.000000</td>
      <td>198397.000000</td>
      <td>198397.000000</td>
      <td>198397.000000</td>
      <td>196435.000000</td>
      <td>195188.000000</td>
      <td>181210.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>68108.227544</td>
      <td>2.003429e+07</td>
      <td>47.340978</td>
      <td>8.059809</td>
      <td>1.753926</td>
      <td>0.367462</td>
      <td>0.224037</td>
      <td>118.095248</td>
      <td>29.012779</td>
      <td>0.104028</td>
      <td>...</td>
      <td>0.241546</td>
      <td>0.062093</td>
      <td>0.175824</td>
      <td>0.235704</td>
      <td>0.331272</td>
      <td>-0.188182</td>
      <td>-0.450399</td>
      <td>-0.083272</td>
      <td>0.021721</td>
      <td>0.007850</td>
    </tr>
    <tr>
      <th>std</th>
      <td>61043.662967</td>
      <td>5.363231e+04</td>
      <td>49.630792</td>
      <td>7.870895</td>
      <td>1.758975</td>
      <td>0.549314</td>
      <td>0.416851</td>
      <td>84.215254</td>
      <td>262.853174</td>
      <td>0.305298</td>
      <td>...</td>
      <td>0.089920</td>
      <td>0.133367</td>
      <td>0.930455</td>
      <td>1.617201</td>
      <td>1.651006</td>
      <td>3.736296</td>
      <td>2.007745</td>
      <td>2.264203</td>
      <td>1.266889</td>
      <td>1.051645</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>1.991000e+07</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>-0.273510</td>
      <td>-8.206004</td>
      <td>-8.399672</td>
      <td>-9.168192</td>
      <td>-9.404106</td>
      <td>-9.639552</td>
      <td>-6.113291</td>
      <td>-6.546556</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>11049.000000</td>
      <td>1.999100e+07</td>
      <td>11.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>71.000000</td>
      <td>9.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.241090</td>
      <td>0.000153</td>
      <td>0.055976</td>
      <td>0.035915</td>
      <td>0.035213</td>
      <td>-3.671179</td>
      <td>-2.037674</td>
      <td>-1.751599</td>
      <td>-0.999469</td>
      <td>-0.426465</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>51297.000000</td>
      <td>2.003100e+07</td>
      <td>30.000000</td>
      <td>6.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>109.000000</td>
      <td>15.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.256870</td>
      <td>0.001192</td>
      <td>0.090502</td>
      <td>0.058192</td>
      <td>0.063063</td>
      <td>1.226635</td>
      <td>-0.459030</td>
      <td>-0.160096</td>
      <td>0.006096</td>
      <td>0.154983</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>118364.000000</td>
      <td>2.007111e+07</td>
      <td>66.000000</td>
      <td>13.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>150.000000</td>
      <td>15.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.265062</td>
      <td>0.104284</td>
      <td>0.120882</td>
      <td>0.081395</td>
      <td>0.094180</td>
      <td>2.678429</td>
      <td>1.120382</td>
      <td>1.584613</td>
      <td>0.928545</td>
      <td>0.702554</td>
    </tr>
    <tr>
      <th>max</th>
      <td>196812.000000</td>
      <td>2.015121e+07</td>
      <td>247.000000</td>
      <td>39.000000</td>
      <td>7.000000</td>
      <td>6.000000</td>
      <td>1.000000</td>
      <td>600.000000</td>
      <td>8086.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>1.411559</td>
      <td>1.387847</td>
      <td>12.357011</td>
      <td>18.819042</td>
      <td>18.801218</td>
      <td>18.802072</td>
      <td>13.562011</td>
      <td>11.147669</td>
      <td>8.658418</td>
      <td>2.743993</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 27 columns</p>
</div>






<p>判断数据缺失和异常</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features.isnull().<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>name                 0
regDate              0
model                0
brand                0
bodyType             0
fuelType             0
gearbox              0
power                0
kilometer            0
notRepairedDamage    0
regionCode           0
creatDate            0
v_0                  0
v_1                  0
v_2                  0
v_3                  0
v_4                  0
v_5                  0
v_6                  0
v_7                  0
v_8                  0
v_9                  0
v_10                 0
v_11                 0
v_12                 0
v_13                 0
v_14                 0
dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nan可视化</span></span><br><span class="line">missing = all_features.isnull().<span class="built_in">sum</span>()</span><br><span class="line">missing = missing[missing &gt; <span class="number">0</span>]</span><br><span class="line">missing.sort_values(inplace=<span class="literal">True</span>)</span><br><span class="line">missing.plot.bar()</span><br></pre></td></tr></table></figure>



<img src="output_51_1.png" alt="png" style="zoom:50%;" />

<p>​    </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 找出所有的数值型数据,进行标准化</span></span><br><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">&#x27;object&#x27;</span>].index</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x - x.mean()) / (x.std()))</span><br><span class="line"><span class="comment"># 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#all_features = all_features.select_dtypes(exclude=[&#x27;float&#x27;, &#x27;int&#x27;]).fillna(0)</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(all_features[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<pre><code>       name   regDate     model     brand  bodyType  fuelType   gearbox  \
0 -1.103673  0.113972 -0.349400 -0.261700 -0.428617 -0.668947 -0.537451   
1 -1.078674 -0.074366 -0.147912 -0.896951  0.139896 -0.668947 -0.537451   
2 -0.872068  0.113991  1.363247  0.881754 -0.428617 -0.668947 -0.537451   
3  0.061542 -1.368231  1.242354  0.246502 -0.997130 -0.668947  1.861488   
4  0.703951  1.600035  1.262503 -0.388750 -0.428617 -0.668947 -0.537451   

      power  kilometer  notRepairedDamage  ...       v_5       v_6       v_7  \
0 -0.689842  -0.062821          -0.340743  ... -0.065283  0.299142 -0.049734   
1 -1.402302  -0.053310           0.000000  ...  0.258353  0.441721 -0.043089   
2  0.533214  -0.062821          -0.340743  ...  0.109697  0.396048 -0.011474   
3  0.889444  -0.053310          -0.340743  ...  0.364179  0.361465 -0.057885   
4 -0.594848  -0.091354          -0.340743  ... -0.150251  0.083322 -0.090217   

        v_8       v_9      v_10      v_11      v_12      v_13      v_14  
0 -0.131639 -0.141617 -0.720934  1.620970 -1.032394  0.610606  0.862376  
1 -0.129301 -0.188182 -1.261222  1.268456 -0.418342 -1.376912  0.226001  
2 -0.107303 -0.184249 -1.246841  1.122631  0.728116 -0.674415 -0.226134  
3 -0.125098 -0.200648 -1.156604  0.864820 -0.184876 -1.941822 -0.462656  
4 -0.097010 -0.127036 -0.457153  0.677966  0.448008  2.220238  1.821559  

[5 rows x 27 columns]
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#划分训练集和测试集</span></span><br><span class="line">n_train = train_data.shape[<span class="number">0</span>]</span><br><span class="line">train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)</span><br><span class="line">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)</span><br><span class="line">train_labels = torch.tensor(</span><br><span class="line">    train_data.price.values.reshape(-<span class="number">1</span>, <span class="number">1</span>), dtype=torch.float32)</span><br></pre></td></tr></table></figure>



<h2 id="四-建立模型"><a href="#四-建立模型" class="headerlink" title="四.建立模型"></a>四.建立模型</h2><p>参考李沐老师的mlp简洁实现,但因为测试集没有label,所以模型不匹配,最后没有用到</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#设置mlp模型超参数,用Xavier方法初始化模型参数</span><br><span class="line">in_features = train_features.shape[1]</span><br><span class="line">input_size = in_features</span><br><span class="line">hidden_size = 16</span><br><span class="line">output_size = 1</span><br><span class="line">fc1 = nn.Linear(input_size, hidden_size)</span><br><span class="line">fc2 = nn.Linear(hidden_size, output_size)</span><br><span class="line">net = nn.Sequential(nn.Flatten(), #将数据变成一维</span><br><span class="line">                    #nn.Linear(in_features,16), #输入层</span><br><span class="line">                    fc1,</span><br><span class="line">                    nn.ReLU(), #激活函数</span><br><span class="line">                    #nn.Linear(16,1)) #隐藏层</span><br><span class="line">                    fc2)</span><br><span class="line">                  </span><br><span class="line">loss = nn.L1Loss()</span><br><span class="line">batch_size,lr,num_epochs = 256 , 0.1 , 10</span><br><span class="line"></span><br><span class="line">#loss = nn.CrossEntropyLoss(reduction=&#x27;none&#x27;)</span><br><span class="line"></span><br><span class="line">#trainer = torch.optim.Adam(net.parameters(),lr=lr)</span><br><span class="line"></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(),lr=lr)</span><br><span class="line"></span><br><span class="line">import torch.nn.init as init</span><br><span class="line">nn.init.xavier_uniform_(fc1.weight)</span><br><span class="line">nn.init.xavier_uniform_(fc1.weight)</span><br><span class="line">#训练模型</span><br><span class="line"></span><br><span class="line">#因为test没有label,所以还不能直接套这个模型</span><br><span class="line">train_iter = d2l.load_array((train_features, train_labels), batch_size)</span><br><span class="line"></span><br><span class="line">#test_iter=(train_features,train_labels),test_features</span><br><span class="line"></span><br><span class="line">test_iter = d2l.load_array(test_features, batch_size)</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)</span><br></pre></td></tr></table></figure>



<p>因为测试集没有label,所以和简洁实现冲突,只能用从零开始来实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">in_features = train_features.shape[<span class="number">1</span>]</span><br><span class="line">in_features</span><br></pre></td></tr></table></figure>


<pre><code>27
</code></pre>
<p>设置模型超参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">in_features = train_features.shape[<span class="number">1</span>]</span><br><span class="line">input_size = in_features</span><br><span class="line"><span class="comment">#hidden_size = 16</span></span><br><span class="line">hidden_size1 = <span class="number">16</span></span><br><span class="line">hidden_size2 = <span class="number">8</span></span><br><span class="line"><span class="comment">#hidden_size3 = 8</span></span><br><span class="line">output_size = <span class="number">1</span></span><br><span class="line"><span class="comment">#fc1 = nn.Linear(input_size, hidden_size)</span></span><br><span class="line"><span class="comment">#fc2 = nn.Linear(hidden_size, output_size)</span></span><br><span class="line">fc1 = nn.Linear(input_size, hidden_size1)</span><br><span class="line">fc2 = nn.Linear(hidden_size1, hidden_size2)</span><br><span class="line">fc3 = nn.Linear(hidden_size2, output_size)</span><br><span class="line"><span class="comment">#fc4 = nn.Linear(hidden_size3, output_size)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    net = nn.Sequential(nn.Flatten(), <span class="comment">#将数据变成一维</span></span><br><span class="line">                    <span class="comment">#nn.Linear(in_features,16), #输入层</span></span><br><span class="line">                    fc1,</span><br><span class="line">                    nn.ReLU(), <span class="comment">#激活函数</span></span><br><span class="line">                    <span class="comment">#nn.Linear(16,1)) #隐藏层</span></span><br><span class="line">                    fc2,</span><br><span class="line">                    nn.ReLU(), <span class="comment">#激活函数</span></span><br><span class="line">                    fc3</span><br><span class="line">                    <span class="comment">#,nn.ReLU(), #激活函数</span></span><br><span class="line">                    <span class="comment">#fc4   </span></span><br><span class="line">                    )</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">loss = nn.L1Loss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.init <span class="keyword">as</span> init</span><br><span class="line">nn.init.xavier_uniform_(fc1.weight)</span><br><span class="line">nn.init.xavier_uniform_(fc1.weight)</span><br></pre></td></tr></table></figure>


<pre><code>Parameter containing:
tensor([[-3.0024e-01,  2.5385e-01, -3.2322e-01, -2.3297e-01, -6.6149e-02,
          3.5693e-01, -6.4112e-02,  1.6300e-02, -3.6862e-01, -1.5616e-01,
         -3.2649e-01, -2.8663e-01,  2.1795e-01, -2.3374e-01,  1.0426e-01,
         -3.1713e-01, -2.2963e-01, -4.5091e-02,  5.8626e-02, -1.7361e-01,
         -2.8709e-01, -2.1905e-01, -2.8345e-01, -1.0926e-01,  6.2456e-02,
         -1.0301e-01, -8.9153e-02],
        ......(中间太长,省略了一部分输出)......
        [-3.1734e-01,  2.0996e-02,  3.4150e-01,  3.0384e-01, -3.6609e-01,
          2.1981e-01, -2.4452e-01, -1.1950e-03,  1.8479e-01, -1.9983e-01,
          3.3413e-01,  2.1996e-01, -2.8354e-01, -3.2801e-01, -3.2709e-01,
         -2.2412e-01,  2.8793e-01, -1.7312e-01,  2.8064e-01,  7.1320e-02,
         -3.5514e-02,  1.8521e-02, -2.3619e-01,  2.8784e-01,  3.3705e-01,
          2.3145e-01, -1.8625e-01]], requires_grad=True)
</code></pre>
<p>用均方根误差衡量预测效果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log_rmse</span>(<span class="params">net, features, labels</span>):</span><br><span class="line">    <span class="comment"># 为了在取对数时进一步稳定该值，将小于1的值设置为1</span></span><br><span class="line">    clipped_preds = torch.clamp(net(features), <span class="number">1</span>, <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">    rmse = torch.sqrt(loss(torch.log(clipped_preds),</span><br><span class="line">                           torch.log(labels)))</span><br><span class="line">    <span class="keyword">return</span> rmse.item()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_features, train_labels, test_features, test_labels,</span></span><br><span class="line"><span class="params">          num_epochs, learning_rate, weight_decay, batch_size</span>):</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels), batch_size)</span><br><span class="line">    <span class="comment"># 这里使用的是Adam优化算法</span></span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(),</span><br><span class="line">                                 lr = learning_rate,</span><br><span class="line">                                 weight_decay = weight_decay)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        train_ls.append(log_rmse(net, train_features, train_labels))</span><br><span class="line">        <span class="keyword">if</span> test_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            test_ls.append(log_rmse(net, test_features, test_labels))</span><br><span class="line">    <span class="keyword">return</span> train_ls, test_ls</span><br></pre></td></tr></table></figure>



<p>k折交叉验证</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_k_fold_data</span>(<span class="params">k, i, X, y</span>):</span><br><span class="line">    <span class="keyword">assert</span> k &gt; <span class="number">1</span></span><br><span class="line">    fold_size = X.shape[<span class="number">0</span>] // k</span><br><span class="line">    X_train, y_train = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        idx = <span class="built_in">slice</span>(j * fold_size, (j + <span class="number">1</span>) * fold_size)</span><br><span class="line">        X_part, y_part = X[idx, :], y[idx]</span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            X_valid, y_valid = X_part, y_part</span><br><span class="line">        <span class="keyword">elif</span> X_train <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            X_train, y_train = X_part, y_part</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X_train = torch.cat([X_train, X_part], <span class="number">0</span>)</span><br><span class="line">            y_train = torch.cat([y_train, y_part], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, y_train, X_valid, y_valid</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">k_fold</span>(<span class="params">k, X_train, y_train, num_epochs, learning_rate, weight_decay,</span></span><br><span class="line"><span class="params">           batch_size</span>):</span><br><span class="line">    train_l_sum, valid_l_sum = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        data = get_k_fold_data(k, i, X_train, y_train)</span><br><span class="line">        net = get_net()</span><br><span class="line">        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,</span><br><span class="line">                                   weight_decay, batch_size)</span><br><span class="line">        train_l_sum += train_ls[-<span class="number">1</span>]</span><br><span class="line">        valid_l_sum += valid_ls[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            d2l.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>)), [train_ls, valid_ls],</span><br><span class="line">                     xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;rmse&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                     legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>], yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;折<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>，训练log rmse<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;验证log rmse<span class="subst">&#123;<span class="built_in">float</span>(valid_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> train_l_sum / k, valid_l_sum / k</span><br></pre></td></tr></table></figure>



<p>用k折交叉验证来选择超参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span>, <span class="number">20</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">32</span></span><br><span class="line">train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,</span><br><span class="line">                          weight_decay, batch_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;k&#125;</span>-折验证: 平均训练log rmse: <span class="subst">&#123;<span class="built_in">float</span>(train_l):f&#125;</span>, &#x27;</span></span><br><span class="line">      <span class="string">f&#x27;平均验证log rmse: <span class="subst">&#123;<span class="built_in">float</span>(valid_l):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>折1，训练log rmse0.426749, 验证log rmse0.425627
折2，训练log rmse0.416341, 验证log rmse0.418826
折3，训练log rmse0.404806, 验证log rmse0.411668
折4，训练log rmse0.405785, 验证log rmse0.403740
折5，训练log rmse0.405048, 验证log rmse0.407075
5-折验证: 平均训练log rmse: 0.411746, 平均验证log rmse: 0.413387
</code></pre>
<p><img src="/2023/03/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_69_1.svg" alt="svg"></p>
<p>训练数据提交预测</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_and_pred</span>(<span class="params">train_features, test_features, train_labels, test_data,</span></span><br><span class="line"><span class="params">                   num_epochs, lr, weight_decay, batch_size</span>):</span><br><span class="line">    net = get_net()</span><br><span class="line">    train_ls, _ = train(net, train_features, train_labels, <span class="literal">None</span>, <span class="literal">None</span>,</span><br><span class="line">                        num_epochs, lr, weight_decay, batch_size)</span><br><span class="line">    d2l.plot(np.arange(<span class="number">1</span>, num_epochs + <span class="number">1</span>), [train_ls], xlabel=<span class="string">&#x27;epoch&#x27;</span>,</span><br><span class="line">             ylabel=<span class="string">&#x27;log rmse&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;训练log rmse：<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将网络应用于测试集。</span></span><br><span class="line">    preds = net(test_features).detach().numpy()</span><br><span class="line">    <span class="comment"># 将其重新格式化以导出到Kaggle</span></span><br><span class="line">    test_data[<span class="string">&#x27;price&#x27;</span>] = pd.Series(preds.reshape(<span class="number">1</span>, -<span class="number">1</span>)[<span class="number">0</span>])</span><br><span class="line">    submission = pd.concat([test_data[<span class="string">&#x27;SaleID&#x27;</span>], test_data[<span class="string">&#x27;price&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">    submission.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_and_pred(train_features, test_features, train_labels, test_data,</span><br><span class="line">               num_epochs, lr, weight_decay, batch_size)</span><br></pre></td></tr></table></figure>

<pre><code>训练log rmse：0.401439
</code></pre>
<p><img src="/2023/03/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_71_1.svg" alt="svg"></p>
<h2 id="五-模型调参记录"><a href="#五-模型调参记录" class="headerlink" title="五.模型调参记录"></a>五.模型调参记录</h2><p>一层mlp, hidden_size&#x3D;16, k&#x3D;5, num_epochs&#x3D;100, lr&#x3D;0.001, weight_decay&#x3D;0.01, batch_size&#x3D;32</p>
<p>平均验证log rmse &#x3D;0.530819</p>
<p>评价:模型明显不够复杂,现增加模型复杂度.epochs&#x3D;40~60左右就够了,多了loss不再下降</p>
<img src="image-20230330160727999.png" alt="image-20230330160727999" style="zoom:50%;" />

<p>两层mlp, hidden_size1&#x3D;16, hidden_size2&#x3D;8, k&#x3D;5, num_epochs&#x3D;10, lr&#x3D;0.001, weight_decay&#x3D;0.01,</p>
<p>batch_size&#x3D;32</p>
<p>平均验证log rmse &#x3D;0.424043</p>
<p>评价:模型复杂后即使epochs很小loss也减小了,再把epochs增加到50</p>
<img src="image-20230330162512647.png" alt="image-20230330162512647" style="zoom:50%;" />

<p>两层mlp, hidden_size1&#x3D;16, hidden_size2&#x3D;8, k&#x3D;5, num_epochs&#x3D;50, lr&#x3D;0.001, weight_decay&#x3D;0.01,</p>
<p>batch_size&#x3D;32</p>
<p>平均验证log rmse &#x3D;0.396399</p>
<p>评价:和10 epochs 结果差别不大,但是运行时间长了很多,没必要</p>
<img src="image-20230330164508079.png" alt="image-20230330164508079" style="zoom:50%;" />

<p>两层mlp, hidden_size1&#x3D;16, hidden_size2&#x3D;8, k&#x3D;5, num_epochs&#x3D;20, lr&#x3D;0.002, weight_decay&#x3D;0.01,batch_size&#x3D;32</p>
<p>平均验证log rmse &#x3D;0.401550</p>
<p>评价:学习率过高,反而出现了过拟合</p>
<img src="image-20230330165340646.png" alt="image-20230330165340646" style="zoom:50%;" />

<p>两层mlp, hidden_size1&#x3D;16, hidden_size2&#x3D;8, k&#x3D;5, num_epochs&#x3D;20, lr&#x3D;0.001, weight_decay&#x3D;0.01,batch_size&#x3D;32</p>
<p>平均验证log rmse &#x3D;0.375542</p>
<p>评价:是这几次里最好的结果了,但是loss曲线一直在波动.说明这应该几乎就是这个模型的极限了,可以再加一层试一试.</p>
<img src="image-20230330170509650.png" alt="image-20230330170509650" style="zoom:50%;" />

<p>三层mlp, hidden_size1&#x3D;16, hidden_size2&#x3D;8, hidden_size3&#x3D;16, k&#x3D;5, num_epochs&#x3D;20, lr&#x3D;0.001, weight_decay&#x3D;0.01,batch_size&#x3D;64</p>
<p>平均验证log rmse &#x3D; 0.412497</p>
<p>评价:loss很大,可能是三层的原因,也可能是hidden_size 设置的原因,再调一下hidden_size.还有一个线性,训练的loss平均要比验证loss小,说明层数高时有一点过拟合现象.</p>
<p>这一次改了mlp层数,和batch_size.同时改了两个也不清楚是谁的锅. epoch 改回32后loss基本在0.40左右,说明两个改动对于loss都是副作用,epoch改回的训练就不放上去了</p>
<img src="image-20230330171219491.png" alt="image-20230330171219491" style="zoom:50%;" />



<p>三层mlp, hidden_size1&#x3D;16, hidden_size2&#x3D;12, hidden_size3&#x3D;8, k&#x3D;5, num_epochs&#x3D;20, lr&#x3D;0.001, weight_decay&#x3D;0.01, batch_size&#x3D;32</p>
<p>平均验证log rmse &#x3D; 0.410163</p>
<p>评价:这次把隐藏层的神经元数量改为递减,loss在0.41左右,说明这样的神经网络效果也不好,看最后的epoch曲线不是因为epoch数量太少.随着epoch增加,train和valid的loss差距在增大,说明正在逐渐出现过拟合.</p>
<img src="image-20230330172530787.png" alt="image-20230330172530787" style="zoom:50%;" />

<p>三层mlp, hidden_size1&#x3D;16, hidden_size2&#x3D;16, hidden_size3&#x3D;16, k&#x3D;5, num_epochs&#x3D;20, lr&#x3D;0.001, weight_decay&#x3D;0.01, batch_size&#x3D;32</p>
<p>平均验证log rmse &#x3D; 4.1小一点</p>
<p>评价:只训练了一折,效果不好不再浪费时间.</p>
<p>三层mlp, hidden_size1&#x3D;16, hidden_size2&#x3D;8, hidden_size3&#x3D;8, k&#x3D;5, num_epochs&#x3D;20, lr&#x3D;0.001, weight_decay&#x3D;0.01, batch_size&#x3D;32</p>
<p>平均验证log rmse &#x3D; 4.1左右</p>
<p>评价:效果不好,不再继续.但是三层mlp为什么拟合效果还不如两层?</p>
<p>两层mlp, hidden_size1&#x3D;16, hidden_size2&#x3D;8, k&#x3D;5, num_epochs&#x3D;20, lr&#x3D;0.001, weight_decay&#x3D;0.01, batch_size&#x3D;32</p>
<p>平均验证log rmse &#x3D; 0.41左右</p>
<p>评价:再次验证上次最好的超参数,检验是否是偶然因素.说明上次调出来的是偶然因素,loss一直在0.4左右徘徊,调整参数并没有很大的改变,直接上测试集看看效果吧</p>
<p>最终的训练loss :0.401439</p>
<img src="image-20230330175100356.png" alt="image-20230330175100356" style="zoom:50%;" />
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
</search>
