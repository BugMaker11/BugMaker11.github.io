<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>My New Post</title>
    <url>/2023/03/20/My-New-Post/</url>
    <content><![CDATA[<span id="more"></span>

<p><strong>Hello! it’s my first blog.</strong></p>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/03/20/hello-world/</url>
    <content><![CDATA[<span id="more"></span>

<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>hexo 设置</title>
    <url>/2023/03/20/hexo-%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<span id="more"></span>
<h3 id="hexo-常用命令">hexo 常用命令</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new &quot;name&quot; 	 #新建文章</span><br><span class="line">hexo new page &quot;name&quot; #新建页面</span><br><span class="line">hexo g 				 #生成页面</span><br><span class="line">hexo d				 #部署</span><br><span class="line">hexo s				 #本地预览</span><br><span class="line">hexo clean 			 #清除缓存和已生成的静态文件</span><br><span class="line">hexo help			 #帮助</span><br></pre></td></tr></table></figure>
<h3 id="hexo-美化">hexo 美化</h3>
<p>....未完待续</p>
<h3 id="hexo-设置观察">hexo 设置观察</h3>
<p><span class="math inline">\(ax = b + c\)</span></p>
]]></content>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-数据操作,预处理笔记</title>
    <url>/2023/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>torch.arange(n)<br>生成从0到n-1的所有整数作为一个张量,n&#x3D;0时张量为空</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">12</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>


<pre><code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.arange(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([], dtype=torch.int64)
</code></pre>
<p><strong>x.shape</strong><br>输出张量的形状(每个轴的长度)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure>


<pre><code>torch.Size([12])
</code></pre>
<p>求张量中元素的总数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.numel()</span><br></pre></td></tr></table></figure>


<pre><code>12
</code></pre>
<p>改变张量的形状,可以输入张量每个维度的长度,也可以少输入一个维度的长度,因为可以自动推断出那个维度的长度.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = x.reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">X</span><br><span class="line">X.shape</span><br></pre></td></tr></table></figure>


<pre><code>torch.Size([3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = x.reshape(<span class="number">3</span>,<span class="number">2</span>,-<span class="number">1</span>)</span><br><span class="line">X</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[[ 0,  1],
         [ 2,  3]],

        [[ 4,  5],
         [ 6,  7]],

        [[ 8,  9],
         [10, 11]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.reshape(-<span class="number">1</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.reshape(<span class="number">3</span>,-<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
</code></pre>
<p>torch.zeros((n1,…)):创建全0的张量<br>torch.ones((n1,n2,n3,…))创建全1的张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.zeros((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]]])
</code></pre>
<p>创建正态分布随机数构成的张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-1.0532, -1.2373,  1.5930, -1.3120],
        [ 1.7310, -0.5996,  0.6864, -1.3625],
        [-0.4236,  0.8607,  0.0581, -1.2500]])
</code></pre>
<p>为创建的张量赋值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.tensor([[<span class="number">2</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[2, 1, 4, 3],
        [1, 2, 3, 4],
        [4, 3, 2, 1]])
</code></pre>
<p>张量的计算 + - * &#x2F; **<br>x ** y 表示x的y次幂</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">8</span>])</span><br><span class="line">y = torch.tensor([<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">x+y , x-y , x*y , x/y , x**y</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([ 3.,  4.,  6., 10.]),
 tensor([-1.,  0.,  2.,  6.]),
 tensor([ 2.,  4.,  8., 16.]),
 tensor([0.5000, 1.0000, 2.0000, 4.0000]),
 tensor([ 1.,  4., 16., 64.]))
</code></pre>
<p>对张量中的每个元素求幂运算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.exp(x)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])
</code></pre>
<p>将两个张量合并在一起 torch.cat( (x,y),dim&#x3D;?)<br>dim&#x3D;0时表示按行合并<br>dim&#x3D;1时表示按列合并</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>,dtype=torch.float32).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]])</span><br><span class="line">torch.cat((X,Y),dim=<span class="number">0</span>),torch.cat((X,Y),dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果:</p>
<pre><code>(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [ 2.,  1.,  4.,  3.],
         [ 1.,  2.,  3.,  4.],
         [ 4.,  3.,  2.,  1.]]),
 tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
</code></pre>
<p>逻辑运算符构建二元张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X == Y</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[False,  True, False,  True],
        [False, False, False, False],
        [False, False, False, False]])
</code></pre>
<p>对张量中的所有元素进行求和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>


<pre><code>tensor(66.)
</code></pre>
<p>广播机制:</p>
<p>当两个张量的形状不同时,通过广播机制可以使两个张量通过复制元素变成相同的形状,比如下面的例子中 a + b 后 a , b的形状就分别从 (3,1) 和 (1,2) 变成 (3,2)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">a,b</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[0],
         [1],
         [2]]),
 tensor([[0, 1]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a + b</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[0, 1],
        [1, 2],
        [2, 3]])
</code></pre>
<p>索引和切片</p>
<p>张量可以直接通过 [数字] 进行索引,同时选择多个元素就是切片,通过 数字:数字 实现.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X[-<span class="number">1</span>],X[<span class="number">1</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([ 8.,  9., 10., 11.]),
 tensor([[ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.]]))
</code></pre>
<p>除读取外还可以通过指定索引来将元素写入矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X[<span class="number">1</span>,<span class="number">2</span>] = <span class="number">9</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  9.,  7.],
        [ 8.,  9., 10., 11.]])
</code></pre>
<p>可以同时为多个元素赋值,只有一个:时表示所有元素</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X[<span class="number">0</span>:<span class="number">2</span>, :] = <span class="number">12</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[12., 12., 12., 12.],
        [12., 12., 12., 12.],
        [ 8.,  9., 10., 11.]])
</code></pre>
<p>执行原地操作,用[:]将张量中的所有元素改写而不是用结果创建一个新的变量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = torch.zeros_like(Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;id(Z)&quot;</span>,<span class="built_in">id</span>(Z))</span><br><span class="line">Z[:] = X + Y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;id(Z)&quot;</span>,<span class="built_in">id</span>(Z))</span><br></pre></td></tr></table></figure>

<pre><code>id(Z) 1382002751504
id(Z) 1382002751504
</code></pre>
<p>类型转换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="built_in">type</span>(A),<span class="built_in">type</span>(B)</span><br></pre></td></tr></table></figure>




<pre><code>(numpy.ndarray, torch.Tensor)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a,a.item(),<span class="built_in">float</span>(a),<span class="built_in">int</span>(a)</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([3.5000]), 3.5, 3.5, 3)
</code></pre>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>首先(<strong>创建一个人工数据集，并存储在CSV（逗号分隔值）文件</strong>) <code>../data/house_tiny.csv</code>中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>,<span class="string">&#x27;data&#x27;</span>),exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>,<span class="string">&#x27;data&#x27;</span>,<span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,10600\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>从创建的CSV文件中加载原始数据集</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install pandas</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line">data</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>NumRooms</th>
      <th>Alley</th>
      <th>Price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>Pave</td>
      <td>127500</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.0</td>
      <td>NaN</td>
      <td>10600</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.0</td>
      <td>NaN</td>
      <td>178100</td>
    </tr>
    <tr>
      <th>3</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>140000</td>
    </tr>
  </tbody>
</table>

</div>

<p>为了处理缺失的数据，我们用插值法,将缺失的房间数插值为房间数的平均值.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs,outputs = data.iloc[:,<span class="number">0</span>:<span class="number">2</span>],data.iloc[:,<span class="number">2</span>]</span><br><span class="line">inputs = inputs.fillna(inputs.mean())</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>

<pre><code>   NumRooms Alley
0       3.0  Pave
1       2.0   NaN
2       4.0   NaN
3       3.0   NaN
</code></pre>
<p>将缺失的Alley单独视为一类,数值为1,不缺失的房子则为0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = pd.get_dummies(inputs,dummy_na=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>

<pre><code>   NumRooms  Alley_Pave  Alley_nan
0       3.0           1          0
1       2.0           0          1
2       4.0           0          1
3       3.0           0          1
</code></pre>
<p>将inputs 和 outputs 中的所有条目转换为张量格式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">X,y = torch.tensor(inputs.values),torch.tensor(outputs.values)</span><br><span class="line">X,y</span><br></pre></td></tr></table></figure>

<p>输出结果:</p>
<pre><code>(tensor([[3., 1., 0.],
         [2., 0., 1.],
         [4., 0., 1.],
         [3., 0., 1.]], dtype=torch.float64),
 tensor([127500,  10600, 178100, 140000]))
</code></pre>
<p>练习:创建原始数据集,删除缺失值最多的列,将预处理后的数据集转换为张量格式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>,<span class="string">&#x27;data&#x27;</span>),exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>,<span class="string">&#x27;data&#x27;</span>,<span class="string">&#x27;information.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;Name,Age,Height,Gender,Province,Grades\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,12,175,&quot;men&quot;,&quot;henan&quot;,100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;&quot;xiaoming&quot;,NA,180,&quot;girl&quot;,&quot;hebei&quot;,80\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;&quot;xiaoli&quot;,17,174,NA,NA,90\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;&quot;xiaoqiang&quot;,NA,168,&quot;girl&quot;,NA,70\n&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line">data</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>Age</th>
      <th>Height</th>
      <th>Gender</th>
      <th>Province</th>
      <th>Grades</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>12.0</td>
      <td>175</td>
      <td>men</td>
      <td>henan</td>
      <td>100</td>
    </tr>
    <tr>
      <th>1</th>
      <td>xiaoming</td>
      <td>NaN</td>
      <td>180</td>
      <td>girl</td>
      <td>hebei</td>
      <td>80</td>
    </tr>
    <tr>
      <th>2</th>
      <td>xiaoli</td>
      <td>17.0</td>
      <td>174</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>90</td>
    </tr>
    <tr>
      <th>3</th>
      <td>xiaoqiang</td>
      <td>NaN</td>
      <td>168</td>
      <td>girl</td>
      <td>NaN</td>
      <td>70</td>
    </tr>
  </tbody>
</table>

</div>

]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习环境配置2</title>
    <url>/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/</url>
    <content><![CDATA[<span id="more"></span>

<h4 id="参考教程与视频"><a href="#参考教程与视频" class="headerlink" title="参考教程与视频"></a>参考教程与视频</h4><p>本文主要参考李沐在B站发布的视频:<a href="https://www.bilibili.com/video/BV18K411w7Vs/?spm_id_from=333.999.0.0&vd_source=4d05ddde14c10a4b5df542785dd6efc5">Windows 下安装 CUDA 和 Pytorch 跑深度学习 - 动手学深度学习v2_哔哩哔哩_bilibili</a></p>
<h4 id="看前提醒"><a href="#看前提醒" class="headerlink" title="看前提醒"></a>看前提醒</h4><ol>
<li>本文基于Windows环境安装.</li>
<li>本文安装GPU版本的pytorch，不含独立显卡的电脑不能安装GUP版本pytorch.</li>
<li></li>
</ol>
<h4 id="检查电脑显卡"><a href="#检查电脑显卡" class="headerlink" title="检查电脑显卡"></a>检查电脑显卡</h4><p><code>windows + r</code> 后输入 <code>dxdiag</code> 打开 <code>DirectX诊断工具</code> ,选择显示栏,查看是否有独立显卡.</p>
<h4 id="下载CUDA"><a href="#下载CUDA" class="headerlink" title="下载CUDA"></a>下载CUDA</h4><p>在CUDA官网中找到想要下载的CUDA版本和下载方式,我选择的是11 Version,下载方式为：exe(local)。选择好后点击 download 开始下载.</p>
<p>下载地址：<a href="https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local">CUDA Toolkit 12.1 Downloads | NVIDIA Developer</a></p>
<p><img src="/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/image-20230320205838829.png" alt="image-20230320205838829"></p>
<p>下载完成后解压并默认安装。</p>
<p>安装后点击 <code>win + r</code> ,输入 <code>cmd</code> 进入命令行界面,输入<code>nvidia-smi</code> 检查 CUDA 是否安装成功.</p>
<p><img src="/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/image-20230320210301689.png" alt="image-20230320210301689"></p>
<h4 id="安装anaconda"><a href="#安装anaconda" class="headerlink" title="安装anaconda"></a>安装anaconda</h4><p>李沐老师在视频中安装的是 miniconda ,因为 anaconda 有图形化界面可以操作,我选择下载 anaconda.</p>
<p>搜索 anaconda 进入 anaconda 官网,选择 download 进行下载.下载完成后选择默认进行安装.</p>
<p>安装完成后打开 anaconda prompt 输入 python 查看能否进入 python 环境.</p>
<p>参考网址:<a href="https://www.anaconda.com/">https://www.anaconda.com</a></p>
<h4 id="安装PyTorch"><a href="#安装PyTorch" class="headerlink" title="安装PyTorch"></a>安装PyTorch</h4><p>搜索 pytorch 进入 PyTorch 官方网站, 点击 install 后选择:</p>
<table>
<thead>
<tr>
<th>选项</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>PyTorch Build</td>
<td>Stable(2.0.0)</td>
</tr>
<tr>
<td>Your OS</td>
<td>Windows</td>
</tr>
<tr>
<td>Package</td>
<td>Pip</td>
</tr>
<tr>
<td>Language</td>
<td>Python</td>
</tr>
<tr>
<td>Compute Platform</td>
<td>CUDA 11.8</td>
</tr>
</tbody></table>
<p>选择完后将下方 <code>Run this Command:</code> 栏的命令复制到 anaconda prompt 中回车安装.</p>
<p>注意:</p>
<ul>
<li>李沐老师推荐安装与 CUDA版本一致的 PyTorch 版本,就算不完全一致也应该选择大版本一致的 PyTorch 版本.</li>
<li>安装时应关闭代理,我在打开代理时安装多次安装不成功.</li>
</ul>
<p>安装完成后输入 python 进入 python 环境,输入以下代码检测是否成功安装:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.ones((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">a = a.cuda(<span class="number">0</span>)</span><br><span class="line">b = torch.ones((<span class="number">3</span>,<span class="number">1</span>)).cuda(<span class="number">0</span>)</span><br><span class="line">a + b</span><br></pre></td></tr></table></figure>

<p>看到如下输出则成功安装:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>]],device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="下载课程"><a href="#下载课程" class="headerlink" title="下载课程"></a>下载课程</h4><p>进入动手学深度学习官网:<a href="https://zh.d2l.ai/">《动手学深度学习》 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p>
<p>点击 <code>Jupyter 记事本</code> 下载课程内容.下载完成后解压到本地.</p>
<p><img src="/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/image-20230320212516743.png" alt="image-20230320212516743"></p>
<p>打开 Anaconda Prompt ,输入如下命令,安装所需要的包.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install jupyter d2l</span><br></pre></td></tr></table></figure>

<p>最后,打开课程所在目录, 按  <code>shift + 鼠标右键</code> 选择在此处打开 Powershell 窗口,在命令行中输入 <code>jupyter notebook</code> 就可以成功打开课程的记事本!</p>
<p><img src="/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/image-20230320213115708.png" alt="image-20230320213115708"></p>
<p><img src="/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE2/image-20230320213324026.png" alt="image-20230320213324026"></p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-微积分</title>
    <url>/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%BE%AE%E7%A7%AF%E5%88%86/</url>
    <content><![CDATA[<h1 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h1><p>在运行时每次都内核崩溃,加上下面代码后能够成功运行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib_inline <span class="keyword">import</span> backend_inline</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">3</span>*x**<span class="number">2</span> - <span class="number">4</span>*x</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">numerical_lim</span>(<span class="params">f, x, h</span>):</span><br><span class="line">    <span class="keyword">return</span> (f(x + h) - f(x)) / h</span><br><span class="line"></span><br><span class="line">h = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;h=<span class="subst">&#123;h:<span class="number">.5</span>f&#125;</span>, numerical limit=<span class="subst">&#123;numerical_lim(f, <span class="number">1</span>, h):<span class="number">.5</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    h *= <span class="number">0.1</span></span><br></pre></td></tr></table></figure>

<pre><code>h=0.10000, numerical limit=2.30000
h=0.01000, numerical limit=2.03000
h=0.00100, numerical limit=2.00300
h=0.00010, numerical limit=2.00030
h=0.00001, numerical limit=2.00003
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">use_svg_display</span>(): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用svg格式在Jupyter中显示绘图&quot;&quot;&quot;</span></span><br><span class="line">    backend_inline.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>) <span class="comment">#指定显示为svg格式</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_figsize</span>(<span class="params">figsize=(<span class="params"><span class="number">3.5</span>,<span class="number">2.5</span></span>)</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置matplotlib的图标大小&quot;&quot;&quot;</span></span><br><span class="line">    use_svg_display()</span><br><span class="line">    d2l.plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] =  figsize</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_axes</span>(<span class="params">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;</span></span><br><span class="line">    axes.set_xlabel(xlabel)</span><br><span class="line">    axes.set_ylabel(ylabel)</span><br><span class="line">    axes.set_xscale(xscale)</span><br><span class="line">    axes.set_yscale(yscale)</span><br><span class="line">    axes.set_xlim(xlim)</span><br><span class="line">    axes.set_ylim(ylim)</span><br><span class="line">    <span class="keyword">if</span> legend:</span><br><span class="line">        axes.legend(legend)</span><br><span class="line">    axes.grid()</span><br></pre></td></tr></table></figure>

<p>通过使用上面三个图形化配置函数,定义一个<strong>plot</strong> 函数用来绘制曲线.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">X,Y=<span class="literal">None</span>,xlabel=<span class="literal">None</span>,ylabel=<span class="literal">None</span>,legend=<span class="literal">None</span>,xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">         ylim=<span class="literal">None</span>,xscale=<span class="string">&#x27;linear&#x27;</span>,yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">         fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>,<span class="string">&#x27;m--&#x27;</span>,<span class="string">&#x27;g-&#x27;</span>,<span class="string">&#x27;r:&#x27;</span></span>),figsize=(<span class="params"><span class="number">3.5</span>,<span class="number">2.5</span></span>),axes=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制数据点&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        legend = []</span><br><span class="line">    </span><br><span class="line">    set_figsize(figsize)</span><br><span class="line">    axes = axes <span class="keyword">if</span> axes <span class="keyword">else</span> d2l.plt.gca()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#如果X有一个轴,输出True</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">has_one_axis</span>(<span class="params">X</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="built_in">hasattr</span>(X,<span class="string">&quot;ndim&quot;</span>) <span class="keyword">and</span> X.ndim == <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(X,<span class="built_in">list</span>)</span><br><span class="line">                <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(X[<span class="number">0</span>],<span class="string">&quot;__len__&quot;</span>))</span><br><span class="line">    <span class="keyword">if</span> has_one_axis(X):</span><br><span class="line">        X=[X]</span><br><span class="line">    <span class="keyword">if</span> Y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        X,Y = [[]]*<span class="built_in">len</span>(X),X</span><br><span class="line">    <span class="keyword">elif</span> has_one_axis(Y):</span><br><span class="line">        Y = [Y]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(X) != <span class="built_in">len</span>(Y):</span><br><span class="line">        X = X * <span class="built_in">len</span>(Y)</span><br><span class="line">    axes.cla()</span><br><span class="line">    <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y, fmts):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(x):</span><br><span class="line">            axes.plot(x, y, fmt)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            axes.plot(y, fmt)</span><br><span class="line">    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">例子:绘制f(x)=<span class="number">2</span>*x-<span class="number">3</span>在 x=<span class="number">1</span>处的切线 y=2x-<span class="number">3</span> </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">3</span>,<span class="number">0.1</span>)</span><br><span class="line">plot(x,[f(x), <span class="number">2</span>*x-<span class="number">3</span>],<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;f(x)&#x27;</span>, legend=[<span class="string">&#x27;f(x)&#x27;</span>,<span class="string">&#x27;Tangent line(x=1)&#x27;</span>])</span><br></pre></td></tr></table></figure>


<p><img src="/output_11_0.svg" alt="svg"></p>
<p>偏导数</p>
<p>用于描述多元函数对于变量的微分</p>
<p>梯度</p>
<p>连结一个多元函数对所有变量的偏导数,得到该函数的梯度</p>
<h1 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h1><p>深度学习框架不再手工进行求导,而是通过自动微分加快求导.系统会构建一个计算图来跟踪求导运算.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">4.0</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>


<pre><code>tensor([0., 1., 2., 3.])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.requires_grad_(<span class="literal">True</span>) <span class="comment">#表示为梯度开辟空间来存储</span></span><br><span class="line">x.grad  <span class="comment">#x.grad表示x的梯度</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = <span class="number">2</span>* torch.dot(x,x) <span class="comment">#y=2xT*x</span></span><br><span class="line">y</span><br></pre></td></tr></table></figure>


<pre><code>tensor(28., grad_fn=&lt;MulBackward0&gt;)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y.backward() <span class="comment">#调用反向传播函数自动计算y对于x每个分量的梯度</span></span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>


<pre><code>tensor([ 0.,  4.,  8., 12.])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad == <span class="number">4</span> * x  <span class="comment">#验证梯度</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([True, True, True, True])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()  <span class="comment">#清除上一次求的梯度,因为pytorch求的导数会累加起来</span></span><br><span class="line">y = x.<span class="built_in">sum</span>()  <span class="comment">#x.sum() = x1+x2+x3+...,所以x.sum()对x的梯度为(1,1,1,...)</span></span><br><span class="line">y.backward() <span class="comment">#求梯度</span></span><br><span class="line">x.grad   <span class="comment">#展示梯度</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([1., 1., 1., 1.])
</code></pre>
<p>非标量变量的反向传播(y不是标量,而是x的函数时)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x*x</span><br><span class="line">y.<span class="built_in">sum</span>().backward() <span class="comment">#将y转化为标量后进行求导y.sum()=x1^2+x2^2+.....</span></span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>


<pre><code>tensor([0., 2., 4., 6.])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y  ,y.backward()<span class="comment">#y是一个向量,x也是一个向量,y对x求导结果是一个矩阵,</span></span><br><span class="line"><span class="comment"># 而我们需要的是x1^2对x1的偏导,x2^2对x2的偏导...,所以要将y转化为标量后求导</span></span><br></pre></td></tr></table></figure>

<p>分离计算</p>
<p>非标量变量需要当作标量变量使用时进行分离,分离后作为常数进行处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()  <span class="comment">#detach()将y分离为常数</span></span><br><span class="line">z = u*x </span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u  <span class="comment">#z对x的导数=u</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([True, True, True, True])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == <span class="number">2</span>*x</span><br></pre></td></tr></table></figure>


<pre><code>tensor([True, True, True, True])
</code></pre>
<p>Python控制流的梯度计算</p>
<p>即使一个具有多种函数的控制流对x求导,我们也能计算梯度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a*<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.randn(size=(),requires_grad=<span class="literal">True</span>)</span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a.grad == d/a <span class="comment">#f(a)是线性的,所以可以用d/a验证a.grad导数是否算对了</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor(True)
</code></pre>
<p>练习</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d.backward()  <span class="comment">#再次运行反向传播函数会报错,因为导数会积累,</span></span><br><span class="line"><span class="comment">#确定要再次运行反向传播函数可以加上retain_graph=True 参数</span></span><br></pre></td></tr></table></figure>


<pre><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

~\AppData\Local\Temp\ipykernel_19960\2703853229.py in &lt;module&gt;
----&gt; 1 d.backward()  #再次运行反向传播函数会报错


F:\anaconda\lib\site-packages\torch\_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)
    485                 inputs=inputs,
    486             )
--&gt; 487         torch.autograd.backward(
    488             self, gradient, retain_graph, create_graph, inputs=inputs
    489         )


F:\anaconda\lib\site-packages\torch\autograd\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    198     # some Python versions print out the first line of a multi-line function
    199     # calls in the traceback and some print out the last line
--&gt; 200     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    201         tensors, grad_tensors_, retain_graph, create_graph, inputs,
    202         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass


RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
</code></pre>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-线性代数</title>
    <url>/2023/03/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</url>
    <content><![CDATA[<h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><p>标量运算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line">y = torch.tensor(<span class="number">2.0</span>)</span><br><span class="line">x+y ,  x*y  , x/y  , x**y</span><br></pre></td></tr></table></figure>


<pre><code>(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))
</code></pre>
<p>向量长度,形状</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>


<pre><code>tensor([0, 1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x[<span class="number">3</span>]</span><br></pre></td></tr></table></figure>


<pre><code>tensor(3)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(x) <span class="comment">#x的长度</span></span><br></pre></td></tr></table></figure>


<pre><code>4
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.shape <span class="comment">#x的形状</span></span><br></pre></td></tr></table></figure>


<pre><code>torch.Size([4])
</code></pre>
<p>矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">A</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.T <span class="comment">#矩阵转置</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0,  4,  8, 12, 16],
        [ 1,  5,  9, 13, 17],
        [ 2,  6, 10, 14, 18],
        [ 3,  7, 11, 15, 19]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">B = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">0</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">B</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[1, 2, 3],
        [2, 0, 4],
        [3, 4, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">B == B.T <span class="comment">#矩阵比较</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<pre><code>tensor([[True, True, True],
        [True, True, True],
        [True, True, True]])
</code></pre>
<p>张量</p>
<p>具有相同形状的张量,他们按元素二元运算的结果是同一形状的张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>,dtype=torch.float32).reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">B = A.clone()</span><br><span class="line">A,A+B</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [12., 13., 14., 15.],
         [16., 17., 18., 19.]]),
 tensor([[ 0.,  2.,  4.,  6.],
         [ 8., 10., 12., 14.],
         [16., 18., 20., 22.],
         [24., 26., 28., 30.],
         [32., 34., 36., 38.]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A*B</span><br><span class="line"><span class="comment">#Hadamard积,按元素进行分别求积</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([[  0.,   1.,   4.,   9.],
        [ 16.,  25.,  36.,  49.],
        [ 64.,  81., 100., 121.],
        [144., 169., 196., 225.],
        [256., 289., 324., 361.]])
</code></pre>
<p>张量乘或加一个标量不会改变张量的形状,因为每个元素都与标量乘或加</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">2</span></span><br><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,-<span class="number">1</span>)</span><br><span class="line">a + X , (a * X).shape </span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[[ 2,  3,  4,  5],
          [ 6,  7,  8,  9],
          [10, 11, 12, 13]],
 
         [[14, 15, 16, 17],
          [18, 19, 20, 21],
          [22, 23, 24, 25]]]),
 torch.Size([2, 3, 4]))
</code></pre>
<h1 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h1><p>计算元素和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>,dtype = torch.float32)</span><br><span class="line">x, x.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([0., 1., 2., 3.]), tensor(6.))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.shape,A.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>


<pre><code>(torch.Size([5, 4]), tensor(190.))
</code></pre>
<p>axis&#x3D;n,该张量的第n维消失,表现在形状中就是第形状的第n个数没了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) <span class="comment">#A的形状是[5,4],第0维消失后形状变为[4]</span></span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([40., 45., 50., 55.]), torch.Size([4]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>,<span class="number">1</span>]) <span class="comment">#[0,1]维都消失相当于计算元素和</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor(190.)
</code></pre>
<p>平均值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean()</span><br></pre></td></tr></table></figure>


<pre><code>tensor(9.5000)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>()/A.numel()</span><br></pre></td></tr></table></figure>


<pre><code>tensor(9.5000)
</code></pre>
<p>计算平均值是可以沿指定轴降低张量维度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean(axis=<span class="number">0</span>) <span class="comment">#axis=0,表示在第0维降低张量,求平均值,结果的形状是[4]</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([ 8.,  9., 10., 11.])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.mean(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / A.shape[<span class="number">0</span>]  <span class="comment">#=a.mean(axis=0)</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([ 8.,  9., 10., 11.])
</code></pre>
<p>非降维求和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>) <span class="comment">#用keepdims参数来保持轴数不变</span></span><br><span class="line">sum_A,sum_A.shape</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[ 6.],
         [22.],
         [38.],
         [54.],
         [70.]]),
 torch.Size([5, 1]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A/sum_A  <span class="comment">#通过广播sum_A被拓展到和A一样大,所以能够相除</span></span><br></pre></td></tr></table></figure>


<pre><code>tensor([[0.0000, 0.1667, 0.3333, 0.5000],
        [0.1818, 0.2273, 0.2727, 0.3182],
        [0.2105, 0.2368, 0.2632, 0.2895],
        [0.2222, 0.2407, 0.2593, 0.2778],
        [0.2286, 0.2429, 0.2571, 0.2714]])
</code></pre>
<p>求A元素的累积总和,axis&#x3D;0表示按行进行累积,A.cumsum(axis&#x3D;0)中的每一个数表示该列到这里数的累计和,例如4行0列的元素40&#x3D;0+4+8+12+16</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.cumsum(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  6.,  8., 10.],
        [12., 15., 18., 21.],
        [24., 28., 32., 36.],
        [40., 45., 50., 55.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])
</code></pre>
<p>点积</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.ones(<span class="number">4</span>, dtype = torch.float32)</span><br><span class="line">x , y , torch.dot(x,y)</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))
</code></pre>
<p>也可以按元素乘法后进行求和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(x*y)</span><br></pre></td></tr></table></figure>


<pre><code>tensor(6.)
</code></pre>
<p>矩阵-向量积</p>
<p>矩阵乘列向量的结果为一个列向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A , x , A.shape, x.shape , torch.mv(A,x) , torch.mv(A,x).shape</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [12., 13., 14., 15.],
         [16., 17., 18., 19.]]),
 tensor([0., 1., 2., 3.]),
 torch.Size([5, 4]),
 torch.Size([4]),
 tensor([ 14.,  38.,  62.,  86., 110.]),
 torch.Size([5]))
</code></pre>
<p>矩阵-矩阵乘法</p>
<p>torch.mm()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">B = torch.ones(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">torch.mm(A,B)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 6.,  6.,  6.],
        [22., 22., 22.],
        [38., 38., 38.],
        [54., 54., 54.],
        [70., 70., 70.]])
</code></pre>
<p>L2范数</p>
<p>L2范数相当于距离,是各元素平方和的平方根</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure>


<pre><code>tensor(5.)
</code></pre>
<p>L1范数</p>
<p>L1范数是各元素绝对值之和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>


<pre><code>tensor(7.)
</code></pre>
<p>矩阵的Frobenius范数</p>
<p>是矩阵元素平方和的平方根,与L2范数类似</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.norm(torch.ones(<span class="number">4</span>,<span class="number">9</span>) )</span><br></pre></td></tr></table></figure>


<pre><code>tensor(6.)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.ones((<span class="number">4</span>,<span class="number">9</span>))</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])
</code></pre>
<p>范数在机器学习中通常作为项目之间的距离:<br>最大化不同项目之间的距离.<br>最小化相似项目之间的距离.</p>
<p>练习</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.T.T == A</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">B = torch.arange(<span class="number">20</span>)</span><br><span class="line">B = B.reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">A,B</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [12., 13., 14., 15.],
         [16., 17., 18., 19.]]),
 tensor([[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11],
         [12, 13, 14, 15],
         [16, 17, 18, 19]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(A+B).T == (A.T + B.T)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[True, True, True, True, True],
        [True, True, True, True, True],
        [True, True, True, True, True],
        [True, True, True, True, True]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">C = torch.arange(<span class="number">16</span>)</span><br><span class="line">C = C.reshape(<span class="number">4</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">C == C.T</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ True, False, False, False],
        [False,  True, False, False],
        [False, False,  True, False],
        [False, False, False,  True]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">C + C.T == (C+C.T).T</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>)</span><br><span class="line">X = X.reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">X</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(X)</span><br></pre></td></tr></table></figure>


<pre><code>2
</code></pre>
<p>len(X)的结果是第一维的长度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A/A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<pre><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

~\AppData\Local\Temp\ipykernel_10048\3337381381.py in &lt;module&gt;
----&gt; 1 A/A.sum(axis=1)


RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1
</code></pre>
<p>A&#x2F;A.sum(axis&#x3D;1)会报错,因为A.sum(axis&#x3D;1)在降维后并没有保持轴数不变,所以A和A.sum(axis&#x3D;1)轴数不同,A有两个轴,A.sum(axis&#x3D;1)只有一个轴,此时并不会触发广播机制.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[12, 14, 16, 18],
        [20, 22, 24, 26],
        [28, 30, 32, 34]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[12, 15, 18, 21],
        [48, 51, 54, 57]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>(axis=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[ 6, 22, 38],
        [54, 70, 86]])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">D = torch.ones(<span class="number">27</span>)</span><br><span class="line">E = torch.ones(<span class="number">81</span>)</span><br><span class="line">D = D.reshape(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">E = E.reshape(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">D,E</span><br></pre></td></tr></table></figure>


<pre><code>(tensor([[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],
 
         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]],
 
         [[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]),
 tensor([[[[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]]],
</code></pre>
<p>​    </p>
<pre><code>         [[[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]]],




         [[[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]],
 
          [[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]]]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.linalg.norm(D)</span><br></pre></td></tr></table></figure>


<pre><code>5.196152
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.linalg.norm(E)</span><br></pre></td></tr></table></figure>


<pre><code>9.0
</code></pre>
<p>计算得到矩阵的范数</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-softmax回归</title>
    <url>/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-softmax%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p>图像分类数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>读取数据集</p>
<p>ToTensor使数据从pil类型变换成32位浮点数格式,root为读取数据集位置,train表示读取的是否是训练集,transform表示读取的是向量格式,download表示从网络下载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trans = transforms.ToTensor()</span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;../data&quot;</span>,train = <span class="literal">True</span>,transform = trans,download=<span class="literal">True</span>) <span class="comment">#训练集</span></span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;../data&quot;</span>,train= <span class="literal">False</span>, transform= trans,download=<span class="literal">True</span>) <span class="comment">#测试集</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(mnist_train),<span class="built_in">len</span>(mnist_test)</span><br></pre></td></tr></table></figure>




<pre><code>(60000, 10000)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mnist_train[<span class="number">0</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([1, 28, 28])
</code></pre>
<p>用于在数字标签索引及其文本名称之间进行转换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签&quot;&quot;&quot;</span></span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>,<span class="string">&#x27;trouser&#x27;</span>,<span class="string">&#x27;pullover&#x27;</span>,<span class="string">&#x27;dress&#x27;</span>,<span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                  <span class="string">&#x27;sandal&#x27;</span>,<span class="string">&#x27;shirt&#x27;</span>,<span class="string">&#x27;sneaker&#x27;</span>,<span class="string">&#x27;bag&#x27;</span>,<span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br></pre></td></tr></table></figure>

<p>创建函数可视化样本</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs,num_rows,num_cols,titles=<span class="literal">None</span>,scale=<span class="number">1.5</span></span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制图像列表&quot;&quot;&quot;</span></span><br><span class="line">    figsize = (num_cols*scale,num_rows*scale)</span><br><span class="line">    _,axes = d2l.plt.subplots(num_rows,num_cols,figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i ,(ax,img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes,imgs)):</span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(img):</span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X,y = <span class="built_in">next</span>(<span class="built_in">iter</span>(data.DataLoader(mnist_train,batch_size=<span class="number">18</span>)))</span><br><span class="line">show_images(X.reshape(<span class="number">18</span>,<span class="number">28</span>,<span class="number">28</span>),<span class="number">2</span>,<span class="number">9</span>,titles=get_fashion_mnist_labels(y));</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-softmax%E5%9B%9E%E5%BD%92/output_11_0.png" alt="png"></p>
<p>读取小批量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_iter = data.DataLoader(mnist_train,batch_size,shuffle=<span class="literal">True</span>,num_workers=get_dataloader_workers())</span><br></pre></td></tr></table></figure>

<p>读取训练数据所需的时间</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">timer = d2l.Timer()</span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.2</span>f&#125;</span> sec&#x27;</span></span><br></pre></td></tr></table></figure>




<pre><code>&#39;25.70 sec&#39;
</code></pre>
<p>将数据读取整合到一起</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size,resize=<span class="literal">None</span></span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-NMIST数据集,然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>,transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">True</span>,transform=trans,download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">False</span>,transform=trans,download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train,batch_size,shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test,batch_size,shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_iter,test_iter = load_data_fashion_mnist(<span class="number">32</span>,resize=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape,X.dtype,y.shape,y.dtype)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64
</code></pre>
<h1 id="线性回归从零开始实现"><a href="#线性回归从零开始实现" class="headerlink" title="线性回归从零开始实现"></a>线性回归从零开始实现</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>生成数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w,b,num_examples</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y和x&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>,<span class="number">1</span>,(num_examples,<span class="built_in">len</span>(w))) <span class="comment">#X为高斯分布的随机数</span></span><br><span class="line">    y = torch.matmul(X,w)+b <span class="comment">#根据X和预设的w计算y</span></span><br><span class="line">    y += torch.normal(<span class="number">0</span>,<span class="number">0.01</span>,y.shape) <span class="comment">#对y添加一个随机扰动</span></span><br><span class="line">    <span class="keyword">return</span> X,y.reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w = torch.tensor([<span class="number">2</span>,-<span class="number">3.4</span>]) <span class="comment">#设置w参数</span></span><br><span class="line">true_b = <span class="number">4.2</span> <span class="comment">#设置b参数</span></span><br><span class="line">features,labels = synthetic_data(true_w,true_b,<span class="number">1000</span>) <span class="comment">#根据参数设置数据集</span></span><br></pre></td></tr></table></figure>

<p>批量读取数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size,features,labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples)) <span class="comment">#设置一个标号用来随机读取数据集中的数据</span></span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,num_examples,batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i:<span class="built_in">min</span>(i + batch_size,num_examples)]) <span class="comment">#选取一个标号生成向量</span></span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices],labels[batch_indices] <span class="comment">#每次单独取一个features,labels</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size,features,labels):</span><br><span class="line">    <span class="built_in">print</span>(X,<span class="string">&#x27;\n&#x27;</span>,y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.1081, -0.5731],
        [-0.4934, -0.1212],
        [-0.1073,  0.8958],
        [-0.2244,  0.1532],
        [-0.0630, -0.7970],
        [-0.8336, -1.2219],
        [-1.0196,  1.6207],
        [-0.1009,  0.5200],
        [-0.4763, -0.1988],
        [ 0.5296, -0.2843]]) 
 tensor([[ 5.9328],
        [ 3.6350],
        [ 0.9321],
        [ 3.2144],
        [ 6.7879],
        [ 6.6725],
        [-3.3650],
        [ 2.2413],
        [ 3.9385],
        [ 6.2498]])
</code></pre>
<p>初始化模型参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.normal(<span class="number">0</span>,<span class="number">0.01</span>,size=(<span class="number">2</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>定义模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X,w,b</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X,w) + b</span><br></pre></td></tr></table></figure>

<p>定义损失函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat,y</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape))**<span class="number">2</span> /<span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>定义优化算法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params,lr,batch_size</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr*param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size,features,labels):</span><br><span class="line">        l = loss(net(X,w,b),y) <span class="comment">#小批量的均方损失</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w,b],lr,batch_size) <span class="comment">#使用梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features,w,b),labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>,loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>) <span class="comment">#打印每个epoch的均方损失</span></span><br></pre></td></tr></table></figure>

<pre><code>epoch1,loss 0.000158
epoch2,loss 0.000051
epoch3,loss 0.000051
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差:<span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的估计误差:<span class="subst">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的估计误差:tensor([-1.5759e-04, -9.2030e-05], grad_fn=&lt;SubBackward0&gt;)
b的估计误差:tensor([-0.0002], grad_fn=&lt;RsubBackward1&gt;)
</code></pre>
<h1 id="softmax回归的简洁实现-使用pytorch"><a href="#softmax回归的简洁实现-使用pytorch" class="headerlink" title="softmax回归的简洁实现(使用pytorch)"></a>softmax回归的简洁实现(使用pytorch)</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>导入数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size) </span><br><span class="line"><span class="comment">#从d2l包中直接导入数据集</span></span><br></pre></td></tr></table></figure>

<p>初始化模型参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),nn.Linear(<span class="number">784</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight,std=<span class="number">0.01</span>) <span class="comment">#m用正态分布随机化一个数</span></span><br><span class="line">        </span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></table></figure>

<p>定义损失函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>调用优化算法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(),lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>

<p>训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-softmax%E5%9B%9E%E5%BD%92/output_50_0.svg" alt="svg"></p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-线性回归</title>
    <url>/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>y^ &#x3D; Xw + b</p>
<p>损失函数</p>
<p>$$l^{(i)}(\mathbf{w}, b) &#x3D; \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$</p>
<p>n个样本的平均损失函数</p>
<p>$$L(\mathbf{w}, b) &#x3D;\frac{1}{n}\sum_{i&#x3D;1}^n l^{(i)}(\mathbf{w}, b) &#x3D;\frac{1}{n} \sum_{i&#x3D;1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$</p>
<p>目的是找到一组参数,使训练样本上的总损失最小:</p>
<p>$$\mathbf{w}^*, b^* &#x3D; \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).$$</p>
<p>矢量化加速</p>
<p>对于矢量的计算不再用for单独遍历,而是直接调用线性代数库进行矢量计算能够更快</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n = <span class="number">10000</span></span><br><span class="line">a = torch.ones([n])</span><br><span class="line">b = torch.ones([n])</span><br></pre></td></tr></table></figure>

<p>定义计数器来测量运行时间</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Timer</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;记录多次运行时间&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.times = []</span><br><span class="line">        self.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;启动计时器&quot;&quot;&quot;</span></span><br><span class="line">        self.tik = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">stop</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;停止计时器并将时间记录在列表中&quot;&quot;&quot;</span></span><br><span class="line">        self.times.append(time.time() - self.tik)</span><br><span class="line">        <span class="keyword">return</span> self.times[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">avg</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回平均时间&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(self.times) / <span class="built_in">len</span>(self.times)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sum</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回时间总和&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(self.times)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cumsum</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回累计时间&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> np.array(self.times).cumsum().tolist()</span><br></pre></td></tr></table></figure>

<p>使用for循环执行加法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = torch.zeros(n)</span><br><span class="line">timer = Timer()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    c[i] = a[i] + b[i]</span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.5</span>f&#125;</span> sec&#x27;</span></span><br></pre></td></tr></table></figure>


<pre><code>&#39;0.13959 sec&#39;
</code></pre>
<p>使用重载的+来求和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">timer.start()</span><br><span class="line">d = a + b</span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.5</span>f&#125;</span> sec&#x27;</span></span><br></pre></td></tr></table></figure>


<pre><code>&#39;0.00127 sec&#39;
</code></pre>
<p>正态分布</p>
<p>计算正态分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">x,mu,sigma</span>):</span><br><span class="line">    p = <span class="number">1</span> / math.sqrt(<span class="number">2</span>*math.pi*sigma**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> p *np.exp(-<span class="number">0.5</span>/sigma**<span class="number">2</span>*(x-mu)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>可视化正态分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(-<span class="number">7</span>,<span class="number">7</span>,<span class="number">0.01</span>)</span><br><span class="line">x</span><br><span class="line">params = [(<span class="number">0</span>,<span class="number">1</span>),(<span class="number">0</span>,<span class="number">2</span>),(<span class="number">3</span>,<span class="number">1</span>)]</span><br><span class="line">d2l.plot(x,[normal(x,mu,sigma) <span class="keyword">for</span> mu, sigma <span class="keyword">in</span> params], xlabel=<span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">         ylabel=<span class="string">&#x27;p(x)&#x27;</span>, figsize=(<span class="number">4.5</span>, <span class="number">2.5</span>),</span><br><span class="line">        legend=[<span class="string">f&#x27;mean <span class="subst">&#123;mu&#125;</span>, std <span class="subst">&#123;sigma&#125;</span>&#x27;</span> <span class="keyword">for</span> mu, sigma <span class="keyword">in</span> params])</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_16_0.svg" alt="svg"></p>
<p>线性回归从零开始实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>生成数据集</p>
<p>使用带噪声的线性模型构造人造数据集.生成一个包含1000个样本的数据集,每个样本包含从正态分布采样的两个特征.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w,b,num_examples</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>,<span class="number">1</span>,(num_examples,<span class="built_in">len</span>(w)))</span><br><span class="line">    y = torch.matmul(X,w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>,<span class="number">0.01</span>,y.shape)</span><br><span class="line">    <span class="keyword">return</span> X,y.reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>x通过随机生成,w,b人为设定,将设定的w,b输入到生成数据集的函数synthetic_data()中,随机生成x和对应的y.也就是假定了线性函数,并通过这个线性函数和噪声生成数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w = torch.tensor([<span class="number">2</span>,-<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features,labels = synthetic_data(true_w,true_b,<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;features:&#x27;</span>,features[<span class="number">0</span>],<span class="string">&#x27;\nlabel:&#x27;</span>,labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>features: tensor([-0.2833, -1.9873]) 
label: tensor([10.3827])
</code></pre>
<p>观察features[0]和labels的散点图,可以看到两者之间有线性关系</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d2l.set_figsize()</span><br><span class="line">d2l.plt.scatter(features[:,<span class="number">0</span>].detach().numpy(),labels.detach().numpy(),<span class="number">1</span>);</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_25_0.svg" alt="svg"></p>
<p>读取数据集</p>
<p>data_iter 函数可以随机抽取生成数据集中的部分数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size,features,labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,num_examples,batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">        indices[i:<span class="built_in">min</span>(i + batch_size,num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices],labels[batch_indices]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size,features,labels):</span><br><span class="line">    <span class="built_in">print</span>(X,<span class="string">&#x27;\n&#x27;</span>,y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.6035,  0.9162],
        [ 1.7614,  1.0970],
        [-0.3809, -2.1023],
        [-1.7844, -0.4626],
        [ 1.1887, -1.6525],
        [-0.8178,  1.2127],
        [-0.2247,  0.8144],
        [ 0.4035,  1.8231],
        [ 0.3496, -0.2373],
        [ 0.4382, -0.9849]]) 
 tensor([[-0.1049],
        [ 3.9912],
        [10.5857],
        [ 2.1945],
        [12.1976],
        [-1.5510],
        [ 0.9946],
        [-1.2080],
        [ 5.7239],
        [ 8.4225]])
</code></pre>
<p>初始化模型参数w,b.初始化为正态分布的随机值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.normal(<span class="number">0</span>,<span class="number">0.01</span>,size=(<span class="number">2</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>定义模型</p>
<p>模型就是y&#x3D;xw+b</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X,w,b</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X,w) + b</span><br></pre></td></tr></table></figure>

<p>定义损失函数<strong>squared_loss()</strong></p>
<p>损失函数&#x3D; $\frac{1}{2} (\hat{y}-y)^2$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat,y</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape))**<span class="number">2</span>/<span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>定义优化算法</p>
<p>小批量梯度下降算法<strong>sgd()</strong>,参数每次-学习率*梯度&#x2F;样本量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params,lr,batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>指定训练参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">num_epochs = <span class="number">15</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size,features,labels):</span><br><span class="line">        l = loss(net(X,w,b),y)  <span class="comment">#计算损失函数</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()  <span class="comment">#求梯度</span></span><br><span class="line">        sgd([w,b],lr,batch_size) <span class="comment">#更新梯度</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment">#在梯度为0条件下,计算三次epoch的损失,对训练进行评价</span></span><br><span class="line">        train_1 = loss(net(features,w,b),labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>,loss <span class="subst">&#123;<span class="built_in">float</span>(train_1.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>epoch1,loss 2.184030
epoch2,loss 0.284604
epoch3,loss 0.037409
epoch4,loss 0.004994
epoch5,loss 0.000712
epoch6,loss 0.000142
epoch7,loss 0.000067
epoch8,loss 0.000056
epoch9,loss 0.000055
epoch10,loss 0.000055
epoch11,loss 0.000055
epoch12,loss 0.000055
epoch13,loss 0.000055
epoch14,loss 0.000055
epoch15,loss 0.000055
</code></pre>
<p>比较训练数据和真实数据的值,评价训练效果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差:<span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的误差:<span class="subst">&#123;true_b-b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>w的估计误差:tensor([-0.0004, -0.0001], grad_fn=&lt;SubBackward0&gt;)
b的误差:tensor([-0.0004], grad_fn=&lt;RsubBackward1&gt;)
</code></pre>
<p>线性回归的简洁实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>生成数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>

<p>读取数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays,batch_size,is_train=<span class="literal">True</span></span>): <span class="comment">#@save</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset,batch_size,shuffle=is_train)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features,labels),batch_size)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br></pre></td></tr></table></figure>




<pre><code>[tensor([[-2.0785, -1.1821],
         [-0.2558,  1.3086],
         [-0.6385,  0.1244],
         [-2.0940,  0.6876],
         [-0.2174,  1.4855],
         [ 0.3694, -0.7484],
         [ 0.6117, -0.2025],
         [-0.5963,  0.7489],
         [ 1.3280,  1.1161],
         [-0.9048,  0.0519]]),
 tensor([[ 4.0596],
         [-0.7617],
         [ 2.4987],
         [-2.3301],
         [-1.2802],
         [ 7.4839],
         [ 6.1274],
         [ 0.4620],
         [ 3.0419],
         [ 2.2106]])]
</code></pre>
<p>定义模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>初始化参数w,b为随机数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>,<span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0.])
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.MSELoss() <span class="comment">#调用pytorch中直接有的均方误差</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(),lr = <span class="number">0.03</span>)<span class="comment">#直接调用pytorch中的sgd算法</span></span><br></pre></td></tr></table></figure>

<p>训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X),y) <span class="comment">#求损失函数</span></span><br><span class="line">        trainer.zero_grad() <span class="comment">#梯度清零</span></span><br><span class="line">        l.backward()  <span class="comment">#求梯度</span></span><br><span class="line">        trainer.step() <span class="comment">#更新模型</span></span><br><span class="line">    l = loss(net(features),labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>,loss:<span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>epoch 1,loss:0.000285
epoch 2,loss:0.000097
epoch 3,loss:0.000098
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = net[<span class="number">0</span>].weight.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w的估计误差：&#x27;</span>, true_w - w.reshape(true_w.shape))</span><br><span class="line">b = net[<span class="number">0</span>].bias.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b的估计误差：&#x27;</span>, true_b - b)</span><br></pre></td></tr></table></figure>

<pre><code>w的估计误差： tensor([0.0005, 0.0004])
b的估计误差： tensor([0.0006])
</code></pre>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-多层感知机</title>
    <url>/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    <content><![CDATA[<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><p>多层感知机与softmax线性回归的区别:</p>
<ol>
<li>多层感知机有多层,而sotfmax线性回归只有一层</li>
<li>多层感知机有隐藏层,隐藏层需要激活函数来免除不同层之间的线性关系</li>
</ol>
<p>如果没有激活函数,多层感知机就与softmax线性回归没什么区别了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>使用ReLU函数作为激活函数来提供非线性变换</p>
<p>$ReLU(x) &#x3D; max(x,0)$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.linspace(-<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#cond = [True if (i&gt;0) else False for i in x] #也使用列表解析的方法</span></span><br><span class="line">y=<span class="number">0</span> + x*(x&gt;<span class="number">0</span>)</span><br><span class="line">plt.plot(x, y, color=<span class="string">&#x27;b&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>[&lt;matplotlib.lines.Line2D at 0x270f3bbf7c0&gt;]
</code></pre>
<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_5_1.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(-<span class="number">8.0</span>,<span class="number">8.0</span>,<span class="number">0.1</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.relu(x)</span><br><span class="line">d2l.plot(x.detach(),y.detach(),<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;relu(x)&#x27;</span>,figsize=(<span class="number">5</span>,<span class="number">2.5</span>))</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_6_0.svg" alt="svg"></p>
<p>使用sigmoid函数作为激活函数来提供非线性变换</p>
<p>$sigmoid(x)&#x3D;\frac{1}{1+e^{-x}}$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.sigmoid(x)</span><br><span class="line">d2l.plot(x.detach(),y.detach(),<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;sigmoid(x)&#x27;</span>,figsize=(<span class="number">5</span>,<span class="number">2.5</span>))</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_8_0.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#x.grad.data.zero_()</span></span><br><span class="line">y.backward(torch.ones_like(x),retain_graph=<span class="literal">True</span>)</span><br><span class="line">d2l.plot(x.detach(),x.grad,<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;grad of sigmoid&#x27;</span>,figsize=(<span class="number">5</span>,<span class="number">2.5</span>))</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_9_0.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p>获取图像分类数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>

<p>初始化模型参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs,num_outputs,num_hiddens = <span class="number">784</span>,<span class="number">10</span>,<span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = nn.Parameter(torch.randn(num_inputs,num_hiddens,requires_grad=<span class="literal">True</span>)*<span class="number">0.01</span>)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens,requires_grad=<span class="literal">True</span>))</span><br><span class="line">W2 = nn.Parameter(torch.randn(num_hiddens,num_outputs,requires_grad=<span class="literal">True</span>)*<span class="number">0.01</span>)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs,requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1,b1,W2,b2]</span><br></pre></td></tr></table></figure>

<p>激活函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X,a)</span><br></pre></td></tr></table></figure>

<p>模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape((-<span class="number">1</span>,num_inputs))</span><br><span class="line">    H = relu(X@W1 + b1)</span><br><span class="line">    <span class="keyword">return</span> (H@W2 + b2)</span><br></pre></td></tr></table></figure>

<p>损失函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs,lr = <span class="number">10</span>,<span class="number">0.1</span></span><br><span class="line">updater = torch.optim.SGD(params,lr=lr)</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,updater)</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_22_0.svg" alt="svg"></p>
<p>多层感知机的简洁实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(), <span class="comment">#将数据变成一维</span></span><br><span class="line">                    nn.Linear(<span class="number">784</span>,<span class="number">256</span>), <span class="comment">#输入层</span></span><br><span class="line">                    nn.ReLU(), <span class="comment">#激活函数</span></span><br><span class="line">                    nn.Linear(<span class="number">256</span>,<span class="number">10</span>)) <span class="comment">#隐藏层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>): <span class="comment">#将nn模型中的权重初始化为随机数</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight,std=<span class="number">0.01</span>)</span><br><span class="line">        </span><br><span class="line">net.apply(init_weights); <span class="comment">#应用初始化</span></span><br></pre></td></tr></table></figure>

<p>设置训练的超参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size,lr,num_epochs = <span class="number">256</span>,<span class="number">0.1</span>,<span class="number">10</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(),lr=lr)</span><br></pre></td></tr></table></figure>

<p>训练模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)</span><br></pre></td></tr></table></figure>


<p><img src="/2023/03/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/output_29_0.svg" alt="svg"></p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-模型选择,过拟合,欠拟合</title>
    <url>/2023/03/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88/</url>
    <content><![CDATA[<h1 id="模型选择欠拟合过拟合">模型选择,欠拟合,过拟合</h1>
<p>欠拟合: 简单的模型用复杂的数据进行训练时容易欠拟合</p>
<p>过拟合:
复杂的模型用简单的数据进行训练时容易过拟合,模型对训练数据的拟合比潜在的分布更接近</p>
<p>所以: 模型容量需要匹配数据的复杂度</p>
<figure>
<img src="1.png" alt="1.png" />
<figcaption aria-hidden="true">1.png</figcaption>
</figure>
<h3 id="模型容量">模型容量</h3>
<p>模型容量:
模型拟合各种函数的能力,低容量难以拟合数据,高容量容易记住所有训练数据</p>
<figure>
<img src="2.png" alt="2.png" />
<figcaption aria-hidden="true">2.png</figcaption>
</figure>
<h3 id="模型容量选择">模型容量选择</h3>
<p>模型容量低时对训练数据拟合的不好,训练误差大</p>
<p>模型容量高时对训练数据拟合更好,但模型容量太高容易导致过拟合,泛化误差反而增加</p>
<p>泛化误差最小时训练的模型对新数据的拟合最好,模型最优</p>
<figure>
<img src="3.png" alt="3.png" />
<figcaption aria-hidden="true">3.png</figcaption>
</figure>
<h3 id="估计模型容量">估计模型容量</h3>
<ol type="1">
<li>参数的个数</li>
<li>参数值的选择范围</li>
</ol>
<p><strong>VC维</strong> :</p>
<p>模型能够记住的最大的数据集的大小,可以用来衡量模型好坏,但深度学习中很少使用,因为不准确且很难计算</p>
<p>例子:
二维感知机能够区分3个点的任意放置,4个点时就有不能记住的类型了(xor),所以二维感知机的vc维=3</p>
<figure>
<img src="4.png" alt="4.png" />
<figcaption aria-hidden="true">4.png</figcaption>
</figure>
<p><strong>数据复杂度:</strong></p>
<ol type="1">
<li>样本个数</li>
<li>每个样本的元素个数</li>
<li>时间,空间结构</li>
<li>多样性</li>
</ol>
<p>训练误差(training error): 模型在训练数据集上计算得到的误差</p>
<p>泛化误差(generalization error):
模型应用在新数据集上计算得到的误差</p>
<p>训练数据集: 训练模型参数</p>
<p>验证数据集: 用来训练时评估模型好坏的数据集,用来选择模型的超参数</p>
<p>测试数据集: 只用一次的数据集,用来应用模型</p>
<p>K折交叉验证</p>
<p>将原始数据集分成K个不交叉的子集,然后执行K次模型训练和验证,每次在K-1个子集上训练,在最后一个子集上验证,最后对K次实验的结果取平均来估计训练和验证误差</p>
<p>K折交叉验证常用于样本数据很小时,避免验证的数据集占比过大,K的数量越大模型训练效果越好,但K越大训练的次数越多,要考虑到成本因素进行控制</p>
<h1 id="过拟合和欠拟合的样例">过拟合和欠拟合的样例</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<p>生成数据集</p>
<p><strong><span class="math display">\[y = 5 + 1.2x - 3.4\frac{x^2}{2!}
+ 5.6 \frac{x^3}{3!} + \epsilon \text{ where }\epsilon \sim
\mathcal{N}(0, 0.1^2).\]</span></strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_degree = <span class="number">20</span> </span><br><span class="line">n_train,n_test = <span class="number">100</span>,<span class="number">100</span> <span class="comment">#训练集,验证集的大小</span></span><br><span class="line">true_w = np.zeros(max_degree) <span class="comment">#创造一个20维的向量w,4维之后的都是扰动</span></span><br><span class="line">true_w[<span class="number">0</span>:<span class="number">4</span>] = np.array([<span class="number">5</span>,<span class="number">1.2</span>,-<span class="number">3.4</span>,<span class="number">5.6</span>]) <span class="comment">#前4维是正确的参数</span></span><br><span class="line">features = np.random.normal(size=(n_train + n_test,<span class="number">1</span>)) <span class="comment">#生成一个200行1列的随机数据作为x</span></span><br><span class="line">np.random.shuffle(features) <span class="comment">#将features打乱</span></span><br><span class="line">poly_features = np.power(features,np.arange(max_degree).reshape(<span class="number">1</span>,-<span class="number">1</span>)) <span class="comment">#将featruers中的每个x做[0次方,1次方,2次方...],此时x作为多项式回归模型的输入，使模型能够捕捉特征与目标变量之间的非线性关系。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_degree):</span><br><span class="line">    poly_features[:,i] /= math.gamma(i+<span class="number">1</span>) <span class="comment">#对生成的features进行归一化</span></span><br><span class="line">labels = np.dot(poly_features,true_w) <span class="comment">#x和w点积</span></span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>,size = labels.shape) <span class="comment">#x和w点积+扰动=y</span></span><br></pre></td></tr></table></figure>
<p>true_w、features、poly_features 和 labels 都是 numpy
数组，分别表示真实权重、样本特征、多项式特征矩阵和目标变量。通过将它们转换为
PyTorch 的 tensor 类型，我们可以在 PyTorch
中进行深度学习模型的训练和优化，同时还可以利用 GPU
等硬件加速功能来提高计算性能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w,features,poly_features,labels = [torch.tensor(x,dtype=torch.float32) </span><br><span class="line">                            <span class="keyword">for</span> x <span class="keyword">in</span> [true_w,features,poly_features,labels]]</span><br></pre></td></tr></table></figure>
<p>features[:2] 表示选取 features
数组的前两行，即前两个样本的特征向量。</p>
<p>poly_features[:2,:] 表示选取 poly_features
数组的前两行和所有列，即前两个样本的多项式特征矩阵。</p>
<p>labels[:2] 表示选取 labels
数组的前两个元素，即前两个样本的目标变量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">features[:<span class="number">2</span>],poly_features[:<span class="number">2</span>,:],labels[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[ 0.4346],
         [-0.0716]]),
 tensor([[ 1.0000e+00,  4.3461e-01,  9.4444e-02,  1.3682e-02,  1.4866e-03,
           1.2922e-04,  9.3600e-06,  5.8114e-07,  3.1571e-08,  1.5246e-09,
           6.6260e-11,  2.6180e-12,  9.4816e-14,  3.1699e-15,  9.8404e-17,
           2.8512e-18,  7.7447e-20,  1.9800e-21,  4.7807e-23,  1.0935e-24],
         [ 1.0000e+00, -7.1576e-02,  2.5616e-03, -6.1116e-05,  1.0936e-06,
          -1.5655e-08,  1.8676e-10, -1.9096e-12,  1.7086e-14, -1.3588e-16,
           9.7258e-19, -6.3285e-21,  3.7748e-23, -2.0783e-25,  1.0626e-27,
          -5.0703e-30,  2.2682e-32, -9.5499e-35,  3.7975e-37, -1.4306e-39]]),
 tensor([5.2514, 4.9395]))</code></pre>
<p>对模型进行训练和测试</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net,data_iter,loss</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>) <span class="comment">#累加器对象,有2维,一个表示累加总和,一个表示累加的数的个数</span></span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X) <span class="comment">#根据x计算y</span></span><br><span class="line">        y = y.reshape(out.shape) <span class="comment">#y改变成和out一样的形状</span></span><br><span class="line">        l = loss(out,y) <span class="comment">#算损失</span></span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(),l.numel()) <span class="comment">#将l的信息输入累加器</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>] <span class="comment">#返回平均损失</span></span><br></pre></td></tr></table></figure>
<p>训练函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features,test_features,train_labels,test_labels,num_epochs=<span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>) <span class="comment">#不损失降维操作的均方误差(即返回所有损失值,不进行求和等运算)</span></span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>] <span class="comment">#最后一个维度的大小,即每个样本的特征数</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape,<span class="number">1</span>,bias=<span class="literal">False</span>))</span><br><span class="line">    batch_size = <span class="built_in">min</span>(<span class="number">10</span>,train_labels.shape[<span class="number">0</span>]) <span class="comment">#barch_size 在不超过训练集大小的情况下,一次取10个</span></span><br><span class="line">    train_iter = d2l.load_array((train_features,train_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),batch_size) <span class="comment">#迭代器输入训练的x,y,每次输出batch_size个数</span></span><br><span class="line">    test_iter = d2l.load_array((test_features,test_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),batch_size,is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(),lr=<span class="number">0.01</span>) <span class="comment">#定义了一个使用随机梯度下降（SGD）算法进行模型参数优化的优化器</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>,ylabel=<span class="string">&#x27;loss&#x27;</span>,yscale=<span class="string">&#x27;log&#x27;</span>,xlim=[<span class="number">1</span>,num_epochs],ylim=[<span class="number">1e-3</span>,<span class="number">1e2</span>],legend=[<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;test&#x27;</span>]) <span class="comment">#绘制训练损失和测试损失随着训练迭代次数的变化趋势</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net,train_iter,loss,trainer)</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch +<span class="number">1</span>,(evaluate_loss(net,train_iter,loss),evaluate_loss(net,test_iter,loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>,net[<span class="number">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure>
<p><strong>三阶多项式函数拟合</strong></p>
<p>四个输入分别是:
前n_train(100)行4列作为训练集,n_train(100)行后面所有行4列作为测试集,labels前n_train行作为train_labels(y<sup>),labels的n_train后面所有行作为test_labels(y</sup>)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train,:<span class="number">4</span>],poly_features[n_train:,:<span class="number">4</span>],labels[:n_train],labels[n_train:])</span><br></pre></td></tr></table></figure>
<pre><code>weight: [[ 4.99034    1.1656325 -3.4056838  5.650331 ]]</code></pre>
<figure>
<img src="output_28_1.svg" alt="svg" />
<figcaption aria-hidden="true">svg</figcaption>
</figure>
<p>可以看出,w给前4维时拟合的很好</p>
<p><strong>线性函数拟合(欠拟合)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>],labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>
<pre><code>weight: [[3.4050567 3.404248 ]]</code></pre>
<figure>
<img src="output_31_1.svg" alt="svg" />
<figcaption aria-hidden="true">svg</figcaption>
</figure>
<p>w只给前2个维度时给的数据不足,欠拟合</p>
<p><strong>高阶多项式函数拟合(过拟合)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br></pre></td></tr></table></figure>
<pre><code>weight: [[ 4.964952    1.2093626  -3.2487652   5.3281727  -0.5115174   1.2561612
  -0.06610011  0.18418686  0.11113311  0.01419835  0.15148187  0.10667927
   0.18928145 -0.13205634  0.18986128  0.16003044 -0.0809902  -0.20620278
  -0.03729296 -0.01795298]]</code></pre>
<figure>
<img src="output_34_1.svg" alt="svg" />
<figcaption aria-hidden="true">svg</figcaption>
</figure>
<p>w给了20维度,因为4维之后的参数都是扰动,所以模型过大会导致过拟合</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>深度学习笔记</tag>
      </tags>
  </entry>
</search>
